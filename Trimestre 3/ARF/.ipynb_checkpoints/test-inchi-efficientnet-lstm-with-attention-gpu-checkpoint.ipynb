{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:14.524347Z",
     "iopub.status.busy": "2021-05-21T07:30:14.524005Z",
     "iopub.status.idle": "2021-05-21T07:30:14.528057Z",
     "shell.execute_reply": "2021-05-21T07:30:14.527106Z",
     "shell.execute_reply.started": "2021-05-21T07:30:14.524309Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:14.534425Z",
     "iopub.status.busy": "2021-05-21T07:30:14.533966Z",
     "iopub.status.idle": "2021-05-21T07:30:19.090475Z",
     "shell.execute_reply": "2021-05-21T07:30:19.088662Z",
     "shell.execute_reply.started": "2021-05-21T07:30:14.534386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (2424186, 2)\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESS\n",
    "\n",
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "\n",
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('../input/bms-molecular-translation/train_labels.csv')\n",
    "print(f'train.shape: {train.shape}')\n",
    "\n",
    "# ====================================================\n",
    "# Preprocess functions\n",
    "# ====================================================\n",
    "def split_form(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n",
    "        elem = re.match(r\"\\D+\", i).group()\n",
    "        num = i.replace(elem, \"\")\n",
    "        if num == \"\":\n",
    "            string += f\"{elem} \"\n",
    "        else:\n",
    "            string += f\"{elem} {str(num)} \"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "def split_form2(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[a-z][^a-z]*\", form):\n",
    "        elem = i[0]\n",
    "        num = i.replace(elem, \"\").replace('/', \"\")\n",
    "        num_string = ''\n",
    "        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n",
    "            num_list = list(re.findall(r'\\d+', j))\n",
    "            assert len(num_list) == 1, f\"len(num_list) != 1\"\n",
    "            _num = num_list[0]\n",
    "            if j == _num:\n",
    "                num_string += f\"{_num} \"\n",
    "            else:\n",
    "                extra = j.replace(_num, \"\")\n",
    "                num_string += f\"{_num} {' '.join(list(extra))} \"\n",
    "        string += f\"/{elem} {num_string}\"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "# ====================================================\n",
    "# Tokenizer\n",
    "# ====================================================\n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    # ====================================================\n",
    "    # preprocess train.csv\n",
    "    # ====================================================\n",
    "    train['InChI_1'] = train['InChI'].progress_apply(lambda x: x.split('/')[1])\n",
    "    train['InChI_text'] = train['InChI_1'].progress_apply(split_form) + ' ' + \\\n",
    "                            train['InChI'].apply(lambda x: '/'.join(x.split('/')[2:])).progress_apply(split_form2).values\n",
    "    # ====================================================\n",
    "    # create tokenizer\n",
    "    # ====================================================\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train['InChI_text'].values)\n",
    "    torch.save(tokenizer, 'tokenizer2.pth')\n",
    "    print('Saved tokenizer')\n",
    "    # ====================================================\n",
    "    # preprocess train.csv\n",
    "    # ====================================================\n",
    "    lengths = []\n",
    "    tk0 = tqdm(train['InChI_text'].values, total=len(train))\n",
    "    for text in tk0:\n",
    "        seq = tokenizer.text_to_sequence(text)\n",
    "        length = len(seq) - 2\n",
    "        lengths.append(length)\n",
    "    train['InChI_length'] = lengths\n",
    "    train.to_pickle('train2.pkl')\n",
    "    print('Saved preprocessed train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:19.092958Z",
     "iopub.status.busy": "2021-05-21T07:30:19.092615Z",
     "iopub.status.idle": "2021-05-21T07:30:19.099414Z",
     "shell.execute_reply": "2021-05-21T07:30:19.098553Z",
     "shell.execute_reply.started": "2021-05-21T07:30:19.092922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__ == '__main__':\\n    main()\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if __name__ == '__main__':\n",
    "    main()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:19.101692Z",
     "iopub.status.busy": "2021-05-21T07:30:19.101132Z",
     "iopub.status.idle": "2021-05-21T07:30:19.110356Z",
     "shell.execute_reply": "2021-05-21T07:30:19.109580Z",
     "shell.execute_reply.started": "2021-05-21T07:30:19.101524Z"
    },
    "papermill": {
     "duration": 0.022561,
     "end_time": "2021-03-09T09:44:33.513503",
     "exception": false,
     "start_time": "2021-03-09T09:44:33.490942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:19.113764Z",
     "iopub.status.busy": "2021-05-21T07:30:19.113335Z",
     "iopub.status.idle": "2021-05-21T07:30:23.586900Z",
     "shell.execute_reply": "2021-05-21T07:30:23.586059Z",
     "shell.execute_reply.started": "2021-05-21T07:30:19.113630Z"
    },
    "papermill": {
     "duration": 13.620951,
     "end_time": "2021-03-09T09:44:47.179832",
     "exception": false,
     "start_time": "2021-03-09T09:44:33.558881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (2424186, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>InChI_1</th>\n",
       "      <th>InChI_text</th>\n",
       "      <th>InChI_length</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000011a64c74</td>\n",
       "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
       "      <td>C13H20OS</td>\n",
       "      <td>C 13 H 20 O S /c 1 - 9 ( 2 ) 8 - 15 - 13 - 6 -...</td>\n",
       "      <td>59</td>\n",
       "      <td>../input/bms-molecular-translation/train/0/0/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000019cc0cd2</td>\n",
       "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
       "      <td>C21H30O4</td>\n",
       "      <td>C 21 H 30 O 4 /c 1 - 12 ( 22 ) 25 - 14 - 6 - 8...</td>\n",
       "      <td>108</td>\n",
       "      <td>../input/bms-molecular-translation/train/0/0/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000252b6d2b</td>\n",
       "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
       "      <td>C24H23N5O4</td>\n",
       "      <td>C 24 H 23 N 5 O 4 /c 1 - 14 - 13 - 15 ( 7 - 8 ...</td>\n",
       "      <td>112</td>\n",
       "      <td>../input/bms-molecular-translation/train/0/0/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026b49b7e</td>\n",
       "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
       "      <td>C17H24N2O4S</td>\n",
       "      <td>C 17 H 24 N 2 O 4 S /c 1 - 12 ( 20 ) 18 - 13 (...</td>\n",
       "      <td>108</td>\n",
       "      <td>../input/bms-molecular-translation/train/0/0/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026fc6c36</td>\n",
       "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
       "      <td>C10H19N3O2S</td>\n",
       "      <td>C 10 H 19 N 3 O 2 S /c 1 - 15 - 10 ( 14 ) 12 -...</td>\n",
       "      <td>72</td>\n",
       "      <td>../input/bms-molecular-translation/train/0/0/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                              InChI  \\\n",
       "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...   \n",
       "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...   \n",
       "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...   \n",
       "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...   \n",
       "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...   \n",
       "\n",
       "       InChI_1                                         InChI_text  \\\n",
       "0     C13H20OS  C 13 H 20 O S /c 1 - 9 ( 2 ) 8 - 15 - 13 - 6 -...   \n",
       "1     C21H30O4  C 21 H 30 O 4 /c 1 - 12 ( 22 ) 25 - 14 - 6 - 8...   \n",
       "2   C24H23N5O4  C 24 H 23 N 5 O 4 /c 1 - 14 - 13 - 15 ( 7 - 8 ...   \n",
       "3  C17H24N2O4S  C 17 H 24 N 2 O 4 S /c 1 - 12 ( 20 ) 18 - 13 (...   \n",
       "4  C10H19N3O2S  C 10 H 19 N 3 O 2 S /c 1 - 15 - 10 ( 14 ) 12 -...   \n",
       "\n",
       "   InChI_length                                          file_path  \n",
       "0            59  ../input/bms-molecular-translation/train/0/0/0...  \n",
       "1           108  ../input/bms-molecular-translation/train/0/0/0...  \n",
       "2           112  ../input/bms-molecular-translation/train/0/0/0...  \n",
       "3           108  ../input/bms-molecular-translation/train/0/0/0...  \n",
       "4            72  ../input/bms-molecular-translation/train/0/0/0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "train = pd.read_pickle('../input/inchi-preprocess-2/train2.pkl')\n",
    "\n",
    "def get_train_file_path(image_id):\n",
    "    return \"../input/bms-molecular-translation/train/{}/{}/{}/{}.png\".format(\n",
    "        image_id[0], image_id[1], image_id[2], image_id \n",
    "    )\n",
    "\n",
    "train['file_path'] = train['image_id'].apply(get_train_file_path)\n",
    "\n",
    "print(f'train.shape: {train.shape}')\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.590749Z",
     "iopub.status.busy": "2021-05-21T07:30:23.590410Z",
     "iopub.status.idle": "2021-05-21T07:30:23.606079Z",
     "shell.execute_reply": "2021-05-21T07:30:23.605273Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.590712Z"
    },
    "papermill": {
     "duration": 0.040687,
     "end_time": "2021-03-09T09:44:47.238213",
     "exception": false,
     "start_time": "2021-03-09T09:44:47.197526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "\n",
    "tokenizer = torch.load('../input/inchi-preprocess-2/tokenizer2.pth')\n",
    "print(f\"tokenizer.stoi: {tokenizer.stoi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.609374Z",
     "iopub.status.busy": "2021-05-21T07:30:23.609011Z",
     "iopub.status.idle": "2021-05-21T07:30:23.624277Z",
     "shell.execute_reply": "2021-05-21T07:30:23.623474Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.609338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['InChI_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.626696Z",
     "iopub.status.busy": "2021-05-21T07:30:23.626094Z",
     "iopub.status.idle": "2021-05-21T07:30:23.634266Z",
     "shell.execute_reply": "2021-05-21T07:30:23.633542Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.626660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[484837:969674]['InChI_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.636145Z",
     "iopub.status.busy": "2021-05-21T07:30:23.635803Z",
     "iopub.status.idle": "2021-05-21T07:30:23.642609Z",
     "shell.execute_reply": "2021-05-21T07:30:23.641669Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.636112Z"
    },
    "papermill": {
     "duration": 0.027167,
     "end_time": "2021-03-09T09:44:47.315067",
     "exception": false,
     "start_time": "2021-03-09T09:44:47.2879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    max_len=275\n",
    "    print_freq=50\n",
    "    num_workers=2\n",
    "    model_name='efficientnetb0'\n",
    "    size=64 # modificar para 224\n",
    "    scheduler='ReduceLROnPlateau' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    epochs=1 # not to exceed 9h\n",
    "    factor=0.2 # ReduceLROnPlateau\n",
    "    patience=4 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    #T_max=4 # CosineAnnealingLR\n",
    "    #T_0=4 # CosineAnnealingWarmRestarts\n",
    "    encoder_lr=1e-4\n",
    "    decoder_lr=4e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=30\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=5\n",
    "    attention_dim=64\n",
    "    embed_dim=64\n",
    "    decoder_dim=64\n",
    "    dropout=0.5\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0] # [0, 1, 2, 3, 4]\n",
    "    train=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.644725Z",
     "iopub.status.busy": "2021-05-21T07:30:23.644358Z",
     "iopub.status.idle": "2021-05-21T07:30:23.655604Z",
     "shell.execute_reply": "2021-05-21T07:30:23.654757Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.644688Z"
    },
    "papermill": {
     "duration": 1.223567,
     "end_time": "2021-03-09T09:44:48.555526",
     "exception": false,
     "start_time": "2021-03-09T09:44:47.331959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.659090Z",
     "iopub.status.busy": "2021-05-21T07:30:23.657665Z",
     "iopub.status.idle": "2021-05-21T07:30:23.670976Z",
     "shell.execute_reply": "2021-05-21T07:30:23.670129Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.659063Z"
    },
    "papermill": {
     "duration": 3.23384,
     "end_time": "2021-03-09T09:44:51.840158",
     "exception": false,
     "start_time": "2021-03-09T09:44:48.606318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import Levenshtein\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "\n",
    "# import torch_xla\n",
    "# import torch_xla.utils.utils as xu\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.debug.metrics as met\n",
    "# import torch_xla.distributed.data_parallel as dp\n",
    "# import torch_xla.distributed.parallel_loader as pl\n",
    "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "#import timm\n",
    "import gc\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.673189Z",
     "iopub.status.busy": "2021-05-21T07:30:23.672907Z",
     "iopub.status.idle": "2021-05-21T07:30:23.686037Z",
     "shell.execute_reply": "2021-05-21T07:30:23.685175Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.673165Z"
    },
    "papermill": {
     "duration": 0.037035,
     "end_time": "2021-03-09T09:44:51.934525",
     "exception": false,
     "start_time": "2021-03-09T09:44:51.89749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        score = Levenshtein.distance(true, pred)\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019365,
     "end_time": "2021-03-09T09:44:51.973096",
     "exception": false,
     "start_time": "2021-03-09T09:44:51.953731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:23.688645Z",
     "iopub.status.busy": "2021-05-21T07:30:23.688021Z",
     "iopub.status.idle": "2021-05-21T07:30:24.113964Z",
     "shell.execute_reply": "2021-05-21T07:30:24.113028Z",
     "shell.execute_reply.started": "2021-05-21T07:30:23.688607Z"
    },
    "papermill": {
     "duration": 0.063587,
     "end_time": "2021-03-09T09:44:52.056037",
     "exception": false,
     "start_time": "2021-03-09T09:44:51.99245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n",
      "0    96968\n",
      "1    96968\n",
      "2    96967\n",
      "3    96967\n",
      "4    96967\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.iloc[484837:969674].reset_index(drop=True).copy() # Plan is to divide train data into n parts for n models. So, use .iloc[] accordingly.\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018688,
     "end_time": "2021-03-09T09:44:52.093619",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.074931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:24.115735Z",
     "iopub.status.busy": "2021-05-21T07:30:24.115209Z",
     "iopub.status.idle": "2021-05-21T07:30:24.127474Z",
     "shell.execute_reply": "2021-05-21T07:30:24.126512Z",
     "shell.execute_reply.started": "2021-05-21T07:30:24.115684Z"
    },
    "papermill": {
     "duration": 0.032248,
     "end_time": "2021-03-09T09:44:52.144808",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.11256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.labels = df['InChI_text'].values\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = self.labels[idx]\n",
    "        label = self.tokenizer.text_to_sequence(label)\n",
    "        label_length = len(label)\n",
    "        label_length = torch.LongTensor([label_length])\n",
    "        return image, torch.LongTensor(label), label_length\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:24.130642Z",
     "iopub.status.busy": "2021-05-21T07:30:24.130389Z",
     "iopub.status.idle": "2021-05-21T07:30:24.143175Z",
     "shell.execute_reply": "2021-05-21T07:30:24.142293Z",
     "shell.execute_reply.started": "2021-05-21T07:30:24.130609Z"
    },
    "papermill": {
     "duration": 0.02994,
     "end_time": "2021-03-09T09:44:52.194849",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.164909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bms_collate(batch):\n",
    "    imgs, labels, label_lengths = [], [], []\n",
    "    for data_point in batch:\n",
    "        imgs.append(data_point[0])\n",
    "        labels.append(data_point[1])\n",
    "        label_lengths.append(data_point[2])\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.stoi[\"<pad>\"])\n",
    "    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021319,
     "end_time": "2021-03-09T09:44:52.239264",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.217945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:24.144857Z",
     "iopub.status.busy": "2021-05-21T07:30:24.144436Z",
     "iopub.status.idle": "2021-05-21T07:30:24.153574Z",
     "shell.execute_reply": "2021-05-21T07:30:24.152605Z",
     "shell.execute_reply.started": "2021-05-21T07:30:24.144816Z"
    },
    "papermill": {
     "duration": 0.032726,
     "end_time": "2021-03-09T09:44:52.293401",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.260675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:24.156355Z",
     "iopub.status.busy": "2021-05-21T07:30:24.156053Z",
     "iopub.status.idle": "2021-05-21T07:30:25.094934Z",
     "shell.execute_reply": "2021-05-21T07:30:25.094104Z",
     "shell.execute_reply.started": "2021-05-21T07:30:24.156318Z"
    },
    "papermill": {
     "duration": 0.358665,
     "end_time": "2021-03-09T09:44:52.673441",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.314776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_dataset = TrainDataset(train, tokenizer, transform=get_transforms(data='train'))\n",
    "\n",
    "for i in range(1):\n",
    "    image, label, label_length = train_dataset[i]\n",
    "    text = tokenizer.sequence_to_text(label.numpy())\n",
    "    plt.imshow(image.transpose(0, 1).transpose(1, 2))\n",
    "    plt.title(f'label: {label}  text: {text}  label_length: {label_length}')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022092,
     "end_time": "2021-03-09T09:44:52.71725",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.695158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.098241Z",
     "iopub.status.busy": "2021-05-21T07:30:25.097939Z",
     "iopub.status.idle": "2021-05-21T07:30:25.108633Z",
     "shell.execute_reply": "2021-05-21T07:30:25.107935Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.098203Z"
    },
    "papermill": {
     "duration": 0.033392,
     "end_time": "2021-03-09T09:44:52.772345",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.738953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "'''\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.cnn  = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        #self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \n",
    "        #Forward propagation.\n",
    "        #:param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        #:return: encoded images\n",
    "        bs = images.size(0)\n",
    "        out = self.cnn(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        #out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out\n",
    "'''\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.cnn  = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        #self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \n",
    "        #Forward propagation.\n",
    "        #:param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        #:return: encoded images\n",
    "        bs = images.size(0)\n",
    "        out = self.cnn(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        #out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.111111Z",
     "iopub.status.busy": "2021-05-21T07:30:25.110701Z",
     "iopub.status.idle": "2021-05-21T07:30:25.140774Z",
     "shell.execute_reply": "2021-05-21T07:30:25.139747Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.111062Z"
    },
    "papermill": {
     "duration": 0.049082,
     "end_time": "2021-03-09T09:44:52.843864",
     "exception": false,
     "start_time": "2021-03-09T09:44:52.794782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention network for calculate attention value\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder network with attention network used for training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
    "        embeddings = self.embedding(start_tockens)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(decode_lengths):\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                (h, c))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020597,
     "end_time": "2021-03-09T09:44:58.570358",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.549761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.142659Z",
     "iopub.status.busy": "2021-05-21T07:30:25.142194Z",
     "iopub.status.idle": "2021-05-21T07:30:25.166735Z",
     "shell.execute_reply": "2021-05-21T07:30:25.164963Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.142620Z"
    },
    "papermill": {
     "duration": 0.044148,
     "end_time": "2021-03-09T09:44:58.635121",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.590973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(train_loader, encoder, decoder, criterion, \n",
    "             encoder_optimizer, decoder_optimizer, epoch,\n",
    "             encoder_scheduler, decoder_scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # switch to train mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (images, labels, label_lengths) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        features = encoder(images)\n",
    "        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        loss = criterion(predictions, targets)\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n",
    "        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n",
    "                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n",
    "                  #'Encoder LR: {encoder_lr:.6f}  '\n",
    "                  #'Decoder LR: {decoder_lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   encoder_grad_norm=encoder_grad_norm,\n",
    "                   decoder_grad_norm=decoder_grad_norm,\n",
    "                   #encoder_lr=encoder_scheduler.get_lr()[0],\n",
    "                   #decoder_lr=decoder_scheduler.get_lr()[0],\n",
    "                   ))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    text_preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (images) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        with torch.no_grad():\n",
    "            features = encoder(images)\n",
    "            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n",
    "        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
    "        _text_preds = tokenizer.predict_captions(predicted_sequence)\n",
    "        text_preds.append(_text_preds)\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    text_preds = np.concatenate(text_preds)\n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021011,
     "end_time": "2021-03-09T09:44:58.676694",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.655683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.168306Z",
     "iopub.status.busy": "2021-05-21T07:30:25.167956Z",
     "iopub.status.idle": "2021-05-21T07:30:25.189393Z",
     "shell.execute_reply": "2021-05-21T07:30:25.188642Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.168270Z"
    },
    "papermill": {
     "duration": 0.041299,
     "end_time": "2021-03-09T09:44:58.739015",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.697716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    # device = xm.xla_device()\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    valid_labels = valid_folds['InChI'].values\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, tokenizer, transform=get_transforms(data='train'))\n",
    "    valid_dataset = TestDataset(valid_folds, transform=get_transforms(data='valid'))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=CFG.num_workers, \n",
    "                              pin_memory=True,\n",
    "                              drop_last=True, \n",
    "                              collate_fn=bms_collate)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True, \n",
    "                              drop_last=False)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    encoder = Encoder()\n",
    "    encoder.to(device)\n",
    "    encoder_optimizer = Adam(encoder.parameters(), lr=CFG.encoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    encoder_scheduler = get_scheduler(encoder_optimizer)\n",
    "    \n",
    "    decoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n",
    "                                   embed_dim=CFG.embed_dim,\n",
    "                                   decoder_dim=CFG.decoder_dim,\n",
    "                                   vocab_size=len(tokenizer),\n",
    "                                   dropout=CFG.dropout)\n",
    "    decoder.to(device)\n",
    "    decoder_optimizer = Adam(decoder.parameters(), lr=CFG.decoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    decoder_scheduler = get_scheduler(decoder_optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi[\"<pad>\"])\n",
    "\n",
    "    best_score = np.inf\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n",
    "                            encoder_optimizer, decoder_optimizer, epoch, \n",
    "                            encoder_scheduler, decoder_scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n",
    "        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n",
    "        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n",
    "        LOGGER.info(f\"preds: {text_preds[:5]}\")\n",
    "        \n",
    "        # scoring\n",
    "        score = get_score(valid_labels, text_preds)\n",
    "        \n",
    "        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n",
    "            encoder_scheduler.step(score)\n",
    "        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n",
    "            encoder_scheduler.step()\n",
    "        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n",
    "            encoder_scheduler.step()\n",
    "            \n",
    "        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n",
    "            decoder_scheduler.step(score)\n",
    "        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n",
    "            decoder_scheduler.step()\n",
    "        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n",
    "            decoder_scheduler.step()\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'encoder': encoder.state_dict(), \n",
    "                        'encoder_optimizer': encoder_optimizer.state_dict(), \n",
    "                        'encoder_scheduler': encoder_scheduler.state_dict(), \n",
    "                        'decoder': decoder.state_dict(), \n",
    "                        'decoder_optimizer': decoder_optimizer.state_dict(), \n",
    "                        'decoder_scheduler': decoder_scheduler.state_dict(), \n",
    "                        'text_preds': text_preds,\n",
    "                       },\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021144,
     "end_time": "2021-03-09T09:44:58.78128",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.760136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.191037Z",
     "iopub.status.busy": "2021-05-21T07:30:25.190702Z",
     "iopub.status.idle": "2021-05-21T07:30:25.203511Z",
     "shell.execute_reply": "2021-05-21T07:30:25.202660Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.191001Z"
    },
    "papermill": {
     "duration": 0.030114,
     "end_time": "2021-03-09T09:44:58.832368",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.802254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare: 1.train  2.folds\n",
    "    \"\"\"\n",
    "\n",
    "    if CFG.train:\n",
    "        # train\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                train_loop(folds, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.205415Z",
     "iopub.status.busy": "2021-05-21T07:30:25.204970Z",
     "iopub.status.idle": "2021-05-21T07:30:25.240014Z",
     "shell.execute_reply": "2021-05-21T07:30:25.239102Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.205378Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "        #Evitar error???\n",
    "        self.device = device\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
    "        embeddings = self.embedding(start_tockens)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(decode_lengths):\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                (h, c))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.241516Z",
     "iopub.status.busy": "2021-05-21T07:30:25.241083Z",
     "iopub.status.idle": "2021-05-21T07:30:25.255733Z",
     "shell.execute_reply": "2021-05-21T07:30:25.254924Z",
     "shell.execute_reply.started": "2021-05-21T07:30:25.241433Z"
    }
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T07:30:25.257494Z",
     "iopub.status.busy": "2021-05-21T07:30:25.256971Z"
    },
    "papermill": {
     "duration": 2043.388258,
     "end_time": "2021-03-09T10:19:02.241899",
     "exception": false,
     "start_time": "2021-03-09T09:44:58.853641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/12928] Data 0.394 (0.394) Elapsed 0m 1s (remain 233m 17s) Loss: 5.2838(5.2838) Encoder Grad: 9.9907  Decoder Grad: 0.5168  \n",
      "Epoch: [1][50/12928] Data 0.000 (0.008) Elapsed 0m 31s (remain 131m 46s) Loss: 3.6903(4.3311) Encoder Grad: 0.6722  Decoder Grad: 0.6089  \n",
      "Epoch: [1][100/12928] Data 0.000 (0.004) Elapsed 1m 1s (remain 129m 34s) Loss: 3.4615(3.9311) Encoder Grad: 0.3322  Decoder Grad: 0.2968  \n",
      "Epoch: [1][150/12928] Data 0.000 (0.003) Elapsed 1m 31s (remain 128m 28s) Loss: 3.3524(3.7503) Encoder Grad: 0.2207  Decoder Grad: 0.2221  \n",
      "Epoch: [1][200/12928] Data 0.000 (0.002) Elapsed 2m 1s (remain 127m 48s) Loss: 3.2902(3.6460) Encoder Grad: 0.2389  Decoder Grad: 0.2049  \n",
      "Epoch: [1][250/12928] Data 0.000 (0.002) Elapsed 2m 30s (remain 126m 59s) Loss: 3.2749(3.5753) Encoder Grad: 0.2230  Decoder Grad: 0.2022  \n",
      "Epoch: [1][300/12928] Data 0.000 (0.001) Elapsed 3m 1s (remain 126m 34s) Loss: 3.1869(3.5231) Encoder Grad: 0.2086  Decoder Grad: 0.2106  \n",
      "Epoch: [1][350/12928] Data 0.000 (0.001) Elapsed 3m 30s (remain 125m 46s) Loss: 3.1011(3.4752) Encoder Grad: 0.1599  Decoder Grad: 0.2502  \n",
      "Epoch: [1][400/12928] Data 0.000 (0.001) Elapsed 4m 0s (remain 125m 21s) Loss: 3.0754(3.4300) Encoder Grad: 0.1237  Decoder Grad: 0.2546  \n",
      "Epoch: [1][450/12928] Data 0.000 (0.001) Elapsed 4m 30s (remain 124m 33s) Loss: 2.9921(3.3852) Encoder Grad: 0.1533  Decoder Grad: 0.2393  \n",
      "Epoch: [1][500/12928] Data 0.000 (0.001) Elapsed 5m 0s (remain 124m 17s) Loss: 2.9281(3.3427) Encoder Grad: 0.1415  Decoder Grad: 0.2318  \n",
      "Epoch: [1][550/12928] Data 0.000 (0.001) Elapsed 5m 31s (remain 124m 13s) Loss: 2.9364(3.3032) Encoder Grad: 0.1314  Decoder Grad: 0.2487  \n",
      "Epoch: [1][600/12928] Data 0.000 (0.001) Elapsed 6m 1s (remain 123m 44s) Loss: 2.7731(3.2647) Encoder Grad: 0.3455  Decoder Grad: 0.2359  \n",
      "Epoch: [1][650/12928] Data 0.000 (0.001) Elapsed 6m 32s (remain 123m 23s) Loss: 2.8212(3.2291) Encoder Grad: 0.2314  Decoder Grad: 0.2176  \n",
      "Epoch: [1][700/12928] Data 0.000 (0.001) Elapsed 7m 2s (remain 122m 55s) Loss: 2.7172(3.1963) Encoder Grad: 0.2169  Decoder Grad: 0.2229  \n",
      "Epoch: [1][750/12928] Data 0.000 (0.001) Elapsed 7m 33s (remain 122m 25s) Loss: 2.7081(3.1656) Encoder Grad: 0.1130  Decoder Grad: 0.2094  \n",
      "Epoch: [1][800/12928] Data 0.000 (0.001) Elapsed 8m 3s (remain 122m 3s) Loss: 2.7405(3.1380) Encoder Grad: 0.1536  Decoder Grad: 0.2029  \n",
      "Epoch: [1][850/12928] Data 0.000 (0.001) Elapsed 8m 34s (remain 121m 47s) Loss: 2.6428(3.1122) Encoder Grad: 0.1450  Decoder Grad: 0.1845  \n",
      "Epoch: [1][900/12928] Data 0.000 (0.001) Elapsed 9m 4s (remain 121m 13s) Loss: 2.6438(3.0870) Encoder Grad: 0.1538  Decoder Grad: 0.2304  \n",
      "Epoch: [1][950/12928] Data 0.000 (0.001) Elapsed 9m 35s (remain 120m 51s) Loss: 2.5866(3.0643) Encoder Grad: 0.1086  Decoder Grad: 0.1880  \n",
      "Epoch: [1][1000/12928] Data 0.000 (0.001) Elapsed 10m 6s (remain 120m 29s) Loss: 2.6313(3.0431) Encoder Grad: 0.1450  Decoder Grad: 0.2027  \n",
      "Epoch: [1][1050/12928] Data 0.000 (0.001) Elapsed 10m 37s (remain 120m 1s) Loss: 2.6216(3.0227) Encoder Grad: 0.1467  Decoder Grad: 0.1797  \n",
      "Epoch: [1][1100/12928] Data 0.000 (0.001) Elapsed 11m 8s (remain 119m 43s) Loss: 2.5738(3.0042) Encoder Grad: 0.1338  Decoder Grad: 0.1781  \n",
      "Epoch: [1][1150/12928] Data 0.000 (0.000) Elapsed 11m 38s (remain 119m 12s) Loss: 2.5314(2.9855) Encoder Grad: 0.1189  Decoder Grad: 0.1837  \n",
      "Epoch: [1][1200/12928] Data 0.000 (0.000) Elapsed 12m 9s (remain 118m 44s) Loss: 2.5861(2.9683) Encoder Grad: 0.1045  Decoder Grad: 0.1858  \n",
      "Epoch: [1][1250/12928] Data 0.000 (0.000) Elapsed 12m 40s (remain 118m 19s) Loss: 2.5617(2.9520) Encoder Grad: 0.0917  Decoder Grad: 0.1810  \n",
      "Epoch: [1][1300/12928] Data 0.000 (0.000) Elapsed 13m 11s (remain 117m 51s) Loss: 2.5467(2.9368) Encoder Grad: 0.0948  Decoder Grad: 0.1824  \n",
      "Epoch: [1][1350/12928] Data 0.000 (0.000) Elapsed 13m 41s (remain 117m 17s) Loss: 2.6057(2.9222) Encoder Grad: 0.0891  Decoder Grad: 0.2642  \n",
      "Epoch: [1][1400/12928] Data 0.000 (0.000) Elapsed 14m 12s (remain 116m 51s) Loss: 2.5581(2.9086) Encoder Grad: 0.1295  Decoder Grad: 0.1733  \n",
      "Epoch: [1][1450/12928] Data 0.000 (0.000) Elapsed 14m 42s (remain 116m 22s) Loss: 2.5300(2.8955) Encoder Grad: 0.0912  Decoder Grad: 0.1764  \n",
      "Epoch: [1][1500/12928] Data 0.000 (0.000) Elapsed 15m 13s (remain 115m 56s) Loss: 2.4771(2.8831) Encoder Grad: 0.2060  Decoder Grad: 0.1899  \n",
      "Epoch: [1][1550/12928] Data 0.000 (0.000) Elapsed 15m 44s (remain 115m 25s) Loss: 2.4630(2.8709) Encoder Grad: 0.0953  Decoder Grad: 0.1862  \n",
      "Epoch: [1][1600/12928] Data 0.000 (0.000) Elapsed 16m 15s (remain 114m 58s) Loss: 2.4219(2.8596) Encoder Grad: 0.1968  Decoder Grad: 0.2260  \n",
      "Epoch: [1][1650/12928] Data 0.000 (0.000) Elapsed 16m 45s (remain 114m 25s) Loss: 2.4948(2.8483) Encoder Grad: 0.1883  Decoder Grad: 0.1719  \n",
      "Epoch: [1][1700/12928] Data 0.000 (0.000) Elapsed 17m 15s (remain 113m 56s) Loss: 2.4624(2.8378) Encoder Grad: 0.1785  Decoder Grad: 0.1911  \n",
      "Epoch: [1][1750/12928] Data 0.000 (0.000) Elapsed 17m 46s (remain 113m 28s) Loss: 2.5222(2.8277) Encoder Grad: 0.1051  Decoder Grad: 0.1905  \n",
      "Epoch: [1][1800/12928] Data 0.000 (0.000) Elapsed 18m 18s (remain 113m 4s) Loss: 2.4625(2.8180) Encoder Grad: 0.1652  Decoder Grad: 0.2252  \n",
      "Epoch: [1][1850/12928] Data 0.000 (0.000) Elapsed 18m 49s (remain 112m 38s) Loss: 2.5218(2.8085) Encoder Grad: 0.1322  Decoder Grad: 0.1871  \n",
      "Epoch: [1][1900/12928] Data 0.000 (0.000) Elapsed 19m 19s (remain 112m 8s) Loss: 2.4322(2.7992) Encoder Grad: 0.0918  Decoder Grad: 0.1575  \n",
      "Epoch: [1][1950/12928] Data 0.000 (0.000) Elapsed 19m 50s (remain 111m 39s) Loss: 2.5451(2.7903) Encoder Grad: 0.0683  Decoder Grad: 0.2548  \n",
      "Epoch: [1][2000/12928] Data 0.000 (0.000) Elapsed 20m 21s (remain 111m 8s) Loss: 2.4600(2.7817) Encoder Grad: 0.0976  Decoder Grad: 0.1966  \n",
      "Epoch: [1][2050/12928] Data 0.000 (0.000) Elapsed 20m 52s (remain 110m 40s) Loss: 2.4947(2.7732) Encoder Grad: 0.0632  Decoder Grad: 0.2271  \n",
      "Epoch: [1][2100/12928] Data 0.000 (0.000) Elapsed 21m 22s (remain 110m 6s) Loss: 2.3805(2.7647) Encoder Grad: 0.1153  Decoder Grad: 0.1939  \n",
      "Epoch: [1][2150/12928] Data 0.000 (0.000) Elapsed 21m 52s (remain 109m 35s) Loss: 2.3853(2.7564) Encoder Grad: 0.0987  Decoder Grad: 0.1759  \n",
      "Epoch: [1][2200/12928] Data 0.000 (0.000) Elapsed 22m 24s (remain 109m 10s) Loss: 2.4096(2.7488) Encoder Grad: 0.1193  Decoder Grad: 0.1797  \n",
      "Epoch: [1][2250/12928] Data 0.000 (0.000) Elapsed 22m 54s (remain 108m 39s) Loss: 2.4500(2.7410) Encoder Grad: 0.0950  Decoder Grad: 0.1771  \n",
      "Epoch: [1][2300/12928] Data 0.000 (0.000) Elapsed 23m 25s (remain 108m 10s) Loss: 2.3774(2.7334) Encoder Grad: 0.0821  Decoder Grad: 0.1746  \n",
      "Epoch: [1][2350/12928] Data 0.000 (0.000) Elapsed 23m 55s (remain 107m 37s) Loss: 2.3872(2.7260) Encoder Grad: 0.1311  Decoder Grad: 0.1932  \n",
      "Epoch: [1][2400/12928] Data 0.000 (0.000) Elapsed 24m 26s (remain 107m 7s) Loss: 2.3590(2.7188) Encoder Grad: 0.0997  Decoder Grad: 0.1855  \n",
      "Epoch: [1][2450/12928] Data 0.000 (0.000) Elapsed 24m 56s (remain 106m 38s) Loss: 2.4767(2.7118) Encoder Grad: 0.0974  Decoder Grad: 0.1937  \n",
      "Epoch: [1][2500/12928] Data 0.000 (0.000) Elapsed 25m 26s (remain 106m 4s) Loss: 2.3375(2.7048) Encoder Grad: 0.1291  Decoder Grad: 0.1743  \n",
      "Epoch: [1][2550/12928] Data 0.000 (0.000) Elapsed 25m 57s (remain 105m 33s) Loss: 2.3671(2.6982) Encoder Grad: 0.1170  Decoder Grad: 0.2195  \n",
      "Epoch: [1][2600/12928] Data 0.000 (0.000) Elapsed 26m 27s (remain 105m 4s) Loss: 2.3531(2.6917) Encoder Grad: 0.0930  Decoder Grad: 0.1736  \n",
      "Epoch: [1][2650/12928] Data 0.002 (0.000) Elapsed 26m 58s (remain 104m 32s) Loss: 2.3950(2.6853) Encoder Grad: 0.1007  Decoder Grad: 0.1666  \n",
      "Epoch: [1][2700/12928] Data 0.000 (0.000) Elapsed 27m 28s (remain 104m 0s) Loss: 2.3004(2.6789) Encoder Grad: 0.0912  Decoder Grad: 0.1674  \n",
      "Epoch: [1][2750/12928] Data 0.000 (0.000) Elapsed 27m 58s (remain 103m 28s) Loss: 2.3416(2.6728) Encoder Grad: 0.1338  Decoder Grad: 0.1620  \n",
      "Epoch: [1][2800/12928] Data 0.000 (0.000) Elapsed 28m 28s (remain 102m 58s) Loss: 2.3968(2.6668) Encoder Grad: 0.0910  Decoder Grad: 0.2515  \n",
      "Epoch: [1][2850/12928] Data 0.000 (0.000) Elapsed 28m 59s (remain 102m 28s) Loss: 2.3497(2.6609) Encoder Grad: 0.1162  Decoder Grad: 0.2042  \n",
      "Epoch: [1][2900/12928] Data 0.000 (0.000) Elapsed 29m 29s (remain 101m 56s) Loss: 2.2702(2.6549) Encoder Grad: 0.1565  Decoder Grad: 0.2907  \n",
      "Epoch: [1][2950/12928] Data 0.000 (0.000) Elapsed 30m 0s (remain 101m 26s) Loss: 2.2934(2.6491) Encoder Grad: 0.1431  Decoder Grad: 0.2100  \n",
      "Epoch: [1][3000/12928] Data 0.000 (0.000) Elapsed 30m 30s (remain 100m 56s) Loss: 2.2649(2.6434) Encoder Grad: 0.1418  Decoder Grad: 0.2111  \n",
      "Epoch: [1][3050/12928] Data 0.000 (0.000) Elapsed 31m 1s (remain 100m 26s) Loss: 2.3067(2.6376) Encoder Grad: 0.1503  Decoder Grad: 0.1814  \n",
      "Epoch: [1][3100/12928] Data 0.000 (0.000) Elapsed 31m 31s (remain 99m 52s) Loss: 2.2273(2.6319) Encoder Grad: 0.1749  Decoder Grad: 0.2165  \n",
      "Epoch: [1][3150/12928] Data 0.000 (0.000) Elapsed 32m 1s (remain 99m 21s) Loss: 2.2512(2.6263) Encoder Grad: 0.1328  Decoder Grad: 0.2118  \n",
      "Epoch: [1][3200/12928] Data 0.000 (0.000) Elapsed 32m 31s (remain 98m 49s) Loss: 2.2049(2.6208) Encoder Grad: 0.0898  Decoder Grad: 0.1811  \n",
      "Epoch: [1][3300/12928] Data 0.000 (0.000) Elapsed 33m 32s (remain 97m 49s) Loss: 2.2616(2.6101) Encoder Grad: 0.0988  Decoder Grad: 0.1892  \n",
      "Epoch: [1][3350/12928] Data 0.000 (0.000) Elapsed 34m 3s (remain 97m 18s) Loss: 2.2925(2.6048) Encoder Grad: 0.1310  Decoder Grad: 0.1964  \n",
      "Epoch: [1][3400/12928] Data 0.000 (0.000) Elapsed 34m 34s (remain 96m 50s) Loss: 2.3114(2.5996) Encoder Grad: 0.1566  Decoder Grad: 0.2228  \n",
      "Epoch: [1][3450/12928] Data 0.000 (0.000) Elapsed 35m 4s (remain 96m 20s) Loss: 2.2052(2.5942) Encoder Grad: 0.1730  Decoder Grad: 0.2269  \n",
      "Epoch: [1][3500/12928] Data 0.000 (0.000) Elapsed 35m 35s (remain 95m 51s) Loss: 2.2325(2.5892) Encoder Grad: 0.2229  Decoder Grad: 0.2029  \n",
      "Epoch: [1][3550/12928] Data 0.000 (0.000) Elapsed 36m 6s (remain 95m 20s) Loss: 2.1842(2.5840) Encoder Grad: 0.1860  Decoder Grad: 0.2042  \n",
      "Epoch: [1][3600/12928] Data 0.000 (0.000) Elapsed 36m 36s (remain 94m 49s) Loss: 2.2000(2.5789) Encoder Grad: 0.1380  Decoder Grad: 0.2229  \n",
      "Epoch: [1][3650/12928] Data 0.000 (0.000) Elapsed 37m 7s (remain 94m 20s) Loss: 2.2295(2.5739) Encoder Grad: 0.2377  Decoder Grad: 0.1950  \n",
      "Epoch: [1][3700/12928] Data 0.000 (0.000) Elapsed 37m 38s (remain 93m 49s) Loss: 2.1087(2.5688) Encoder Grad: 0.2069  Decoder Grad: 0.2142  \n",
      "Epoch: [1][3750/12928] Data 0.000 (0.000) Elapsed 38m 9s (remain 93m 20s) Loss: 2.2706(2.5639) Encoder Grad: 0.1399  Decoder Grad: 0.2139  \n",
      "Epoch: [1][3800/12928] Data 0.000 (0.000) Elapsed 38m 39s (remain 92m 49s) Loss: 2.1698(2.5590) Encoder Grad: 0.1386  Decoder Grad: 0.2217  \n",
      "Epoch: [1][3850/12928] Data 0.000 (0.000) Elapsed 39m 9s (remain 92m 18s) Loss: 2.1008(2.5541) Encoder Grad: 0.2910  Decoder Grad: 0.2983  \n",
      "Epoch: [1][3900/12928] Data 0.000 (0.000) Elapsed 39m 40s (remain 91m 47s) Loss: 2.2121(2.5492) Encoder Grad: 0.1390  Decoder Grad: 0.2068  \n",
      "Epoch: [1][3950/12928] Data 0.000 (0.000) Elapsed 40m 11s (remain 91m 18s) Loss: 2.2902(2.5445) Encoder Grad: 0.1496  Decoder Grad: 0.2125  \n",
      "Epoch: [1][4000/12928] Data 0.000 (0.000) Elapsed 40m 41s (remain 90m 46s) Loss: 2.2010(2.5398) Encoder Grad: 0.1458  Decoder Grad: 0.1748  \n",
      "Epoch: [1][4050/12928] Data 0.000 (0.000) Elapsed 41m 11s (remain 90m 16s) Loss: 2.1526(2.5352) Encoder Grad: 0.1808  Decoder Grad: 0.2169  \n",
      "Epoch: [1][4100/12928] Data 0.000 (0.000) Elapsed 41m 41s (remain 89m 44s) Loss: 2.2088(2.5306) Encoder Grad: 0.1998  Decoder Grad: 0.2321  \n",
      "Epoch: [1][4150/12928] Data 0.000 (0.000) Elapsed 42m 12s (remain 89m 14s) Loss: 2.1570(2.5262) Encoder Grad: 0.0906  Decoder Grad: 0.2586  \n",
      "Epoch: [1][4200/12928] Data 0.000 (0.000) Elapsed 42m 42s (remain 88m 42s) Loss: 2.0985(2.5216) Encoder Grad: 0.1128  Decoder Grad: 0.2161  \n",
      "Epoch: [1][4250/12928] Data 0.000 (0.000) Elapsed 43m 13s (remain 88m 13s) Loss: 2.1563(2.5172) Encoder Grad: 0.1316  Decoder Grad: 0.2001  \n",
      "Epoch: [1][4300/12928] Data 0.000 (0.000) Elapsed 43m 43s (remain 87m 42s) Loss: 2.1281(2.5127) Encoder Grad: 0.2552  Decoder Grad: 0.2553  \n",
      "Epoch: [1][4350/12928] Data 0.000 (0.000) Elapsed 44m 14s (remain 87m 11s) Loss: 2.0849(2.5083) Encoder Grad: 0.0856  Decoder Grad: 0.1738  \n",
      "Epoch: [1][4400/12928] Data 0.000 (0.000) Elapsed 44m 44s (remain 86m 41s) Loss: 2.2675(2.5039) Encoder Grad: 0.1700  Decoder Grad: 0.2885  \n",
      "Epoch: [1][4450/12928] Data 0.000 (0.000) Elapsed 45m 15s (remain 86m 11s) Loss: 2.1042(2.4997) Encoder Grad: 0.3639  Decoder Grad: 0.2596  \n",
      "Epoch: [1][4500/12928] Data 0.000 (0.000) Elapsed 45m 46s (remain 85m 42s) Loss: 2.1244(2.4955) Encoder Grad: 0.1447  Decoder Grad: 0.1944  \n",
      "Epoch: [1][4550/12928] Data 0.000 (0.000) Elapsed 46m 17s (remain 85m 12s) Loss: 2.0731(2.4913) Encoder Grad: 0.2605  Decoder Grad: 0.2149  \n",
      "Epoch: [1][4600/12928] Data 0.000 (0.000) Elapsed 46m 48s (remain 84m 42s) Loss: 2.1961(2.4872) Encoder Grad: 0.4339  Decoder Grad: 0.2275  \n",
      "Epoch: [1][4650/12928] Data 0.000 (0.000) Elapsed 47m 18s (remain 84m 12s) Loss: 2.1383(2.4831) Encoder Grad: 0.1673  Decoder Grad: 0.2300  \n",
      "Epoch: [1][4700/12928] Data 0.000 (0.000) Elapsed 47m 49s (remain 83m 41s) Loss: 2.0373(2.4789) Encoder Grad: 0.1491  Decoder Grad: 0.2658  \n",
      "Epoch: [1][4750/12928] Data 0.000 (0.000) Elapsed 48m 19s (remain 83m 9s) Loss: 2.1304(2.4747) Encoder Grad: 0.1377  Decoder Grad: 0.2572  \n",
      "Epoch: [1][4800/12928] Data 0.000 (0.000) Elapsed 48m 49s (remain 82m 39s) Loss: 2.0772(2.4707) Encoder Grad: 0.1389  Decoder Grad: 0.1996  \n",
      "Epoch: [1][4850/12928] Data 0.000 (0.000) Elapsed 49m 19s (remain 82m 8s) Loss: 2.0306(2.4666) Encoder Grad: 0.2003  Decoder Grad: 0.2185  \n",
      "Epoch: [1][4900/12928] Data 0.000 (0.000) Elapsed 49m 49s (remain 81m 36s) Loss: 2.0227(2.4625) Encoder Grad: 0.3111  Decoder Grad: 0.2302  \n",
      "Epoch: [1][4950/12928] Data 0.000 (0.000) Elapsed 50m 20s (remain 81m 6s) Loss: 2.0852(2.4585) Encoder Grad: 0.1430  Decoder Grad: 0.3112  \n",
      "Epoch: [1][5000/12928] Data 0.000 (0.000) Elapsed 50m 50s (remain 80m 35s) Loss: 2.0558(2.4545) Encoder Grad: 0.3507  Decoder Grad: 0.2354  \n",
      "Epoch: [1][5050/12928] Data 0.000 (0.000) Elapsed 51m 21s (remain 80m 5s) Loss: 2.1047(2.4506) Encoder Grad: 0.1068  Decoder Grad: 0.2503  \n",
      "Epoch: [1][5100/12928] Data 0.000 (0.000) Elapsed 51m 52s (remain 79m 35s) Loss: 2.1214(2.4467) Encoder Grad: 0.1405  Decoder Grad: 0.2054  \n",
      "Epoch: [1][5150/12928] Data 0.000 (0.000) Elapsed 52m 23s (remain 79m 5s) Loss: 2.1139(2.4427) Encoder Grad: 0.1278  Decoder Grad: 0.2066  \n",
      "Epoch: [1][5200/12928] Data 0.000 (0.000) Elapsed 52m 54s (remain 78m 35s) Loss: 2.0300(2.4388) Encoder Grad: 0.1227  Decoder Grad: 0.2713  \n",
      "Epoch: [1][5250/12928] Data 0.000 (0.000) Elapsed 53m 25s (remain 78m 5s) Loss: 1.9742(2.4348) Encoder Grad: 0.1077  Decoder Grad: 0.2496  \n",
      "Epoch: [1][5300/12928] Data 0.000 (0.000) Elapsed 53m 56s (remain 77m 36s) Loss: 1.9499(2.4309) Encoder Grad: 0.1580  Decoder Grad: 0.2402  \n",
      "Epoch: [1][5350/12928] Data 0.000 (0.000) Elapsed 54m 27s (remain 77m 6s) Loss: 2.0634(2.4270) Encoder Grad: 0.1039  Decoder Grad: 0.2578  \n",
      "Epoch: [1][5400/12928] Data 0.000 (0.000) Elapsed 54m 57s (remain 76m 36s) Loss: 1.9700(2.4231) Encoder Grad: 0.1653  Decoder Grad: 0.2870  \n",
      "Epoch: [1][5450/12928] Data 0.000 (0.000) Elapsed 55m 28s (remain 76m 6s) Loss: 1.9393(2.4191) Encoder Grad: 0.2363  Decoder Grad: 0.3239  \n",
      "Epoch: [1][5500/12928] Data 0.000 (0.000) Elapsed 55m 59s (remain 75m 35s) Loss: 1.9970(2.4151) Encoder Grad: 0.1249  Decoder Grad: 0.2398  \n",
      "Epoch: [1][5550/12928] Data 0.000 (0.000) Elapsed 56m 30s (remain 75m 5s) Loss: 1.9242(2.4111) Encoder Grad: 0.2586  Decoder Grad: 0.3096  \n",
      "Epoch: [1][5600/12928] Data 0.000 (0.000) Elapsed 57m 1s (remain 74m 35s) Loss: 1.9925(2.4071) Encoder Grad: 0.1459  Decoder Grad: 0.2857  \n",
      "Epoch: [1][5650/12928] Data 0.000 (0.000) Elapsed 57m 30s (remain 74m 3s) Loss: 1.9770(2.4032) Encoder Grad: 0.1652  Decoder Grad: 0.2906  \n",
      "Epoch: [1][5700/12928] Data 0.000 (0.000) Elapsed 58m 1s (remain 73m 32s) Loss: 1.9326(2.3992) Encoder Grad: 0.1640  Decoder Grad: 0.3093  \n",
      "Epoch: [1][5750/12928] Data 0.000 (0.000) Elapsed 58m 32s (remain 73m 3s) Loss: 1.9978(2.3954) Encoder Grad: 0.1911  Decoder Grad: 0.3052  \n",
      "Epoch: [1][5800/12928] Data 0.000 (0.000) Elapsed 59m 3s (remain 72m 33s) Loss: 1.9161(2.3915) Encoder Grad: 0.1517  Decoder Grad: 0.3048  \n",
      "Epoch: [1][5850/12928] Data 0.000 (0.000) Elapsed 59m 34s (remain 72m 3s) Loss: 1.8969(2.3878) Encoder Grad: 0.1565  Decoder Grad: 0.2599  \n",
      "Epoch: [1][5900/12928] Data 0.000 (0.000) Elapsed 60m 4s (remain 71m 32s) Loss: 1.9199(2.3839) Encoder Grad: 0.1500  Decoder Grad: 0.2812  \n",
      "Epoch: [1][5950/12928] Data 0.000 (0.000) Elapsed 60m 35s (remain 71m 2s) Loss: 1.8676(2.3801) Encoder Grad: 0.1707  Decoder Grad: 0.3006  \n",
      "Epoch: [1][6000/12928] Data 0.000 (0.000) Elapsed 61m 6s (remain 70m 31s) Loss: 1.9092(2.3763) Encoder Grad: 0.1767  Decoder Grad: 0.3594  \n",
      "Epoch: [1][6050/12928] Data 0.000 (0.000) Elapsed 61m 36s (remain 70m 1s) Loss: 1.9674(2.3726) Encoder Grad: 0.3205  Decoder Grad: 0.3134  \n",
      "Epoch: [1][6100/12928] Data 0.000 (0.000) Elapsed 62m 7s (remain 69m 30s) Loss: 2.0853(2.3690) Encoder Grad: 0.1660  Decoder Grad: 0.3326  \n",
      "Epoch: [1][6150/12928] Data 0.000 (0.000) Elapsed 62m 38s (remain 69m 1s) Loss: 1.9407(2.3654) Encoder Grad: 0.3148  Decoder Grad: 0.8361  \n",
      "Epoch: [1][6200/12928] Data 0.000 (0.000) Elapsed 63m 9s (remain 68m 30s) Loss: 1.9219(2.3617) Encoder Grad: 0.1714  Decoder Grad: 0.3273  \n",
      "Epoch: [1][6250/12928] Data 0.000 (0.000) Elapsed 63m 40s (remain 68m 0s) Loss: 1.9393(2.3581) Encoder Grad: 0.2154  Decoder Grad: 0.6702  \n",
      "Epoch: [1][6300/12928] Data 0.000 (0.000) Elapsed 64m 10s (remain 67m 30s) Loss: 1.9313(2.3546) Encoder Grad: 0.1468  Decoder Grad: 0.5756  \n",
      "Epoch: [1][6350/12928] Data 0.000 (0.000) Elapsed 64m 42s (remain 67m 0s) Loss: 1.9188(2.3512) Encoder Grad: 0.2083  Decoder Grad: 0.3128  \n",
      "Epoch: [1][6400/12928] Data 0.000 (0.000) Elapsed 65m 13s (remain 66m 30s) Loss: 1.9211(2.3476) Encoder Grad: 0.1636  Decoder Grad: 0.3374  \n",
      "Epoch: [1][6450/12928] Data 0.000 (0.000) Elapsed 65m 44s (remain 66m 0s) Loss: 1.8971(2.3442) Encoder Grad: 0.1364  Decoder Grad: 0.2254  \n",
      "Epoch: [1][6500/12928] Data 0.000 (0.000) Elapsed 66m 15s (remain 65m 30s) Loss: 1.9038(2.3408) Encoder Grad: 0.2929  Decoder Grad: 1.0633  \n",
      "Epoch: [1][6550/12928] Data 0.000 (0.000) Elapsed 66m 46s (remain 64m 59s) Loss: 1.8387(2.3374) Encoder Grad: 0.1921  Decoder Grad: 0.2526  \n",
      "Epoch: [1][6600/12928] Data 0.000 (0.000) Elapsed 67m 16s (remain 64m 28s) Loss: 1.9181(2.3340) Encoder Grad: 0.1935  Decoder Grad: 0.4272  \n",
      "Epoch: [1][6650/12928] Data 0.000 (0.000) Elapsed 67m 47s (remain 63m 58s) Loss: 1.8491(2.3306) Encoder Grad: 0.1321  Decoder Grad: 0.2460  \n",
      "Epoch: [1][6700/12928] Data 0.000 (0.000) Elapsed 68m 17s (remain 63m 27s) Loss: 1.9009(2.3272) Encoder Grad: 0.1412  Decoder Grad: 0.2622  \n",
      "Epoch: [1][6750/12928] Data 0.000 (0.000) Elapsed 68m 48s (remain 62m 57s) Loss: 1.9201(2.3241) Encoder Grad: 0.1781  Decoder Grad: 0.2874  \n",
      "Epoch: [1][6800/12928] Data 0.000 (0.000) Elapsed 69m 19s (remain 62m 27s) Loss: 1.8645(2.3209) Encoder Grad: 0.1840  Decoder Grad: 0.2366  \n",
      "Epoch: [1][6850/12928] Data 0.000 (0.000) Elapsed 69m 50s (remain 61m 56s) Loss: 1.8770(2.3176) Encoder Grad: 0.1595  Decoder Grad: 0.5422  \n",
      "Epoch: [1][6900/12928] Data 0.000 (0.000) Elapsed 70m 20s (remain 61m 26s) Loss: 1.9197(2.3144) Encoder Grad: 0.1758  Decoder Grad: 0.2383  \n",
      "Epoch: [1][6950/12928] Data 0.000 (0.000) Elapsed 70m 51s (remain 60m 55s) Loss: 1.8498(2.3112) Encoder Grad: 0.1842  Decoder Grad: 0.6108  \n",
      "Epoch: [1][7000/12928] Data 0.000 (0.000) Elapsed 71m 21s (remain 60m 24s) Loss: 1.8386(2.3080) Encoder Grad: 0.1912  Decoder Grad: 0.3845  \n",
      "Epoch: [1][7050/12928] Data 0.000 (0.000) Elapsed 71m 52s (remain 59m 54s) Loss: 1.9063(2.3048) Encoder Grad: 0.2455  Decoder Grad: 0.4308  \n",
      "Epoch: [1][7100/12928] Data 0.000 (0.000) Elapsed 72m 22s (remain 59m 23s) Loss: 1.7886(2.3018) Encoder Grad: 0.1551  Decoder Grad: 0.3461  \n",
      "Epoch: [1][7150/12928] Data 0.000 (0.000) Elapsed 72m 53s (remain 58m 53s) Loss: 1.8754(2.2989) Encoder Grad: 0.1468  Decoder Grad: 0.5187  \n",
      "Epoch: [1][7200/12928] Data 0.000 (0.000) Elapsed 73m 23s (remain 58m 22s) Loss: 1.8568(2.2960) Encoder Grad: 0.2199  Decoder Grad: 0.2816  \n",
      "Epoch: [1][7250/12928] Data 0.000 (0.000) Elapsed 73m 54s (remain 57m 51s) Loss: 1.8479(2.2929) Encoder Grad: 0.2073  Decoder Grad: 0.2624  \n",
      "Epoch: [1][7300/12928] Data 0.000 (0.000) Elapsed 74m 25s (remain 57m 21s) Loss: 1.8191(2.2899) Encoder Grad: 0.1092  Decoder Grad: 0.2708  \n",
      "Epoch: [1][7350/12928] Data 0.000 (0.000) Elapsed 74m 55s (remain 56m 50s) Loss: 1.8722(2.2871) Encoder Grad: 0.1290  Decoder Grad: 0.2770  \n",
      "Epoch: [1][7400/12928] Data 0.000 (0.000) Elapsed 75m 26s (remain 56m 20s) Loss: 1.8604(2.2842) Encoder Grad: 0.1479  Decoder Grad: 0.2587  \n",
      "Epoch: [1][7450/12928] Data 0.000 (0.000) Elapsed 75m 57s (remain 55m 49s) Loss: 1.8041(2.2812) Encoder Grad: 0.2011  Decoder Grad: 0.2097  \n",
      "Epoch: [1][7500/12928] Data 0.000 (0.000) Elapsed 76m 28s (remain 55m 19s) Loss: 1.8773(2.2783) Encoder Grad: 0.2202  Decoder Grad: 0.5407  \n",
      "Epoch: [1][7550/12928] Data 0.000 (0.000) Elapsed 76m 58s (remain 54m 48s) Loss: 1.8013(2.2753) Encoder Grad: 0.3188  Decoder Grad: 0.4026  \n",
      "Epoch: [1][7600/12928] Data 0.000 (0.000) Elapsed 77m 28s (remain 54m 17s) Loss: 1.8947(2.2725) Encoder Grad: 0.1724  Decoder Grad: 0.3913  \n",
      "Epoch: [1][7650/12928] Data 0.000 (0.000) Elapsed 77m 59s (remain 53m 47s) Loss: 1.8203(2.2696) Encoder Grad: 0.1947  Decoder Grad: 0.2665  \n",
      "Epoch: [1][7700/12928] Data 0.000 (0.000) Elapsed 78m 30s (remain 53m 17s) Loss: 1.8084(2.2668) Encoder Grad: 0.1799  Decoder Grad: 0.4433  \n",
      "Epoch: [1][7750/12928] Data 0.000 (0.000) Elapsed 79m 1s (remain 52m 46s) Loss: 1.8710(2.2640) Encoder Grad: 0.2542  Decoder Grad: 0.4024  \n",
      "Epoch: [1][7800/12928] Data 0.000 (0.000) Elapsed 79m 31s (remain 52m 16s) Loss: 1.7861(2.2613) Encoder Grad: 0.1649  Decoder Grad: 0.2924  \n",
      "Epoch: [1][7850/12928] Data 0.000 (0.000) Elapsed 80m 2s (remain 51m 45s) Loss: 1.8698(2.2586) Encoder Grad: 0.3868  Decoder Grad: 0.3141  \n",
      "Epoch: [1][7900/12928] Data 0.000 (0.000) Elapsed 80m 33s (remain 51m 15s) Loss: 1.8681(2.2559) Encoder Grad: 0.1946  Decoder Grad: 0.2382  \n",
      "Epoch: [1][7950/12928] Data 0.000 (0.000) Elapsed 81m 4s (remain 50m 44s) Loss: 1.8651(2.2531) Encoder Grad: 0.1687  Decoder Grad: 0.3628  \n",
      "Epoch: [1][8000/12928] Data 0.000 (0.000) Elapsed 81m 35s (remain 50m 14s) Loss: 1.9271(2.2504) Encoder Grad: 0.2760  Decoder Grad: 0.5174  \n",
      "Epoch: [1][8050/12928] Data 0.000 (0.000) Elapsed 82m 6s (remain 49m 44s) Loss: 1.9017(2.2478) Encoder Grad: 0.1685  Decoder Grad: 0.5101  \n",
      "Epoch: [1][8100/12928] Data 0.000 (0.000) Elapsed 82m 36s (remain 49m 13s) Loss: 1.8171(2.2452) Encoder Grad: 0.1992  Decoder Grad: 0.2753  \n",
      "Epoch: [1][8150/12928] Data 0.000 (0.000) Elapsed 83m 7s (remain 48m 42s) Loss: 1.8090(2.2425) Encoder Grad: 0.2069  Decoder Grad: 0.5661  \n",
      "Epoch: [1][8200/12928] Data 0.000 (0.000) Elapsed 83m 38s (remain 48m 12s) Loss: 1.8413(2.2399) Encoder Grad: 0.2705  Decoder Grad: 0.2883  \n",
      "Epoch: [1][8250/12928] Data 0.000 (0.000) Elapsed 84m 8s (remain 47m 41s) Loss: 1.8568(2.2374) Encoder Grad: 0.1461  Decoder Grad: 0.2246  \n",
      "Epoch: [1][8300/12928] Data 0.000 (0.000) Elapsed 84m 38s (remain 47m 10s) Loss: 1.7973(2.2349) Encoder Grad: 0.1797  Decoder Grad: 0.3339  \n",
      "Epoch: [1][8350/12928] Data 0.000 (0.000) Elapsed 85m 9s (remain 46m 40s) Loss: 1.8228(2.2324) Encoder Grad: 0.2315  Decoder Grad: 0.2212  \n",
      "Epoch: [1][8400/12928] Data 0.000 (0.000) Elapsed 85m 39s (remain 46m 9s) Loss: 1.8774(2.2298) Encoder Grad: 0.2167  Decoder Grad: 0.2990  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "#     flags = {}\n",
    "#     xmp.spawn(_mp_fn, args = (flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xm.get_xla_supported_devices()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
