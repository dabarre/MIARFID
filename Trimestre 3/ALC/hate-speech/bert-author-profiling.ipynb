{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Concatenate\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### AUTHOR PROFILING FUNCTION FOR JOINING PREDICTIONS #########\n",
    "\n",
    "# de 20 0s y 20 1s\n",
    "\n",
    "def author_profiling_report(author_profile, number_authors=40):\n",
    "    n = int(number_authors/2)\n",
    "    a = np.zeros(n)\n",
    "    b = np.ones(n)        \n",
    "    author_profile = np.concatenate([a,b])\n",
    "    \n",
    "    # Check author profiling -> 8,000 predictions\n",
    "    # Split into 40 authors -> 200 tweets per author\n",
    "    author_predictions = np.average(np.array_split(predictions, number_authors), axis=1)\n",
    "    author_predictions = np.array([1 if ap >= 0.5 else 0 for ap in author_predictions])\n",
    "\n",
    "    print(classification_report(author_profile, author_predictions, labels=[0, 1], target_names=['not hate','hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_path = '/kaggle/input/bert-preprocesed-author-profiling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(kaggle_path+'es_indv_bert.pickle', 'rb')\n",
    "es_indv_bert = pickle.load(pickle_file)\n",
    "\n",
    "train_padded_es_indv_bert, train_mask_es_indv, y_train_es_indv = es_indv_bert[0], es_indv_bert[1], es_indv_bert[2]\n",
    "valid_padded_es_indv_bert, valid_mask_es_indv, y_valid_es_indv = es_indv_bert[3], es_indv_bert[4], es_indv_bert[5]\n",
    "test_padded_es_indv_bert, test_mask_es_indv = es_indv_bert[6], es_indv_bert[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(kaggle_path+'es_20_bert.pickle', 'rb')\n",
    "es_20_bert = pickle.load(pickle_file)\n",
    "\n",
    "train_padded_es_20_bert, train_mask_es_20, y_train_es_20 = es_20_bert[0], es_20_bert[1], es_20_bert[2]\n",
    "valid_padded_es_20_bert, valid_mask_es_20, y_valid_es_20 = es_20_bert[3], es_20_bert[4], es_20_bert[5]\n",
    "test_padded_es_20_bert, test_mask_es_20 = es_20_bert[6], es_20_bert[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  769       \n",
      "=================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  ES individual tweets\n",
    "\n",
    "bert_model1 = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "bert_model1.compile(loss='binary_crossentropy',\n",
    "                   optimizer=Adam(learning_rate=2e-5,epsilon=1e-08),\n",
    "                   metrics=['accuracy'])\n",
    "bert_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 73s 252ms/step - loss: 1.0463 - accuracy: 0.5112 - val_loss: 0.6825 - val_accuracy: 0.5576\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 60s 242ms/step - loss: 0.6640 - accuracy: 0.6043 - val_loss: 0.6668 - val_accuracy: 0.6085\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.6242 - accuracy: 0.6541 - val_loss: 0.6855 - val_accuracy: 0.6036\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6083 - accuracy: 0.6777 - val_loss: 0.6687 - val_accuracy: 0.6190\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.6388 - accuracy: 0.6273 - val_loss: 0.6496 - val_accuracy: 0.6274\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.5627 - accuracy: 0.7218 - val_loss: 0.7024 - val_accuracy: 0.6223\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.5904 - accuracy: 0.6860 - val_loss: 0.6554 - val_accuracy: 0.6260\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.5608 - accuracy: 0.7284 - val_loss: 0.8931 - val_accuracy: 0.6127\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 60s 242ms/step - loss: 0.5646 - accuracy: 0.7177 - val_loss: 0.7319 - val_accuracy: 0.6071\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.5163 - accuracy: 0.7473 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6252 - accuracy: 0.6584 - val_loss: 0.9862 - val_accuracy: 0.6053\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.4818 - accuracy: 0.8007 - val_loss: 0.7788 - val_accuracy: 0.5990\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4425 - accuracy: 0.8233 - val_loss: 0.8929 - val_accuracy: 0.5971\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.4550 - accuracy: 0.8138 - val_loss: 0.8279 - val_accuracy: 0.5878\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4247 - accuracy: 0.8232 - val_loss: 1.3137 - val_accuracy: 0.6037\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.3812 - accuracy: 0.8482 - val_loss: 1.0438 - val_accuracy: 0.5878\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.4362 - accuracy: 0.8239 - val_loss: 1.4000 - val_accuracy: 0.6069\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.3676 - accuracy: 0.8726 - val_loss: 2.0763 - val_accuracy: 0.5987\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.3059 - accuracy: 0.8890 - val_loss: 1.9159 - val_accuracy: 0.5997\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.3438 - accuracy: 0.8733 - val_loss: 0.9217 - val_accuracy: 0.5819\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.3250 - accuracy: 0.8815 - val_loss: 1.2028 - val_accuracy: 0.5919\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.3128 - accuracy: 0.8911 - val_loss: 1.6738 - val_accuracy: 0.6003\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.2684 - accuracy: 0.9063 - val_loss: 1.7225 - val_accuracy: 0.5947\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.2575 - accuracy: 0.9145 - val_loss: 2.8032 - val_accuracy: 0.5986\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.2278 - accuracy: 0.9288 - val_loss: 3.1312 - val_accuracy: 0.5804\n"
     ]
    }
   ],
   "source": [
    "bert_model1.fit(x=[train_padded_es_indv_bert, train_mask_es_indv],\n",
    "               y=y_train_es_indv,\n",
    "               batch_size=128,\n",
    "               epochs=25,\n",
    "               validation_data=([valid_padded_es_indv_bert, valid_mask_es_indv], y_valid_es_indv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = bert_model1.predict([valid_padded_es_indv_bert, valid_mask_es_indv], batch_size=128)\n",
    "#author_profiling_report(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not hate       1.00      0.10      0.18        20\n",
      "        hate       0.53      1.00      0.69        20\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.76      0.55      0.44        40\n",
      "weighted avg       0.76      0.55      0.44        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_authors = 40\n",
    "n = int(number_authors/2)\n",
    "a = np.zeros(n)\n",
    "b = np.ones(n)        \n",
    "author_profile = np.concatenate([a,b])\n",
    "\n",
    "p = []\n",
    "for logit in predictions1.logits:\n",
    "    p.append(math.exp(logit)/(1+math.exp(logit)))\n",
    "np.array(p)\n",
    "\n",
    "# Check author profiling -> 8,000 predictions\n",
    "# Split into 40 authors -> 200 tweets per author\n",
    "author_predictions = np.average(np.array_split(p, number_authors), axis=1)\n",
    "author_predictions = np.array([1 if ap >= 0.5 else 0 for ap in author_predictions])\n",
    "\n",
    "print(classification_report(author_profile, author_predictions, labels=[0, 1], target_names=['not hate','hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_227 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  769       \n",
      "=================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  ES individual tweets\n",
    "\n",
    "bert_model2 = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "bert_model2.compile(loss='binary_crossentropy',\n",
    "                   optimizer=Adam(learning_rate=6e-5,epsilon=1e-08),\n",
    "                   metrics=['accuracy'])\n",
    "bert_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 72s 252ms/step - loss: 0.7012 - accuracy: 0.5637 - val_loss: 0.6657 - val_accuracy: 0.5954\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6512 - accuracy: 0.6386 - val_loss: 0.6914 - val_accuracy: 0.6064\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6123 - accuracy: 0.6793 - val_loss: 0.6758 - val_accuracy: 0.6236\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6076 - accuracy: 0.6769 - val_loss: 0.6916 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6306 - accuracy: 0.6300 - val_loss: 0.6915 - val_accuracy: 0.5359\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6024 - accuracy: 0.6729 - val_loss: 0.6921 - val_accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.5898 - accuracy: 0.6750 - val_loss: 0.7024 - val_accuracy: 0.6034\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6856 - accuracy: 0.5719 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.6358 - accuracy: 0.6115 - val_loss: 0.9551 - val_accuracy: 0.6004\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4519 - accuracy: 0.8373 - val_loss: 0.6933 - val_accuracy: 0.5004\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6819 - accuracy: 0.5858 - val_loss: 0.9974 - val_accuracy: 0.6046\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4883 - accuracy: 0.8039 - val_loss: 1.7158 - val_accuracy: 0.6014\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4013 - accuracy: 0.8481 - val_loss: 1.0468 - val_accuracy: 0.5941\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.5629 - accuracy: 0.7280 - val_loss: 1.1515 - val_accuracy: 0.5930\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4194 - accuracy: 0.8498 - val_loss: 1.6405 - val_accuracy: 0.6070\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4481 - accuracy: 0.8222 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6984 - accuracy: 0.5037 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.6977 - accuracy: 0.5031 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 60s 242ms/step - loss: 0.6951 - accuracy: 0.5085 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.6851 - accuracy: 0.5451 - val_loss: 0.7794 - val_accuracy: 0.5962\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4618 - accuracy: 0.8056 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6965 - accuracy: 0.5010 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6962 - accuracy: 0.5029 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6957 - accuracy: 0.5021 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 60s 242ms/step - loss: 0.6963 - accuracy: 0.4953 - val_loss: 0.6956 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "bert_model2.fit(x=[train_padded_es_indv_bert, train_mask_es_indv],\n",
    "               y=y_train_es_indv,\n",
    "               batch_size=128,\n",
    "               epochs=25,\n",
    "               validation_data=([valid_padded_es_indv_bert, valid_mask_es_indv], y_valid_es_indv))\n",
    "\n",
    "predictions = bert_model2.predict([valid_padded_es_indv_bert, valid_mask_es_indv], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not hate       0.00      0.00      0.00        20\n",
      "        hate       0.50      1.00      0.67        20\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.25      0.50      0.33        40\n",
      "weighted avg       0.25      0.50      0.33        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions2 = bert_model2.predict([valid_padded_es_indv_bert, valid_mask_es_indv], batch_size=128)\n",
    "p = []\n",
    "for logit in predictions2.logits:\n",
    "    p.append(math.exp(logit)/(1+math.exp(logit)))\n",
    "np.array(p)\n",
    "\n",
    "# Check author profiling -> 8,000 predictions\n",
    "# Split into 40 authors -> 200 tweets per author\n",
    "author_predictions = np.average(np.array_split(p, number_authors), axis=1)\n",
    "author_predictions = np.array([1 if ap >= 0.5 else 0 for ap in author_predictions])\n",
    "\n",
    "print(classification_report(author_profile, author_predictions, labels=[0, 1], target_names=['not hate','hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_265 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  769       \n",
      "=================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  ES individual tweets\n",
    "\n",
    "bert_model3 = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "bert_model3.compile(loss='binary_crossentropy',\n",
    "                   optimizer=Adam(learning_rate=1e-4,epsilon=1e-08),\n",
    "                   metrics=['accuracy'])\n",
    "bert_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 72s 252ms/step - loss: 1.0102 - accuracy: 0.5300 - val_loss: 0.6993 - val_accuracy: 0.5766\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.6803 - accuracy: 0.5901 - val_loss: 0.6746 - val_accuracy: 0.6034\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.8167 - accuracy: 0.6388 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5998 - accuracy: 0.5016 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6451 - accuracy: 0.4987 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5789 - accuracy: 0.5030 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6151 - accuracy: 0.5006 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6059 - accuracy: 0.5012 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6141 - accuracy: 0.5007 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6302 - accuracy: 0.4996 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5451 - accuracy: 0.5052 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5804 - accuracy: 0.5029 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 7.6342 - accuracy: 0.4994 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6093 - accuracy: 0.5010 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5335 - accuracy: 0.5060 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6566 - accuracy: 0.4979 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6025 - accuracy: 0.5014 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 7.5730 - accuracy: 0.5034 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5661 - accuracy: 0.5038 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 60s 242ms/step - loss: 7.6519 - accuracy: 0.4982 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6176 - accuracy: 0.5005 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6542 - accuracy: 0.4981 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 7.6740 - accuracy: 0.4968 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.5810 - accuracy: 0.5029 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 7.6205 - accuracy: 0.5003 - val_loss: 7.6246 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "bert_model3.fit(x=[train_padded_es_indv_bert, train_mask_es_indv],\n",
    "               y=y_train_es_indv,\n",
    "               batch_size=128,\n",
    "               epochs=25,\n",
    "               validation_data=([valid_padded_es_indv_bert, valid_mask_es_indv], y_valid_es_indv))\n",
    "\n",
    "predictions = bert_model3.predict([valid_padded_es_indv_bert, valid_mask_es_indv], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not hate       0.00      0.00      0.00        20\n",
      "        hate       0.50      1.00      0.67        20\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.25      0.50      0.33        40\n",
      "weighted avg       0.25      0.50      0.33        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions3 = bert_model3.predict([valid_padded_es_indv_bert, valid_mask_es_indv], batch_size=128)\n",
    "p = []\n",
    "for logit in predictions3.logits:\n",
    "    p.append(math.exp(logit)/(1+math.exp(logit)))\n",
    "np.array(p)\n",
    "\n",
    "# Check author profiling -> 8,000 predictions\n",
    "# Split into 40 authors -> 200 tweets per author\n",
    "author_predictions = np.average(np.array_split(p, number_authors), axis=1)\n",
    "author_predictions = np.array([1 if ap >= 0.5 else 0 for ap in author_predictions])\n",
    "\n",
    "print(classification_report(author_profile, author_predictions, labels=[0, 1], target_names=['not hate','hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_303 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  769       \n",
      "=================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ES joined 20 tweets\n",
    "\n",
    "bert_model4 = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "bert_model4.compile(loss='binary_crossentropy',\n",
    "                   optimizer=Adam(learning_rate=2e-5,epsilon=1e-08),\n",
    "                   metrics=['accuracy'])\n",
    "bert_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2475/2475 [==============================] - 1176s 475ms/step - loss: 0.6538 - accuracy: 0.6211 - val_loss: 0.5457 - val_accuracy: 0.7400\n",
      "Epoch 2/5\n",
      "2475/2475 [==============================] - 1174s 474ms/step - loss: 0.6127 - accuracy: 0.6718 - val_loss: 0.5542 - val_accuracy: 0.7375\n",
      "Epoch 3/5\n",
      "2475/2475 [==============================] - 1174s 474ms/step - loss: 0.6189 - accuracy: 0.6669 - val_loss: 0.6244 - val_accuracy: 0.6750\n",
      "Epoch 4/5\n",
      "2475/2475 [==============================] - 1174s 474ms/step - loss: 0.5804 - accuracy: 0.7136 - val_loss: 0.5447 - val_accuracy: 0.7750\n",
      "Epoch 5/5\n",
      "2475/2475 [==============================] - 1174s 474ms/step - loss: 0.5396 - accuracy: 0.7442 - val_loss: 1.0081 - val_accuracy: 0.6775\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis1: axis 0 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-f7f097bdb51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_padded_es_20_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask_es_20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mauthor_profiling_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-c7e1d2b03e0e>\u001b[0m in \u001b[0;36mauthor_profiling_report\u001b[0;34m(author_profile, number_authors)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Check author profiling -> 8,000 predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Split into 40 authors -> 200 tweets per author\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mauthor_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_authors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mauthor_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0map\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0map\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthor_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0msub_arys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0msary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNsections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(a, axis1, axis2)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \"\"\"\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'swapaxes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis1: axis 0 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "bert_model4.fit(x=[train_padded_es_20_bert, train_mask_es_20],\n",
    "               y=y_train_es_20,\n",
    "               batch_size=16,\n",
    "               epochs=5,\n",
    "               validation_data=([valid_padded_es_20_bert, valid_mask_es_20], y_valid_es_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not hate       0.00      0.00      0.00        20\n",
      "        hate       0.50      1.00      0.67        20\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.25      0.50      0.33        40\n",
      "weighted avg       0.25      0.50      0.33        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions4= bert_model4.predict([test_padded_es_20_bert, test_mask_es_20], batch_size=16)\n",
    "p = []\n",
    "for logit in predictions4.logits:\n",
    "    p.append(math.exp(logit)/(1+math.exp(logit)))\n",
    "np.array(p)\n",
    "\n",
    "# Check author profiling -> 8,000 predictions\n",
    "# Split into 40 authors -> 200 tweets per author\n",
    "author_predictions = np.average(np.array_split(p, number_authors), axis=1)\n",
    "author_predictions = np.array([1 if ap >= 0.5 else 0 for ap in author_predictions])\n",
    "\n",
    "print(classification_report(author_profile, author_predictions, labels=[0, 1], target_names=['not hate','hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
