{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3u-vZStpPa01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Concatenate\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4u9xkayDoUAK"
   },
   "outputs": [],
   "source": [
    "!unzip -q \"/content/pan21-author-profiling-training-2021-03-14.zip\"\n",
    "!mkdir \"/content/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LZ5LmhXrxxT7"
   },
   "outputs": [],
   "source": [
    "#enDirectory = '/content/pan21-author-profiling-training-2021-03-14/en'\n",
    "#esDirectory = '/content/pan21-author-profiling-training-2021-03-14/es'\n",
    "\n",
    "esDirectory = 'dataset-pan21-author-profiling-training-2021-03-14/es'\n",
    "esTestDirectory = 'pan21-author-profiling-test-without-gold/es'\n",
    "\n",
    "enDirectory = 'dataset-pan21-author-profiling-training-2021-03-14/en'\n",
    "enTestDirectory = 'pan21-author-profiling-test-without-gold/en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHgFE5qCOsSm"
   },
   "outputs": [],
   "source": [
    "# 40,000 per language\n",
    "# 200 authors per language\n",
    "# 200 tweets per author\n",
    "# balanced dataset 50/50 of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "Funciones auxiliares:\n",
    "- Recoger todos los tweets\n",
    "- Limpiarlos, users un token, hastag un token, url un token, quitar stopwords y punctuation y tokenizar con TweetTokenizer\n",
    "- Hacer un 80:20 split manteniendo autores\n",
    "- Preentrenar word embeddings, luego no son tan importantes preentrenarlos\n",
    "- Convertir sequencias a ints para leer por la red + OOV y PAD\n",
    "- Hacer un preproceso extra para bert con BertTokenizer -> secuencias de ints y mask\n",
    "- Funcion para juntar predictions y hacer author profiling (Mejorar la revisión del f1 de los modelos)\n",
    "- Funcion para sacar a xml los resultados de test\n",
    "\n",
    "Se decide explorar la longitud de los tweets para decidir una longitud máxima del vector, se exploran individualmente y por 20 tweets juntos -> 15 y 250 en longitud respectivamente. (Se pueden enseñar las tablas si eso). Entonces lo que se mete a los modelos puede ser:\n",
    "- Es tweet individual\n",
    "- En tweet individual\n",
    "- Es 20 tweets juntos\n",
    "- En 20 tweets juntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zw8u2TPfO0ma"
   },
   "outputs": [],
   "source": [
    "def create_dataframe(path):\n",
    "    data = []\n",
    "\n",
    "    # Load tweets\n",
    "    for filename in os.listdir(path):\n",
    "        if(filename == \"truth.txt\"):\n",
    "            continue\n",
    "        tree = ET.parse(path + '/' + filename)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        author_id = filename.split('.')[0]\n",
    "        lang = root.attrib['lang']\n",
    "        t_class = int(root.attrib['class'])\n",
    "\n",
    "        for document in root.iter('document'):        \n",
    "            info = [author_id, lang, document.text, t_class]\n",
    "            data.append(info)\n",
    "    \n",
    "    # Create DataFrame with all data\n",
    "    col_names = ['author_id', 'lang', 'tweet', 'type']\n",
    "    return pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "def create_dataframe_test(path):\n",
    "    data = []\n",
    "\n",
    "    # Load tweets\n",
    "    for filename in os.listdir(path):\n",
    "        tree = ET.parse(path + '/' + filename)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        author_id = filename.split('.')[0]\n",
    "        lang = root.attrib['lang']\n",
    "\n",
    "        for document in root.iter('document'):        \n",
    "            info = [author_id, lang, document.text]\n",
    "            data.append(info)\n",
    "    \n",
    "    # Create DataFrame with all data\n",
    "    col_names = ['author_id', 'lang', 'tweet']\n",
    "    return pd.DataFrame(data, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tweets(df, n_tweets):\n",
    "    if len(df) % n_tweets != 0:\n",
    "        print(\"Select a divisible number of {}\".format(len(df)))\n",
    "        return None\n",
    "    \n",
    "    data = []\n",
    "    for i in range(0, len(df), n_tweets):\n",
    "        author_id = df['author_id'][i]\n",
    "        lang = df['lang'][i]\n",
    "        t_class = df['type'][i]\n",
    "        tweet_clean = df['tweet_clean'][i]\n",
    "        \n",
    "        j = i + n_tweets\n",
    "        tweet_clean = \" \".join(df['tweet_clean'][i:j])\n",
    "\n",
    "        info = [author_id, lang, tweet_clean, t_class]\n",
    "        data.append(info)    \n",
    "    \n",
    "    # Create DataFrame with all data\n",
    "    col_names = ['author_id', 'lang', 'tweet_clean', 'type']\n",
    "    return pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "def join_tweets_test(df, n_tweets):\n",
    "    if len(df) % n_tweets != 0:\n",
    "        print(\"Select a divisible number of {}\".format(len(df)))\n",
    "        return None\n",
    "    \n",
    "    data = []\n",
    "    for i in range(0, len(df), n_tweets):\n",
    "        author_id = df['author_id'][i]\n",
    "        lang = df['lang'][i]\n",
    "        #t_class = df['type'][i]\n",
    "        tweet_clean = df['tweet_clean'][i]\n",
    "        \n",
    "        j = i + n_tweets\n",
    "        tweet_clean = \" \".join(df_es['tweet_clean'][i:j])\n",
    "\n",
    "        info = [author_id, lang, tweet_clean]\n",
    "        data.append(info)    \n",
    "    \n",
    "    # Create DataFrame with all data\n",
    "    col_names = ['author_id', 'lang', 'tweet_clean']\n",
    "    return pd.DataFrame(data, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reUser = re.compile(r'@+\\w+')\n",
    "reHashtag = re.compile(r'#+\\w+')\n",
    "reWeb = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n",
    "\n",
    "\n",
    "def clean_text_es(text):\n",
    "    aux = []\n",
    "    \n",
    "    # Remove stopwords\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words(\"spanish\"):\n",
    "            aux.append(word)\n",
    "    #text = \" \".join(aux)\n",
    "    \n",
    "    # Normalize user tags\n",
    "    for item in re.finditer(reUser, text):\n",
    "        text = reUser.sub('#user', text)\n",
    "    \n",
    "    # Normalize hastags\n",
    "    for item in re.finditer(reHashtag, text):\n",
    "        text = reHashtag.sub('#hastag', text)\n",
    "    \n",
    "    # Normalize urls\n",
    "    for item in re.finditer(reWeb, text):\n",
    "        text = reWeb.sub('#web', text)\n",
    "\n",
    "    # Remove punctuation except #\n",
    "    punctuation = '!\"$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    text = list(tokenizer.tokenize(text))\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def clean_text_en(text):\n",
    "    aux = []\n",
    "    \n",
    "    # Remove stopwords\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            aux.append(word)\n",
    "    #text = \" \".join(aux)\n",
    "    \n",
    "    # Normalize user tags\n",
    "    for item in re.finditer(reUser, text):\n",
    "        text = reUser.sub('#user', text)\n",
    "    \n",
    "    # Normalize hastags\n",
    "    for item in re.finditer(reHashtag, text):\n",
    "        text = reHashtag.sub('#hastag', text)\n",
    "    \n",
    "    # Normalize urls\n",
    "    for item in re.finditer(reWeb, text):\n",
    "        text = reWeb.sub('#web', text)\n",
    "\n",
    "    # Remove punctuation except #\n",
    "    punctuation = '!\"$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    text = list(tokenizer.tokenize(text))\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_df(dataframe, split=0.2):\n",
    "    # 20 authors hate + 20 authors not in test \n",
    "    # = 40 authors = 8,000 tweets                   -> 4000 each\n",
    "    # =     ...    = 8,000 tweets / 20 = 400 tweets ->  200 each\n",
    "    \n",
    "    n_tweets = int(len(dataframe) * split / 2)\n",
    "\n",
    "    x_train0 = df_es.loc[(df_es.type == 0), 'tweet_clean'][n_tweets:].values\n",
    "    x_train1 = df_es.loc[(df_es.type == 1), 'tweet_clean'][n_tweets:].values\n",
    "\n",
    "    y_train0 = df_es.loc[(df_es.type == 0), 'type'][n_tweets:].values\n",
    "    y_train1 = df_es.loc[(df_es.type == 1), 'type'][n_tweets:].values\n",
    "\n",
    "    x_valid0 = df_es.loc[(df_es.type == 0), 'tweet_clean'][:n_tweets].values\n",
    "    x_valid1 = df_es.loc[(df_es.type == 1), 'tweet_clean'][:n_tweets].values\n",
    "\n",
    "    y_valid0 = df_es.loc[(df_es.type == 0), 'type'][:n_tweets].values\n",
    "    y_valid1 = df_es.loc[(df_es.type == 1), 'type'][:n_tweets].values\n",
    "\n",
    "    # HAY QUE HACER SHUFFLE AL TRAINING SET\n",
    "    x_train = np.concatenate((x_train0, x_train1), axis=None)\n",
    "    y_train = np.concatenate((y_train0, y_train1), axis=None)\n",
    "\n",
    "    x_valid = np.concatenate((x_valid0, x_valid1), axis=None)\n",
    "    y_valid = np.concatenate((y_valid0, y_valid1), axis=None)\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(corpus, word_embedding_size, max_words=10000, oov_tok='<OOV>'):\n",
    "    '''\n",
    "    Parameters:\n",
    "    size - size of dense vector to represent each token/word\n",
    "    window - max distance between target and neighbour word\n",
    "    min_count - min frequency of count of words. Ignore below threshold\n",
    "    workers - # of threads to run\n",
    "\n",
    "    sample - to downsample frequent words\n",
    "    sg - if skip-gram is used (=1) else CBOW\n",
    "    window - # of words looked before and after the analyzed one\n",
    "    '''\n",
    "    \n",
    "    word_sequence = [] # w2v necesita listas de tokens\n",
    "    for text in corpus:\n",
    "        word_sequence.append(text.split())\n",
    "    \n",
    "    # Train w2v model\n",
    "    w2v = Word2Vec(word_sequence,\n",
    "               sg=0,\n",
    "               vector_size=word_embedding_size,\n",
    "               window=5,\n",
    "               min_count=1,\n",
    "               workers=4)\n",
    "\n",
    "    # Needs to recieve string, hace split=(' ')\n",
    "    keras_tokenizer = Tokenizer(num_words=max_words,\n",
    "                                filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # all minus #\n",
    "                                lower=True,\n",
    "                                split=' ',\n",
    "                                oov_token=oov_tok)\n",
    "    # Transform text into integers\n",
    "    keras_tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((max_words, word_embedding_size))\n",
    "\n",
    "    for word, i in keras_tokenizer.word_index.items():\n",
    "        if i >= max_words:\n",
    "            break\n",
    "        if word in w2v.wv.key_to_index:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = w2v.wv[word]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_seq(x_train, x_valid, x_test, max_length, max_words=10000, oov_tok='<OOV>', trunc_type='post', pad_type='post'):\n",
    "    \n",
    "    # Needs to recieve string, hace split=(' ')\n",
    "    keras_tokenizer = Tokenizer(num_words=max_words,\n",
    "                                filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # all minus #\n",
    "                                lower=True,\n",
    "                                split=' ',\n",
    "                                oov_token=oov_tok)\n",
    "    # Transform text into integers\n",
    "    corpus = np.concatenate((x_train, x_valid), axis=None)\n",
    "    keras_tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    train_sequences = keras_tokenizer.texts_to_sequences(x_train)\n",
    "    train_padded = pad_sequences(train_sequences,\n",
    "                                 maxlen=max_length,\n",
    "                                 padding=pad_type,\n",
    "                                 truncating=trunc_type)\n",
    "\n",
    "    valid_sequences = keras_tokenizer.texts_to_sequences(x_valid)\n",
    "    valid_padded = pad_sequences(valid_sequences,\n",
    "                                 maxlen=max_length,\n",
    "                                 padding=pad_type,\n",
    "                                 truncating=trunc_type)\n",
    "\n",
    "    test_sequences = keras_tokenizer.texts_to_sequences(x_test)\n",
    "    test_padded = pad_sequences(test_sequences,\n",
    "                                maxlen=max_length,\n",
    "                                padding=pad_type,\n",
    "                                truncating=trunc_type)\n",
    "    \n",
    "    return train_padded, valid_padded, test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_bert(sentences, max_length):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    # Cambiar para que no sea uno a uno\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True)\n",
    "        \n",
    "        input_ids.append(bert_inp['input_ids'])\n",
    "        attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "    input_ids=np.asarray(input_ids)\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### AUTHOR PROFILING FUNCTION FOR JOINING PREDICTIONS #########\n",
    "\n",
    "# de 20 0s y 20 1s\n",
    "author_profile = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "# Example predictions\n",
    "predictions = np.random.rand(len(y_test))\n",
    "\n",
    "def author_profiling_report(author_profile, author_predictions):\n",
    "    # Check author profiling -> 8,000 predictions\n",
    "    # Split into 40 authors -> 200 tweets per author\n",
    "    author_predictions = np.average(np.array_split(predictions, 40), axis=1)\n",
    "    author_predictions = np.array([1 if ap >= 0.5 else 0 for ap in author_predictions])\n",
    "\n",
    "    print(classification_report(author_profile, author_predictions, labels=[0, 1], target_names=['not hate','hate']))\n",
    "\n",
    "print(predictions)\n",
    "author_profiling_report(author_profile, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt output\n",
    "# <author id=\"author-id\" lang=\"en|es\" type=\"0|1\"/>\n",
    "def format_output(author_predictions):\n",
    "    root = ET.Element(\"author\")\n",
    "    root.set(\"id\", \"123\")\n",
    "    root.set(\"lang\", \"es\")\n",
    "    root.set(\"type\", \"0\")\n",
    "\n",
    "    tree = ET.ElementTree(root)\n",
    "    xml_str = ET.tostring(root).decode('utf8')\n",
    "\n",
    "    save_path_file = \"person.xml\"\n",
    "\n",
    "    with open(save_path_file, \"w\") as f:\n",
    "        f.write(xml_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Trhq5STgXcDD",
    "outputId": "f84377d8-68cd-4b87-fea7-00bbf5ecc58f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_es), len(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "Joo-_niuO4Se",
    "outputId": "806fd1d2-fb15-4d2a-bde7-42f9edb0f2b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>type</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>muero de amor con milo 🥰</td>\n",
       "      <td>0</td>\n",
       "      <td>muero de amor con milo 🥰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>vieron cuando les da paja hablar con alguien, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>vieron cuando les da paja hablar con alguien b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>mi abuela me manda audios de TRES MINUTOS Y ME...</td>\n",
       "      <td>0</td>\n",
       "      <td>mi abuela me manda audios de tres minutos y me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>lo amo fuerte #URL#</td>\n",
       "      <td>0</td>\n",
       "      <td>lo amo fuerte hastag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>mi especialidad son las tartas no se habla mas</td>\n",
       "      <td>0</td>\n",
       "      <td>mi especialidad son las tartas no se habla mas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          author_id lang  \\\n",
       "0  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "1  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "2  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "3  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "4  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "\n",
       "                                               tweet  type  \\\n",
       "0                           muero de amor con milo 🥰     0   \n",
       "1  vieron cuando les da paja hablar con alguien, ...     0   \n",
       "2  mi abuela me manda audios de TRES MINUTOS Y ME...     0   \n",
       "3                                lo amo fuerte #URL#     0   \n",
       "4     mi especialidad son las tartas no se habla mas     0   \n",
       "\n",
       "                                         tweet_clean  \n",
       "0                           muero de amor con milo 🥰  \n",
       "1  vieron cuando les da paja hablar con alguien b...  \n",
       "2  mi abuela me manda audios de tres minutos y me...  \n",
       "3                               lo amo fuerte hastag  \n",
       "4     mi especialidad son las tartas no se habla mas  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH2owtiBO52y",
    "outputId": "354f5be7-fe2f-43c7-e7d1-828147fa28ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Es---\n",
      "count    40000.000000\n",
      "mean        13.300525\n",
      "std          5.568387\n",
      "min          3.000000\n",
      "25%          9.000000\n",
      "50%         13.000000\n",
      "75%         17.000000\n",
      "max         81.000000\n",
      "Name: tweetLength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_es['tweetLength'] = df_es['tweet_clean'].str.split().map(len)\n",
    "print(\"---Es---\")\n",
    "print(df_es['tweetLength'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "GqBzmAYtYwEH",
    "outputId": "15ef7873-9937-40c9-b5e9-489c544bff22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_es['tweetLength'], 50)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "QieoKJPYO7NE",
    "outputId": "7781b08a-b1fe-4ce5-e814-12615132c2f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>type</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>First Catering 😎 #URL#</td>\n",
       "      <td>0</td>\n",
       "      <td>first catering 😎 hastag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>RT #USER#: vibes with someone come naturally. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt hastag vibes with someone come naturally do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>RT #USER#: never force nobody to appreciate you.</td>\n",
       "      <td>0</td>\n",
       "      <td>rt hastag never force nobody to appreciate you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>RT #USER#: That camera pan to the horny niggas...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt hastag that camera pan to the horny niggas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>RT #USER#: Buying a $50 dollar bottle of liquo...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt hastag buying a 50 dollar bottle of liquor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          author_id lang  \\\n",
       "0  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "1  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "2  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "3  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "4  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "\n",
       "                                               tweet  type  \\\n",
       "0                             First Catering 😎 #URL#     0   \n",
       "1  RT #USER#: vibes with someone come naturally. ...     0   \n",
       "2   RT #USER#: never force nobody to appreciate you.     0   \n",
       "3  RT #USER#: That camera pan to the horny niggas...     0   \n",
       "4  RT #USER#: Buying a $50 dollar bottle of liquo...     0   \n",
       "\n",
       "                                         tweet_clean  \n",
       "0                            first catering 😎 hastag  \n",
       "1  rt hastag vibes with someone come naturally do...  \n",
       "2     rt hastag never force nobody to appreciate you  \n",
       "3  rt hastag that camera pan to the horny niggas ...  \n",
       "4  rt hastag buying a 50 dollar bottle of liquor ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHgsBpAfO82C",
    "outputId": "ef86904f-b23e-4832-c43e-33d52ba4d7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---En---\n",
      "count    40000.000000\n",
      "mean        12.924000\n",
      "std          5.706929\n",
      "min          2.000000\n",
      "25%          8.000000\n",
      "50%         12.000000\n",
      "75%         17.000000\n",
      "max         61.000000\n",
      "Name: tweetLength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_en['tweetLength'] = df_en['tweet_clean'].str.split().map(len)\n",
    "print(\"---En---\")\n",
    "print(df_en['tweetLength'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "auWTynLuZLJk",
    "outputId": "cc0b72e2-907b-44fc-a4ad-fbf0d8cb2517"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_en['tweetLength'], 50)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 joined tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_es_20_long_text), len(df_en_20_long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>type</th>\n",
       "      <th>tweetLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>muero de amor con milo 🥰 vieron cuando les da ...</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>que dolor de cabeza maldita birra buena noche ...</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>soy yo diciendo que tengo la gota jajsjssjjaja...</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>no se que hacer con mi vida we rt hastag foo q...</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f558a21dd8e02f2f7d4e5d5c3ea24195</td>\n",
       "      <td>es</td>\n",
       "      <td>estuve diciendo duermo 15min mas desde las 8 d...</td>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          author_id lang  \\\n",
       "0  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "1  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "2  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "3  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "4  f558a21dd8e02f2f7d4e5d5c3ea24195   es   \n",
       "\n",
       "                                         tweet_clean  type  tweetLength  \n",
       "0  muero de amor con milo 🥰 vieron cuando les da ...     0          224  \n",
       "1  que dolor de cabeza maldita birra buena noche ...     0          223  \n",
       "2  soy yo diciendo que tengo la gota jajsjssjjaja...     0          211  \n",
       "3  no se que hacer con mi vida we rt hastag foo q...     0          233  \n",
       "4  estuve diciendo duermo 15min mas desde las 8 d...     0          290  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es_20_long_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Es---\n",
      "count    2000.000000\n",
      "mean      266.010500\n",
      "std        40.318348\n",
      "min       140.000000\n",
      "25%       240.750000\n",
      "50%       268.000000\n",
      "75%       293.000000\n",
      "max       413.000000\n",
      "Name: tweetLength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_es_20_long_text['tweetLength'] = df_en_20_long_text['tweet_clean'].str.split().map(len)\n",
    "print(\"---Es---\")\n",
    "print(df_es_20_long_text['tweetLength'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_es_20_long_text['tweetLength'], 50)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>first catering 😎 hastag rt hastag vibes with s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>rt hastag wine tipsy gt alcohol tipsy rt hasta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>rt hastag hastag hastag all of a sudden its as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>rt hastag seen a 400 million contract at 033 r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99cb2cd23f7cf613d394502d17dedad0</td>\n",
       "      <td>en</td>\n",
       "      <td>am i the only mf who smoke weed and is so sick...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          author_id lang  \\\n",
       "0  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "1  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "2  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "3  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "4  99cb2cd23f7cf613d394502d17dedad0   en   \n",
       "\n",
       "                                         tweet_clean  type  \n",
       "0  first catering 😎 hastag rt hastag vibes with s...     0  \n",
       "1  rt hastag wine tipsy gt alcohol tipsy rt hasta...     0  \n",
       "2  rt hastag hastag hastag all of a sudden its as...     0  \n",
       "3  rt hastag seen a 400 million contract at 033 r...     0  \n",
       "4  am i the only mf who smoke weed and is so sick...     0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_20_long_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---En---\n",
      "count    2000.000000\n",
      "mean      258.480000\n",
      "std        42.085011\n",
      "min       128.000000\n",
      "25%       229.000000\n",
      "50%       259.000000\n",
      "75%       286.000000\n",
      "max       459.000000\n",
      "Name: tweetLength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_en_20_long_text['tweetLength'] = df_en_20_long_text['tweet_clean'].str.split().map(len)\n",
    "print(\"---En---\")\n",
    "print(df_en_20_long_text['tweetLength'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_en_20_long_text['tweetLength'], 50)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish individual tweets\n",
    "\n",
    "df_es = create_dataframe(esDirectory)\n",
    "df_es['tweet_clean'] = df_es.tweet.apply(clean_text_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es)\n",
    "\n",
    "df_es_test = create_dataframe(esTestDirectory)\n",
    "df_es_test['tweet_clean'] = df_es_test.tweet.apply(clean_text_es)\n",
    "x_test = df_es_test['tweet_clean'].values\n",
    "\n",
    "es_cml = (x_train, y_train,\n",
    "          x_valid, y_valid,\n",
    "          x_test)\n",
    "\n",
    "pickle_file = open('es_cml.pickle', 'wb')\n",
    "pickle.dump(es_cml, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MyMt0E2VBNvg"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es)\n",
    "\n",
    "corpus_es = np.concatenate((x_train, x_valid), axis=None)\n",
    "embedding_matrix = create_embedding_matrix(corpus_es, word_embedding_size=300)\n",
    "\n",
    "df_es_test = create_dataframe(esTestDirectory)\n",
    "df_es_test['tweet_clean'] = df_es_test.tweet.apply(clean_text_es)\n",
    "x_test = df_es_test['tweet_clean'].values\n",
    "\n",
    "train_padded, valid_padded, test_padded = to_int_seq(x_train, x_valid, x_test, max_length=15)\n",
    "\n",
    "# GUARDAR AQUI\n",
    "# embedding_matrix\n",
    "# train_padded, y_train\n",
    "# valid_padded, y_valid\n",
    "# test_padded\n",
    "\n",
    "es_indv = (embedding_matrix,\n",
    "           train_padded, y_train,\n",
    "           valid_padded, y_valid,\n",
    "           test_padded)\n",
    "\n",
    "pickle_file = open('es_indv.pickle', 'wb')\n",
    "pickle.dump(es_indv, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English individual tweets\n",
    "\n",
    "df_en = create_dataframe(enDirectory)\n",
    "df_en['tweet_clean'] = df_en.tweet.apply(clean_text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_en)\n",
    "\n",
    "df_es_test = create_dataframe(esTestDirectory)\n",
    "df_es_test['tweet_clean'] = df_es_test.tweet.apply(clean_text_es)\n",
    "x_test = df_es_test['tweet_clean'].values\n",
    "\n",
    "en_cml = (x_train, y_train,\n",
    "          x_valid, y_valid,\n",
    "          x_test)\n",
    "\n",
    "pickle_file = open('en_cml.pickle', 'wb')\n",
    "pickle.dump(en_cml, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_en)\n",
    "\n",
    "corpus_en = np.concatenate((x_train, x_valid), axis=None)\n",
    "embedding_matrix = create_embedding_matrix(corpus_en, word_embedding_size=300)\n",
    "\n",
    "df_en_test = create_dataframe(enTestDirectory)\n",
    "df_en_test['tweet_clean'] = df_en_test.tweet.apply(clean_text_en)\n",
    "x_test = df_en_test['tweet_clean'].values\n",
    "\n",
    "train_padded, valid_padded, test_padded = to_int_seq(x_train, x_valid, x_test, max_length=15)\n",
    "\n",
    "# GUARDAR AQUI\n",
    "# embedding_matrix\n",
    "# train_padded, y_train\n",
    "# valid_padded, y_valid\n",
    "# test_padded\n",
    "\n",
    "en_indv = (embedding_matrix,\n",
    "           train_padded, y_train,\n",
    "           valid_padded, y_valid,\n",
    "           test_padded)\n",
    "\n",
    "pickle_file = open('en_indv.pickle', 'wb')\n",
    "pickle.dump(en_indv, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 joined tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish 20 joined tweets\n",
    "\n",
    "df_es_20_long_text = join_tweets(df_es, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es_20_long_text)\n",
    "\n",
    "df_es_20_long_text = join_tweets_test(df_es_test, 20)\n",
    "x_test = df_es_20_long_text['tweet_clean'].values\n",
    "\n",
    "train_padded, valid_padded, test_padded = to_int_seq(x_train, x_valid, x_test, max_length=250)\n",
    "\n",
    "# GUARDAR AQUI\n",
    "# embedding_matrix\n",
    "# train_padded, y_train\n",
    "# valid_padded, y_valid\n",
    "# test_padded\n",
    "\n",
    "es_20 = (#embedding_matrix,\n",
    "           train_padded, y_train,\n",
    "           valid_padded, y_valid,\n",
    "           test_padded)\n",
    "\n",
    "pickle_file = open('es_20.pickle', 'wb')\n",
    "pickle.dump(es_20, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English 20 joined tweets\n",
    "\n",
    "df_en_20_long_text = join_tweets(df_en, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_en_20_long_text)\n",
    "\n",
    "df_en_20_long_text = join_tweets_test(df_en_test, 20)\n",
    "x_test = df_en_20_long_text['tweet_clean'].values\n",
    "\n",
    "train_padded, valid_padded, test_padded = to_int_seq(x_train, x_valid, x_test, max_length=250)\n",
    "\n",
    "# GUARDAR AQUI\n",
    "# embedding_matrix\n",
    "# train_padded, y_train\n",
    "# valid_padded, y_valid\n",
    "# test_padded\n",
    "\n",
    "en_20 = (#embedding_matrix,\n",
    "           train_padded, y_train,\n",
    "           valid_padded, y_valid,\n",
    "           test_padded)\n",
    "\n",
    "pickle_file = open('en_20.pickle', 'wb')\n",
    "pickle.dump(en_20, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish individual tweets\n",
    "\n",
    "df_es = create_dataframe(esDirectory)\n",
    "df_es['tweet_clean'] = df_es.tweet.apply(clean_text_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es)\n",
    "\n",
    "df_es_test = create_dataframe(esTestDirectory)\n",
    "df_es_test['tweet_clean'] = df_es_test.tweet.apply(clean_text_es)\n",
    "x_test = df_es_test['tweet_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "train_padded, train_mask = prepare_data_bert(x_train, 15)\n",
    "valid_padded, valid_mask = prepare_data_bert(x_valid, 15)\n",
    "test_padded, test_mask = prepare_data_bert(x_test, 15)\n",
    "\n",
    "es_indv_bert = (train_padded, train_mask, y_train,\n",
    "                valid_padded, valid_mask, y_valid,\n",
    "                test_padded, test_mask)\n",
    "\n",
    "pickle_file = open('es_indv_bert.pickle', 'wb')\n",
    "pickle.dump(es_indv_bert, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es_20_long_text = join_tweets(df_es, 20)\n",
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es_20_long_text)\n",
    "\n",
    "df_es_20_long_text = join_tweets_test(df_es_test, 20)\n",
    "x_test = df_es_20_long_text['tweet_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/david/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "train_padded, train_mask = prepare_data_bert(x_train, 250)\n",
    "valid_padded, valid_mask = prepare_data_bert(x_valid, 250)\n",
    "test_padded, test_mask = prepare_data_bert(x_test, 250)\n",
    "\n",
    "es_20_bert = (train_padded, train_mask, y_train,\n",
    "              valid_padded, valid_mask, y_valid,\n",
    "              test_padded, test_mask)\n",
    "\n",
    "pickle_file = open('es_20_bert.pickle', 'wb')\n",
    "pickle.dump(es_20_bert, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish individual tweets\n",
    "\n",
    "df_es = create_dataframe(esDirectory)\n",
    "df_es['tweet_clean'] = df_es.tweet.apply(clean_text_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es)\n",
    "\n",
    "df_es_test = create_dataframe(esTestDirectory)\n",
    "df_es_test['tweet_clean'] = df_es_test.tweet.apply(clean_text_es)\n",
    "\n",
    "test_id_es, x_test_es = df_es_test['author_id'].values, df_es_test['tweet_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish 20 joined tweets\n",
    "\n",
    "df_es_20_long_text = join_tweets(df_es, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_es_20_long_text)\n",
    "\n",
    "df_es_20_long_text = join_tweets_test(df_es_test, 20)\n",
    "x_test = df_es_20_long_text['tweet_clean'].values\n",
    "\n",
    "# Get 1 every 20 ids or groupby?\n",
    "\n",
    "_, _, test_padded = to_int_seq(x_train, x_valid, x_test, max_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = []\n",
    "for i in range(0, len(test_id_es), 200\n",
    "    test_id.append(test_id_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_es_20 = (test_id,\n",
    "              test_padded)\n",
    "\n",
    "pickle_file = open('test_es_20.pickle', 'wb')\n",
    "pickle.dump(test_es_20, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English individual tweets\n",
    "\n",
    "df_en = create_dataframe(enDirectory)\n",
    "df_en['tweet_clean'] = df_en.tweet.apply(clean_text_en)\n",
    "\n",
    "df_en_test = create_dataframe(enTestDirectory)\n",
    "df_en_test['tweet_clean'] = df_en_test.tweet.apply(clean_text_en)\n",
    "\n",
    "test_id_en, x_test_en = df_en_test['author_id'].values, df_en_test['tweet_clean'].values\n",
    "\n",
    "# Spanish 20 joined tweets\n",
    "df_en_20_long_text = join_tweets(df_en, 20)\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = get_data_from_df(df_en_20_long_text)\n",
    "\n",
    "df_en_20_long_text = join_tweets_test(df_en_test, 20)\n",
    "x_test = df_en_20_long_text['tweet_clean'].values\n",
    "\n",
    "_, _, test_padded = to_int_seq(x_train, x_valid, x_test, max_length=250)\n",
    "\n",
    "test_id = []\n",
    "for i in range(0, len(test_id_en), 200):\n",
    "    test_id.append(test_id_en[i])\n",
    "    \n",
    "test_en_20 = (test_id,\n",
    "              test_padded)\n",
    "\n",
    "pickle_file = open('test_en_20.pickle', 'wb')\n",
    "pickle.dump(test_en_20, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f8e9b6073d2fcbf9d2de3df07c62d86e'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id_en[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HateProfiling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
