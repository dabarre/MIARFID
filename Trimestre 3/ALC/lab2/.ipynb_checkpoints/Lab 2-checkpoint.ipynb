{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(path):\n",
    "    # load data\n",
    "    root = ET.parse(path).getroot()    \n",
    "    ids = []\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for tag in root.findall('tweet'):\n",
    "        tweetid = tag.find('tweetid')\n",
    "        content = tag.find('content')\n",
    "        value = tag.find('sentiment/polarity/value')\n",
    "        ids.append(tweetid.text)\n",
    "        tweets.append(content.text)\n",
    "        labels.append(value.text)\n",
    "        \n",
    "    return ids, tweets, labels\n",
    "\n",
    "def parse_data_test(path):\n",
    "    # load data\n",
    "    root = ET.parse(path).getroot()    \n",
    "    ids = []\n",
    "    tweets = []\n",
    "    for tag in root.findall('tweet'):\n",
    "        tweetid = tag.find('tweetid')\n",
    "        content = tag.find('content')\n",
    "        ids.append(tweetid.text)\n",
    "        tweets.append(content.text)\n",
    "        \n",
    "    return ids, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, x_train, y_train = parse_data('TASS2017_T1_training.xml')\n",
    "id_dev, x_dev, y_dev = parse_data('TASS2017_T1_development.xml')\n",
    "id_test, x_test = parse_data_test(\"TASS2017_T1_test.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reUser = re.compile(r'@+\\w+')\n",
    "reHashtag = re.compile(r'#+\\w+')\n",
    "reWeb = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "\n",
    "def clean_text(text):\n",
    "    res = []\n",
    "    for element in text:\n",
    "        aux = []    \n",
    "\n",
    "        # Remove stopwords\n",
    "        for word in element.split():\n",
    "            if word not in stopwords.words(\"spanish\"):\n",
    "                aux.append(word)\n",
    "        element = \" \".join(aux)\n",
    "        # Normalize user tags\n",
    "        for item in re.finditer(reUser, element):\n",
    "            element = reUser.sub('#user', element)\n",
    "        # Normalize hastags\n",
    "        for item in re.finditer(reHashtag, element):\n",
    "            element = reHashtag.sub('#hastag', element)\n",
    "        # Normalize urls\n",
    "        for item in re.finditer(reWeb, element):\n",
    "            element = reWeb.sub('#web', element)\n",
    "        # Remove punctuation\n",
    "        element = element.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        res.append(element)\n",
    "    return res\n",
    "    \n",
    "train = clean_text(x_train)\n",
    "dev = clean_text(x_dev)\n",
    "test = clean_text(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n",
    "\n",
    "train_clean = list(map(\" \".join, map(tokenizer.tokenize, train)))\n",
    "dev_clean = list(map(\" \".join, map(tokenizer.tokenize, dev)))\n",
    "test_clean = list(map(\" \".join, map(tokenizer.tokenize, test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 980 candidates, totalling 4900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4900 out of 4900 | elapsed: 16.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3877597477137727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 100,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.8,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (3, 6)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", LogisticRegression())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__C\":[1,10,100,1000,10000]\n",
    "             }\n",
    "\n",
    "clf_lr = GridSearchCV(pipe,\n",
    "                      param_grid,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=2,\n",
    "                      scoring=\"f1_macro\")\n",
    "\n",
    "clf_lr.fit(train_clean, y_train)\n",
    "print(clf_lr.best_score_)\n",
    "clf_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1960 candidates, totalling 9800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9800 out of 9800 | elapsed: 39.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3848031342070236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10,\n",
       " 'clf__kernel': 'linear',\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.8,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (4, 5)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", SVC())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__kernel\":['linear', 'rbf'],\n",
    "              \"clf__C\":[1,10,100,1000,10000]\n",
    "             }\n",
    "\n",
    "clf_svc = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_svc.fit(train_clean, y_train)\n",
    "print(clf_svc.best_score_)\n",
    "clf_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 980 candidates, totalling 4900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4900 out of 4900 | elapsed: 28.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31993195546535513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__n_estimators': 50,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.5,\n",
       " 'tfidf__min_df': 3,\n",
       " 'tfidf__ngram_range': (4, 5)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__n_estimators\":[50,100,150,200,300]\n",
    "             }\n",
    "\n",
    "clf_rfc = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_rfc.fit(train_clean, y_train)\n",
    "print(clf_rfc.best_score_)\n",
    "clf_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1372 candidates, totalling 6860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   42.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36833977444122495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6860 out of 6860 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__n_neighbors': 5,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.3,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (3, 4)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__n_neighbors\":[3,5,7,11,15,21,25]\n",
    "             }\n",
    "\n",
    "clf_knn = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_knn.fit(train_clean, y_train)\n",
    "print(clf_knn.best_score_)\n",
    "clf_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation over development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.56      0.67      0.61       219\n",
      "         NEU       0.11      0.07      0.09        69\n",
      "        NONE       0.39      0.19      0.26        62\n",
      "           P       0.54      0.57      0.55       156\n",
      "\n",
      "    accuracy                           0.50       506\n",
      "   macro avg       0.40      0.38      0.38       506\n",
      "weighted avg       0.47      0.50      0.48       506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,6),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.56      0.71      0.63       219\n",
      "         NEU       0.07      0.01      0.02        69\n",
      "        NONE       0.32      0.19      0.24        62\n",
      "           P       0.55      0.63      0.59       156\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.38      0.39      0.37       506\n",
      "weighted avg       0.46      0.53      0.49       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.5,\n",
    "                                 min_df=3,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=50))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.57      0.71      0.63       219\n",
      "         NEU       0.14      0.10      0.12        69\n",
      "        NONE       0.38      0.19      0.26        62\n",
      "           P       0.55      0.53      0.54       156\n",
      "\n",
      "    accuracy                           0.51       506\n",
      "   macro avg       0.41      0.38      0.39       506\n",
      "weighted avg       0.48      0.51      0.49       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,4),\n",
    "                                 max_df=0.3,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.52      0.63      0.57       219\n",
      "         NEU       0.15      0.12      0.13        69\n",
      "        NONE       0.43      0.32      0.37        62\n",
      "           P       0.55      0.49      0.52       156\n",
      "\n",
      "    accuracy                           0.48       506\n",
      "   macro avg       0.41      0.39      0.40       506\n",
      "weighted avg       0.47      0.48      0.47       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=10, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.53      0.83      0.65       219\n",
      "         NEU       0.10      0.01      0.03        69\n",
      "        NONE       0.70      0.11      0.19        62\n",
      "           P       0.59      0.55      0.57       156\n",
      "\n",
      "    accuracy                           0.55       506\n",
      "   macro avg       0.48      0.38      0.36       506\n",
      "weighted avg       0.51      0.55      0.48       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=10, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.58      0.73      0.65       219\n",
      "         NEU       0.17      0.12      0.14        69\n",
      "        NONE       0.51      0.31      0.38        62\n",
      "           P       0.57      0.54      0.55       156\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.46      0.42      0.43       506\n",
      "weighted avg       0.51      0.53      0.52       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=100, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS\n",
    "{'clf__C': 100,\n",
    " 'clf__kernel': 'linear',\n",
    " 'tfidf__analyzer': 'char_wb',\n",
    " 'tfidf__max_df': 0.8,\n",
    " 'tfidf__min_df': 1,\n",
    " 'tfidf__ngram_range': (3, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=100, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.txt\", \"w\") as f:\n",
    "    for id_ts, pred in zip(id_test , predictions):\n",
    "        f.write(\"{}\\t{}\\n\".format(id_ts, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
