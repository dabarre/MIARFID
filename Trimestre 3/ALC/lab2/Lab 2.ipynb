{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(path):\n",
    "    # load data\n",
    "    root = ET.parse(path).getroot()    \n",
    "    ids = []\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for tag in root.findall('tweet'):\n",
    "        tweetid = tag.find('tweetid')\n",
    "        content = tag.find('content')\n",
    "        value = tag.find('sentiment/polarity/value')\n",
    "        ids.append(tweetid.text)\n",
    "        tweets.append(content.text)\n",
    "        labels.append(value.text)\n",
    "        \n",
    "    return ids, tweets, labels\n",
    "\n",
    "def parse_data_test(path):\n",
    "    # load data\n",
    "    root = ET.parse(path).getroot()    \n",
    "    ids = []\n",
    "    tweets = []\n",
    "    for tag in root.findall('tweet'):\n",
    "        tweetid = tag.find('tweetid')\n",
    "        content = tag.find('content')\n",
    "        ids.append(tweetid.text)\n",
    "        tweets.append(content.text)\n",
    "        \n",
    "    return ids, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, x_train, y_train = parse_data('TASS2017_T1_training.xml')\n",
    "id_dev, x_dev, y_dev = parse_data('TASS2017_T1_development.xml')\n",
    "id_test, x_test = parse_data_test(\"TASS2017_T1_test.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reUser = re.compile(r'@+\\w+')\n",
    "reHashtag = re.compile(r'#+\\w+')\n",
    "reWeb = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "\n",
    "def clean_text(text):\n",
    "    res = []\n",
    "    for element in text:\n",
    "        aux = []    \n",
    "\n",
    "        # Remove stopwords\n",
    "        for word in element.split():\n",
    "            if word not in stopwords.words(\"spanish\"):\n",
    "                aux.append(word)\n",
    "        element = \" \".join(aux)\n",
    "        # Normalize user tags\n",
    "        for item in re.finditer(reUser, element):\n",
    "            element = reUser.sub('#user', element)\n",
    "        # Normalize hastags\n",
    "        for item in re.finditer(reHashtag, element):\n",
    "            element = reHashtag.sub('#hastag', element)\n",
    "        # Normalize urls\n",
    "        for item in re.finditer(reWeb, element):\n",
    "            element = reWeb.sub('#web', element)\n",
    "        # Remove punctuation\n",
    "        element = element.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        res.append(element)\n",
    "    return res\n",
    "    \n",
    "train = clean_text(x_train)\n",
    "dev = clean_text(x_dev)\n",
    "test = clean_text(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n",
    "\n",
    "train_clean = list(map(\" \".join, map(tokenizer.tokenize, train)))\n",
    "dev_clean = list(map(\" \".join, map(tokenizer.tokenize, dev)))\n",
    "test_clean = list(map(\" \".join, map(tokenizer.tokenize, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hastag 100010 verdad voy decir petarda quiero mismo ✨',\n",
       " 'hastag hastag hastag aún leído caerán prontito',\n",
       " 'al final sido 3h bueno mañana fiesta así que no quejo',\n",
       " 'hastag tiempo cosas ahora mismo',\n",
       " 'hastag ves brillo coso hace sepan kk',\n",
       " 'tengo perrina adorable sabéis acompaña habitación voy dormir',\n",
       " 'hastag es ojeando año pasado tampoco muchas canciones jajajajaja',\n",
       " 'bueno batalla final conquista después faltaría revelación',\n",
       " 'hastag ¿ mañana sábado 31 en día vives mañana miércoles 31',\n",
       " 'hastag caminante mar niebla cuadros favoritos portada',\n",
       " 'hastag ¡ sí y encantado ¿ tú visto ¿ cuándo comentamos',\n",
       " 'hastag se olvidaban grandes hastag hastag a ver si interesa hilillo',\n",
       " 'hastag mejor si pones link cuenta costado encontrarte',\n",
       " 'hastag por tenía pensado verla después segunda daredevil',\n",
       " 'hastag lado manita usas vea negro',\n",
       " 'llevo despierto 8 puto mosquito volando puta oreja',\n",
       " 'hastag qué estupendo y ¿ cómo encargo ¿ es estupendísima versión barce',\n",
       " 'cosas enamora tostadas calentitas horas mañana',\n",
       " 'hola buenos días dormido 5 acabo despertar ido sueño',\n",
       " 'hastag creo diseñado tipo propia',\n",
       " 'hastag sabes 2012 13 novela mierda perrie mala',\n",
       " 'hastag ninguna visto directo concierto madrid pronto pls',\n",
       " 'hastag esperanza realidad personaje mas oscuro',\n",
       " 'pa vez pongo ver películas amigo queda dormido',\n",
       " 'hastag nfin puede ser peor minikayato clauyato is real',\n",
       " 'hastag que kinox quiere zi zoy buena perzona',\n",
       " 'este viernes iré cine ver hastag celebrar cumple por cierto invitados',\n",
       " 'podríamos empezar septiembre día tres sé guay',\n",
       " 'peor funcionaba maldita jaco quiero',\n",
       " 'aqui fardando vegano hace 2 dias amigos tuiter',\n",
       " 'hastag pues gente así mala mundo',\n",
       " 'hastag atraigo dos lados problema duro',\n",
       " 'hastag ofrecería gerald demasiado lindo',\n",
       " 'hastag dirección proyectos empresas audiovisuales',\n",
       " 'como destrozaba puto movil ahora mismo',\n",
       " 'hastag todas grandes tardes hacen pasar pasaran',\n",
       " 'hastag va tía viejos madre chino los encantan comprando kiko',\n",
       " 'parece mañana podré catar madden hoy habra directo',\n",
       " 'mi compañero trabajo futuro marido pena sepa',\n",
       " 'hastag hastag fallecido hace meses hay q contrastar',\n",
       " 'hastag necesito ración atención diaria si mantengo autoestima',\n",
       " 'hastag gracias dar me gusta tweets ¿ que tal ¿ estás trabajando cristina',\n",
       " 'hastag nuestros niños andaban animado acaban vacaciones',\n",
       " 'hastag ser malo bueno salvas queremos igual',\n",
       " 'pero solo cojo 5 días cambio curro',\n",
       " 'hastag yo imaginaba así también creen casting buenísimo menos contenta',\n",
       " 'despues trabajar duro merezco caprichos',\n",
       " 'hastag yo puse bonito cuanto desprecio',\n",
       " 'no encuentro mando tele poniendo nerviosita',\n",
       " 'lo mejor málaga poder ir ver escuadrón suicida tarde',\n",
       " 'hastag insoportable eso dices ya contarás espero vaya genial',\n",
       " 'hastag cierto respeto y caso admiración mutuo',\n",
       " 'hastag subido gayumbos hace nada qué lees paisana hastag',\n",
       " 'hastag ser mala persiana quitarte diversion',\n",
       " 'hastag ¿ verdad sí yo quité gran peso encima',\n",
       " 'hastag mañana seguro gente buena hastag hastag',\n",
       " 'hastag hastag sé pobre uno te acompaño feels',\n",
       " 'hastag hastag aún así campo l edición coleccionista acabar pagarla',\n",
       " 'mantengo optimista hace difícil veces verdad',\n",
       " 'hastag hastag hastag sí yo creo vamos acabar muuuy contentas',\n",
       " 'vale entiendo rápido bajada haber entrado hastag',\n",
       " 'hastag correcció nin pasa pola cabeza non ser segundos',\n",
       " 'ha muerto profesora volumenescultura serio bajonazo',\n",
       " 'haces vaya melancolía devuelves nuevo vida',\n",
       " 'hastag tia punto acabar novena voy retraso',\n",
       " 'hastag acabo aportar pequeña contribución te sientes tan bien',\n",
       " 'hastag pues aludida y mismosíbuen día',\n",
       " 'hoy mosca dejó dormir cago puta vida',\n",
       " 'hastag gran gran tipo hastag',\n",
       " 'hastag entiende sueño además vi creo 5 primaria así',\n",
       " 'hastag hastag aquí habemos saturado leer idnwhy',\n",
       " 'hastag espero mejores pronto vomitar desagradable',\n",
       " 'chancla representa asco harta madre aquí menda',\n",
       " 'hola hastag gracias información tomamos nota qué buen día',\n",
       " 'hastag gracias etiquetarme suelo hacer cosas',\n",
       " 'hastag hastag ¿ socialistas honrados esto oxímoron',\n",
       " 'pasa tumblrr oo vuelto extraño sale musica',\n",
       " 'cada busque mecanicas haga rapido morir',\n",
       " 'hastag hastag hastag bien pienso enquistas temas secundarios ver toda película',\n",
       " 'ojalá alguien llamandome bonito precioso sad',\n",
       " 'hastag hastag hacemos presión social avanza',\n",
       " 'hastag consigo ayudame porfi super importante',\n",
       " 'despues tener combo perfecto pa sentirme mierda dormir',\n",
       " 'necesito ver menos vez riri irme dormir conforme ¿ por echar horas gala hastag',\n",
       " 'hastag barbaridad dinero aunque puedo contrata show noe',\n",
       " 'hastag ¿ eliminada nooo eliminado pasó semifinal día',\n",
       " 'hastag deseo muchísima suerte próxima aventura tío',\n",
       " 'hastag hastag preciosa vista gran resultado enhorabuena',\n",
       " 'hastag amo aunque parece dormido quedaban 5 6',\n",
       " 'hastag hastag jajajajaja el all bran pro buenos días compis',\n",
       " 'de verdad puse gafas moradas soporto ningún tío',\n",
       " 'el puente mas bonito constitución año alicante 1213 14 hastag directo allí',\n",
       " 'hastag ouch llego ver si próxima si puedo saludo',\n",
       " 'aaaa aaa tan cansada espera semana tan horrible jueves',\n",
       " 'hastag es chisme expresa matices no idea elemento cortante',\n",
       " 'venga va luego solo dormido 3 horas',\n",
       " 'hastag hastag uf freudianos vinieron tomar café miércoles aquí aún acampados pasillo bon dia',\n",
       " 'un vibrador estudiase adapta necesidades lo mejor hastag hastag inteligente web',\n",
       " 'hastag ja ja gente cansina partir cierta edad paciencia',\n",
       " 'hastag holaaa origen vegetal nunca nunca nunca origen animal buen día',\n",
       " 'hastag si trabajando 😭 vistas sí buenos días',\n",
       " 'hastag troleaer usar store data valdría auqn tampoco preciso hastag',\n",
       " 'oir hastag hastag haran directos wow acuerdan server mas seguro vaya jaja',\n",
       " 'hastag sí en realidad pensaba q recortar tanto la querencia mutua',\n",
       " 'hastag hastag no si pagar pagaba bien tranquilo',\n",
       " 'mejor to examino justo dia cumple bueno vida',\n",
       " 'hastag hastag hay historia escrita salen mujeres tan popular estudia escuelas',\n",
       " 'hastag genial muchas gracias mi fanpage facebook sigue activa también aunque',\n",
       " '2 si eres crush máximo aunque considere mejores amigos',\n",
       " 'digo mala memoria vecino tatuado nombre mujer',\n",
       " 'hastag mil gracias compartir enric feliz noche',\n",
       " 'hastag eres emo fav lo emo coña eres quiero verte cantas bien maldito',\n",
       " 'hastag hastag hastag hastag hahahahha mmm nos vamos en 2 semanas hotel pa primera noche 😂 😂 😂',\n",
       " 'hastag sigue mejor pelorizo feelsbadman',\n",
       " 'limo sadboy mierda vida maravillosa',\n",
       " 'hastag ufff imaginas claro encima dándome golpes cabeza humor',\n",
       " 'aquí segundo vídeo día 14 días tener pes 2017 demo fifa 17 wuouuoo web',\n",
       " 'verdad duele vida recordar momento tan bonito',\n",
       " 'hastag ehhh sido yo digo serio cojones roben puto identidad',\n",
       " 'que mal rollo da dormir sola diariamente casa',\n",
       " 'lo ma ser optimista que lo hay aunque sea minúsculo lo meno hecho confiar web',\n",
       " 'bueno links guardados summer finale pll espero funcionen hmm',\n",
       " 'hastag pues deja hacer pervertido viciar haz vida',\n",
       " 'dicen twitch va mal hoy aún así pondré stream 20 mins',\n",
       " 'hastag a haces favor encantado hastag',\n",
       " 'hastag hastag hastag hastag elpartido pa lugano',\n",
       " 'hastag hastag gracias feliz noche ambos',\n",
       " 'necesito dos rodamientos marcos mañana traemelos porfa',\n",
       " 'hastag podemos usar términoequipo pepino',\n",
       " 'hastag hastag hastag hastag yo creo consumo cerebral inversamente proporcional uso',\n",
       " 'tengo dinero nivel nivel 6 bitches así historia farmear contratos muerte',\n",
       " 'hastag hastag hastag claro claro buscamos justo',\n",
       " 'hastag hastag dios guapo sale pena españa supongo saldrá',\n",
       " 'hastag si salón 25m pasará ser 50 … igual si caso ahora mismo ideal',\n",
       " '¡ eh hastag hastag pero nosotros si vendéis mandarlo valencia eh hastag hastag',\n",
       " 'hastag quedo tónic sabes clásico',\n",
       " 'hastag es verdad prefiero mil veces estea sana ir verla concierto encuentre mal',\n",
       " 'hastag hastag por desgracia vende riñastrifulcaspeleasal cuello mátalo',\n",
       " 'hastag propio ellos ensucio manos personas dependientes',\n",
       " 'hastag te comprendodominar anular haces conmigo hablarme voz altaok',\n",
       " 'hastag ¡ enhorabuena eres ganadora sorteoescríbenos mensaje privado darte premio',\n",
       " 'hastag ¡ enhorabuena eres ganadora sorteoescríbenos mensaje privado darte premio',\n",
       " 'hastag habrá conozco tb mucha etimología popular falsa el wiktionary mirar va bastante bien',\n",
       " 'hastag claro sí parís merece mil visitas',\n",
       " 'hastag vamos primera hora viendo hastag hastag bienvenidas alternativas emergentes',\n",
       " 'hastag pffftambien gente caia genialy da muchisima pena',\n",
       " 'la persona vale algo otra cosa creas maravilloso arte puedes desplegarfingelo exprese',\n",
       " 'hastag le conocí 2011 pareció inteligente respetuoso respetable combativo si',\n",
       " 'hastag hastag hastag malo pide pib déficit 4 4000 millones gastados ingresados jaja',\n",
       " 'hastag legionnovia lo bonito legión novia pack asi juegas ella',\n",
       " 'hastag asi va quedar encuentre villa nueva calidad',\n",
       " 'doght amabledoght cariñosotodos quieren doghtdisponiblesen ninguna parte',\n",
       " 'hastag especie titanic versión cutre no',\n",
       " '15 no gusta término 16 meh 17 depende 18 no 19 un perrete prechiocho 20 no 21 no 22 el año pasado',\n",
       " 'hastag doy gusta rt cuenta privadavale',\n",
       " 'hastag practico ya no dijiste hacías prácticas',\n",
       " 'hastag hastag muchas gracias feliz noche ambos',\n",
       " 'hastag hastag hastag sí epónimos rebeca',\n",
       " 'hastag mago dejaras vídeo subido youtube ahora puedo ver',\n",
       " 'hastag llévales bibliografía la evidencia educación médica existe',\n",
       " 'si papá vas regalar piano tranquilo dejo preocupes',\n",
       " 'hastag hastag ojo nombres hsm ponen dignos',\n",
       " 'me retuitea gente conozco amigos mensajes abiertos sé haser',\n",
       " 'hastag espero guste escocia precioso',\n",
       " 'hastag hastag hastag hastag hastag hastag dulce leche hizo daño',\n",
       " 'hastag hastag meto sacar cosas contexto llevarlas terreno diferente',\n",
       " 'hastag en castalla alicante oportunamente primeros mes celebran fiestas moros i cristians i fiesta',\n",
       " 'hastag gracias mónica feliz noche guapa',\n",
       " 'encantado hastag siempre bienvenido espero visitaros málaga',\n",
       " 'hastag ¿ y agente cni deseas buena noche pobre',\n",
       " 'hastag ostia jugaba primeros snes japonés y jodido acabé me encantaban juegos',\n",
       " 'hastag hastag vale san confundio oido hacía escuchaba cara sol jajaja',\n",
       " 'hastag siquiera llegué ver competiciones río directo solo barras asimétricas aliya',\n",
       " 'hastag hastag hastag hastag ah no se uso sickrage entiende divisiones',\n",
       " 'hastag hastag no sabe mierda buena',\n",
       " 'hastag hastag 1 coincido imaginaba 4k notaría distancias pensando pasar netflix uhd',\n",
       " 'hastag siento juan sabes hetero',\n",
       " 'hastag hastag tiro pedos princeso',\n",
       " 'mi móvil deja abrir grupos 600 mensajes así regalo',\n",
       " 'hastag pues sí precioso bien puedes ver vídeo',\n",
       " 'hastag veo ahora mismo fácil pnv mañana dejado clarito piensa diálogo aquí',\n",
       " 'hastag bueno piro piscina hace día estupendo',\n",
       " 'hastag hastag suma semana verte abocado elecciones',\n",
       " 'isco juande seria titular sabeis mister van mas tissones',\n",
       " 'esto salir punto casa bus tarde 10 min llegar vaya muuuy tranquilo gusta eh',\n",
       " 'hastag hastag puede hacer directo ps4 capturadora',\n",
       " 'me meo voces españolas encima pronuncian raven raiven vez reiven',\n",
       " 'hastag y noel significa navideño pedo asumimos pero alguien poner primera vez',\n",
       " 'muchas ganas mimitos sabia pasaria ahora pos ser fuerte',\n",
       " 'verdadera eriichii dado besitos so marrana',\n",
       " 'hastag ay pues gracias al menos ves ingenioso ayer dijeron insufrible sarcasmo ácido',\n",
       " 'hastag vale vuelvo preguntar no sabia siguiera',\n",
       " 'hastag feria tierra solo espero éxito ser gran anfitrión',\n",
       " 'hastag hastag gracias por cierto vete rellenando bodega 17 allí',\n",
       " 'hastag hastag ¡ enhorabuena eres ganadora sorteoescríbenos mensaje privado darte descuento',\n",
       " 'hastag nada menos compré guitarra eléctrica salió parecido comprado thomann',\n",
       " 'un poquito ministerio tiempo dormir mañana hacer muchas cosas bona nit',\n",
       " 'hastag si algún día rica llevo conmigo verlo sea promesita 👉 👈',\n",
       " 'no no ya quiero declaro soltera partir ahora hecho daño hastag',\n",
       " 'hastag corazón dejó palpitar papa frita empezó capítulo hastag',\n",
       " 'lo mejor todola felicidad felices pronto veremos boda jp … web',\n",
       " 'hastag de jo veces solo hace falta esperar tener paciencia',\n",
       " 'hastag lo mejor amiguitos defendiéndole diciéndome pase página deja paz',\n",
       " 'hastag peor todo verdad cómo da vergüenza dan ganas llorar',\n",
       " 'hastag emberdá dibujos paint mejores',\n",
       " 'hastag hastag creo duran tantos amores eternos',\n",
       " 'hastag preguntas mediocres respuestas ingeniosas',\n",
       " 'hastag oye madre cortó leche mirame 185 xd aunque enfermizo pequeño ahora leche x',\n",
       " 'ya llegó unidos podemos hastag hastag acaban llegar juntos hastag',\n",
       " 'hastag hastag destrozar posibilidades gente guapa',\n",
       " 'hastag bueno fuistes decir quevaaa poca cosa tambien',\n",
       " 'hastag hastag tio libros encantaban verdad mensaje pasado clasista xd',\n",
       " 'hastag hastag idiota pude comprarlas hace días parezco nuevo',\n",
       " 'ido ducha olvidado coger ropa ahors salir alguien ventana ve desnuda',\n",
       " 'hastag ¡ hastag lo hablaremos hablaremos seguro coincidimos convención',\n",
       " 'hastag hastag hastag hastag hastag hastag hastag hastag abrazo mutuo',\n",
       " 'hastag jajajaja creo improvisto eh muchas grasias hermosa',\n",
       " 'hastag tu vida parido grandisimo hijo gran p maravilloso hombre',\n",
       " 'hastag dice él así nada tómate tiempo preparado llegará',\n",
       " 'hastag te puedes creer iba poner mismo te quiero nini',\n",
       " 'hastag pues disfrutarás dulce suelo',\n",
       " 'hastag hastag jaja peor juegos ubisoft tan malos si malo ser solo sony',\n",
       " '¡ ya contactado ganadores hastag atentos mañana lanzamos nuevo hastag web',\n",
       " 'cuando puedo dormir escribo preocupa libreta alguien regaló somnífero instantáneo',\n",
       " 'hastag omito con alguien pensaba claro',\n",
       " 'hastag espero bien ahora gran semana',\n",
       " 'hastag si puedo trabajar madrid buenos aires bogotá feliz hihihihi',\n",
       " 'al principio gusta acaba gustando deja extrañas',\n",
       " 'hastag hastag hastag término coloquial análisis',\n",
       " 'encantaría tener buena cámara saber usarla perfección',\n",
       " 'hastag cualquier caso gracias welcome back qué cálido sido',\n",
       " 'han buena iniciativa hastag encima amigos',\n",
       " 'hastag dicho bienvenida nuevo pero hecho ir allí sabes suele haber bueno',\n",
       " 'ahora daba manguerazo ver si quitaba cara hola dormido 2 horas',\n",
       " 'hastag anda subido ahí paseo nuevo fiestas semana grande',\n",
       " 'hastag tio librillo recien comprao der chino toca polla 😂 😂 vente pa casa doy piti',\n",
       " 'janowiiicz ole cojones ole set over no pago fiestas si algún cubata buenos diiias mundo',\n",
       " 'hastag póntela das vuelta málaga porfa',\n",
       " 'me explotado vaso mano forma tan bonita empezar día',\n",
       " 'hastag significa resultado gracioso cómo dicho como comentamos final 2016 imposible traerlos',\n",
       " 'hastag es asqueroso esto amiguitos ex diciendo normal pegase tengo mucha mala leche increíble',\n",
       " 'alguien sabe puedo intercambiar photocards exo oo',\n",
       " 'ahhh todo mundo manda audios puedo escuchar canciones tranquila',\n",
       " 'hastag somos pequeños modestos podemos abarcar más',\n",
       " 'hastag 3 ediciones originales buenono aguanto demasiado antiguos',\n",
       " 'hastag que bonita zona entreno escogieron',\n",
       " 'hastag verlo tio jaja merece hacia tiempo merecia',\n",
       " 'hastag veía hermano jugar malas pulgaaas',\n",
       " 'hastag buen artículo carlos va lucas tampoco dramatizar casi haces llorar',\n",
       " 'despues sigues pendiente sabes autoengañarse is moreee easyy',\n",
       " 'hastag acabo hora jajaja expectante ver si tocado hastag llevarlo lados puesto',\n",
       " 'algún alma caritativa prestar pokemon',\n",
       " 'hastag mejor cuando pagan esa nasa fin cabo pagamos que chulada',\n",
       " 'hastag puedo cambiar opinión cara difícil vale pasta',\n",
       " 'hastag aunque aquí precisamente ninguno dos da igual haces',\n",
       " 'hastag im back aunque desaparecida combate alegro lot graduación dr mucho ánimo mir un abrazo grande',\n",
       " 'hastag hastag hastag bastante bastante ganitas verlo romper premier',\n",
       " 'hastag nah pringao lleva aguantándole cansado',\n",
       " 'hastag ahora abel resfriado ke juntos tiempo jajaja curro bien',\n",
       " 'hastag aún no si jugable tgs creo tarde demasiado',\n",
       " 'está puesta película cómo entrenar dragón autobús pantalla demasiado lejos topo',\n",
       " 'último dia elche ahora toca benidorm',\n",
       " 'hastag aunque lucha jodidos punto',\n",
       " 'de aquí semana hará cuatro años vivo barcelona creo momento dado mal',\n",
       " 'hastag hastag si vaticina club 2 3 años malas palabras vale',\n",
       " 'hastag hecho nintendo magic principal fuente información especiales biográficos iwata',\n",
       " 'tio dawn dispara beth daryl dispara inmediatamente pone llorar',\n",
       " 'hastag bueno algo cuando encima sabes pasan cosas peor dicho ánimos',\n",
       " 'ha ardido caravana aparcada lado playa fuego sofocado bomberos … web',\n",
       " 'hastag pero gral acuerd opinión interesante tendría q escribir entrada blog hastag',\n",
       " 'hastag cosas hilo discrepo como sigues hace poco aviso hago rt interesante coincida',\n",
       " 'hastag hastag hastag hastag está camino friéndose bajo sol sevillano',\n",
       " 'hastag pero verdad general voy mal motivación casi revés sobra motivación y ego',\n",
       " 'hastag hastag hastag hastag ten seguro si queda libre cuento contigo',\n",
       " 'hastag ja ja imagino cada forma distinta yo compro gadgets veo peli como interstellar',\n",
       " 'mi última partida jugada sona support la grandes razones jugar sona web',\n",
       " 'me cansando dar rt d rt suspendes pa luego suspender fijo',\n",
       " 'mi madre deja ponerme rubia pelo morado',\n",
       " 'por parte necesito cena sábado volver juntos lado pienso vamos separar',\n",
       " 'he hecho primer puré verduras vida parece pota sabe mal',\n",
       " 'por ser pelo tan goals mojado tan mierda despues',\n",
       " 'hastag duchate rapido babe sueño',\n",
       " 'eso echar menos propia casa leyendas urbanas',\n",
       " 'tengo ganas bronca aburrido proximo tweet abro cajon mierda',\n",
       " 'ea mañana despierte empieza fiesta estare distraido largo viaje valencia cadiz',\n",
       " 'hastag las chicas fuertes guapas luchar oscuridad mundo mucha',\n",
       " 'hastag hastag hastag hastag hastag hastag pequeñas diferencias enfrentan dos españas',\n",
       " 'hastag ¡ muchas gracias respuesta cualquier cosa aquí necesites ¡ saludos',\n",
       " 'estaba autobús benidormmadrid lado puertas abiertas tentado subirme volver',\n",
       " 'hastag es solo piensa todas canciones tristes loser ejemplo basadas experiencia',\n",
       " 'hastag hastag no no ¡ dijo dejaban surfear cornea que quedaban quietecitas',\n",
       " 'hastag hastag ¡ hola ana te contestado mensaje privado limitación caracteres ¡ gracias',\n",
       " 'hastag hecho confirmo caminos casitas mismas',\n",
       " 'si algún alma caritativa mac sabe instalar excell windows diga pls',\n",
       " 'hastag hastag derecha aparecen personajes jugados ranked vamos jugar',\n",
       " 'ya decía bueno iba llegar tarde temprano espero poder equivocarme',\n",
       " 'voy soñar fórmulas matemáticas historia mundo pasiva inglés le explotará cabesa',\n",
       " 'hastag pues escapó y dimos encontrado menos 5 personas buscando',\n",
       " 'hastag yo veces hecho menos pelo largo mismo cosas',\n",
       " 'hastag cosas molan a mejor conoces speakingio zack holman',\n",
       " 'cansado tenes albacete cansado sino vuelvo rosario verano ojo',\n",
       " 'he visto documental pirámides moais predicciones ahora da cosita dormir sola',\n",
       " 'la regla destrzando cuerpo quiero vomitar duele tripa cabeza fiebre mareada pf',\n",
       " 'hastag hastag hastag hastag hastag hastag qué bonito pagar horas extras también así',\n",
       " 'hastag nazismo fangirleo filosofía vida doble moral juju',\n",
       " 'hastag hastag si ayer reunidos moncloa final repetir elecciones 😂 😂 😂 está responde',\n",
       " 'hastag cuando borré twitter jeje cuenta nueva aunque mismo nick',\n",
       " 'ya oficia quedamos konoplyanka hastag eso sí alternativa barata reus bundes web',\n",
       " 'con ganas llegue septiembre y comenzamos mañana nueva temporada hastag hastag',\n",
       " 'hastag gracias ídolo aunque solo persona normal corriente escribe libros',\n",
       " 'hastag todos siempre carita super buenos adorables luego lían',\n",
       " 'hastag hola vengo intagram verdad seguia hola igualmente 🙄 ❤ ️',\n",
       " 'llevo toda mañana haciendo cosas podido ver moon lovers signal',\n",
       " 'no sensación ir trabajo din hastag eu na ofi lendo hastag tampouco teño web',\n",
       " 'hastag hastag hastag hombre leído la pregunta es ¿ cuál pretende ser titular completo',\n",
       " 'las horas sigo ser capaz ponerme estudiarel sueño puede conmigo',\n",
       " 'voy sentar sobrina médico dicen súper mal está ocupado digo yo pues desocupa niña',\n",
       " 'hastag sabueso malo leñador mola quiero minero mago hielo',\n",
       " 'hijos de puta dejad darle rt preguntadle alguien cuantos dao rt solo respondido 1',\n",
       " 'hastag hastag anda castrón comí aquel trocín pequeño cortaste',\n",
       " 'a nunca podrán hacer broma cojo llamadas menos ocultas',\n",
       " 'hastag hastag hastag pequeña donación hará felices miles chicas hastag',\n",
       " 'hastag muchas gracias feliz haber formado parte edición master',\n",
       " 'cago puta vida puto vecino hacer ruido buena mañana',\n",
       " 'hastag llama wolf mascota hastag apoyo deportistas paralímpicos más inf web 😃',\n",
       " 'hastag hastag genial rebeca este deseo escribirlo seguiré escribiendo linea práctica',\n",
       " 'hastag hoy trabajo compañía hastag trabaja rodeado gente maja hastag hastag',\n",
       " 'hastag calla ahora día libre horas pasan rápido poniendo copas bar',\n",
       " 'hastag hastag como mejor luchar crisis unidos siempre enfrentados lo claro',\n",
       " 'así leyendo líneas discurso hastag igual vota rajoy hastag',\n",
       " 'hastag precaución amigo conductor senda peligrosa lo importante llegar',\n",
       " 'hastag hola julio pizzas lleguen mejores condiciones marcamos pizza cortamos',\n",
       " 'ayer día emociones encontrada acerca septiembre lado personita siempre hace sonreír',\n",
       " 'no silencio calla es vida suspendida brotan ganas bailarla hastag web',\n",
       " 'qué bonito ser mujer joda planes tenias despierte dolor ovarios 😡 😡 😡',\n",
       " 'hastag casi nadie hablando tema y tampoco puedo encontrar vídeo sad',\n",
       " 'hastag ¡ hooola buenas soy ojos rojos fiesta elle ¡ te sigo',\n",
       " 'hastag 178 sabes caes bien futuro daré coñazo dudas diseño gráfico',\n",
       " 'hastag ¡ muchas gracias carla de momento caído tres así empieza mal día',\n",
       " '¡ ¡ muchas felicidades día cumpleaños buen amigo hastag a pasarlo grande compañero',\n",
       " 'hola reviento haberme comido box kebab ayudadme morir gracias',\n",
       " 'después casi mes entero él puff espero echar temporadas tan largas verlo',\n",
       " '¿ no pasa ferias podéis ver persona unas 29 veces justo quieres ver lela ves jodida vez',\n",
       " 'tambien subire changelog largo dias estaros preparados viene golpe',\n",
       " 'hastag que guapísimo dejaron mendizorroza san mames guapos mundo ¡ ¡ dudaopinión',\n",
       " 'baby hastag hastag suerte semana pasada demasiado grande mi',\n",
       " 'hastag ja ja ja ja ja teneis ojino bandera japón hijos gran puta',\n",
       " 'hastag últimos días sido detrás otra ahora mismo vamos la fe miren pau vez bronquitis',\n",
       " 'creo pocas veces tan enfadado mañana bueno uncharted 4 ejercicio animado',\n",
       " 'echo menos lexa bragas vuelan antes ya hace falta limpiarlas cada 5 minutos',\n",
       " 'hastag lo pasado grande 4 estaciones una auténtica gozada toda familia',\n",
       " 'plan perfecto terminar fin semana hastag hastag después hastag hastag',\n",
       " 'utopí audiencia estable media 85 imdb etc pero cancelada no puede tirar manta farmacéutica',\n",
       " 'vida mierda primero enamoro macedonio ibiza ahora timbales panorama',\n",
       " 'tranquilos eldinero camacho seguro mañana empiezan escabadoras arraijanal',\n",
       " 'hastag hastag pellegrino dotado equipo gran consistencia defensiva y buenos fichajes',\n",
       " 'hastag queda bien tio espero esteis pasando buen verano',\n",
       " 'hastag denada porcierto hazme hueco agenda combates quiero ver capaz monotype mio',\n",
       " 'esta semana sido tan chachiosa puedo decir verano acabado manera decente lloro',\n",
       " 'hastag hastag hastag más bien imposible a ser q convierta islam claro',\n",
       " 'apenas muerto miedo pasando plaza hace hoguera san juan toda oscura',\n",
       " 'hastag mclaren parece q mejora 15s mas rápido año pasado qualy mantuvo raya williams spa',\n",
       " 'como dice hastag escapa verano furgón blindado hastag hastag hastag web',\n",
       " 'no ganado ganaré concurso me esperabasoy gafe fin cabo suerte nunca parte hastag',\n",
       " 'en verdá llevo jugar pokemon go volví galicia ver madrid sale guay deprime',\n",
       " 'mi padre dado manotazo móvil gracias dios roto sido cristal templado',\n",
       " 'quiero pelo negro azulado sé cómo conseguirlo tintes color echado hecho nada',\n",
       " 'hastag diciendo feminismo contrario machismo hembrismo q dices existe',\n",
       " 'hastag hastag la verdad si bonito me alegro gustado y aprovechado paseo virtual jeje',\n",
       " 'hastag textos inglés voces japonés que desilusión llevado poner juego acostumbrarse 😕',\n",
       " '¿ lo gracioso si defiendesapoyaseres activista feminismo pues machista opresor',\n",
       " 'hastag hastag hastag genial está genial gente conozca alternativas',\n",
       " 'hastag porque nunca toca arbitro bueno hay alguno bueno valencia',\n",
       " 'nunca sensacion ¿ me harías sentir es importante blas hastag hastag',\n",
       " 'hastag hastag hastag obviamente tia jajajajaja q mayor osqqq venir ofu',\n",
       " 'hastag ahora alternativa incoherente igual inviable día hoy en creo acuerdo',\n",
       " 'hastag pero verano tampoco llegado hacer calor si comparamos pasado',\n",
       " 'hastag regales capturas imagen actividades sorpresa lo sorprendente inivitado',\n",
       " 'hastag bq aquaris m55 hace mes exacto fallan cosas busco fluido problemas buena garantia',\n",
       " 'hastag jajaja genial deseo mismo días construyen verdad',\n",
       " 'hastag ya sido grotesco parte poner foto ojeda 1030 perdona',\n",
       " 'hastag por cierto usa obligan parir niña 12 años embarazada caua violación es unas amigas',\n",
       " 'y directos deberían ser totalmente estables ya madrid solían darme problemas se vienen cosas chulas',\n",
       " 'odio ubicación tw quiero dejar puesta cada vez tuiteo sale nombre pueblo quiero',\n",
       " 'hola hastag gustaría escuchar hastag nuevo single promocional hastag hastag hastag',\n",
       " 'hastag home pues todos l m levantar prontisimo solo hora luego x j v muchas horas clase',\n",
       " 'hastag hastag brutal como parece manis separatistas catalunya espero consigan anhelada libertad',\n",
       " 'hastag hastag hastag sí usual país vasco zonas limítrofes cabía último tuit',\n",
       " 'hastag hastag ia llegó ser social fue espacio social dinamización útil fecha caducidad y abrumador',\n",
       " 'hastag juro ganó pantin classic tiro gorra salte calleron 2 encima aplastaron jaj',\n",
       " 'hastag aun hace calor piscinas cerradas mar pilla lejos el corte inglés dicho aún acaba',\n",
       " 'hastag hastag hastag hastag hastag el sigu escuela administración trilingüe ¿ le parece hastag',\n",
       " 'la envidia conociendo jenko danielle ahora mismo normal es aunque coja avión llego',\n",
       " 'me alegro vosotros sois familia ideal hastag aqui v si niño erik si niña esther jaja 💗',\n",
       " 'hastag ojalá personaje pero examen 12 de todas formas hablamos semana siguiente dicho fernando',\n",
       " 'hastag necesito se acaba romper snifff snifff gran podcast saludos a coruña españa',\n",
       " 'bueno pues vamos recuperar tiempo perdido empezare descargarme manchester city west ham ver tal',\n",
       " 'hastag hastag hastag hastag hastag gran capacidad análisis hastag entrevista hastag',\n",
       " 'hastag domingo encontremos gloria bendita ocio magia besos compartidos amor duende',\n",
       " 'hastag hastag hastag hastag hastag hastag hastag gracias paso genial haciéndolo',\n",
       " 'hastag en algún momento empecé mirar suelo cielo estoy harto llorar así intento sonreír',\n",
       " 'hastag tengo entendido vídeo salen trampeadores dedican atrapar luego liberar lobos muerte',\n",
       " 'ahora empiezan jornadas toros ciudad pongo triste ver seguimos evolucionar muchas cosas',\n",
       " 'que dificil retomar estudios despues dos meses tocar libro hastag hastag hastagexamen',\n",
       " 'hastag hastag hastag ¡ cada vez más las risas aseguradas entonces mañana preguntamos dadlo hecho',\n",
       " 'hastag digamos bastante mayor q si pone tontaponerla rodillas darle azotes culete',\n",
       " 'hastag hastag hablaba normal alguien q usted metiera insultando debería tener educación',\n",
       " 'hastag muchas felicidades día cumpleaños sensual espero pases genial recibas regalos suculentos',\n",
       " 'hastag septiembrees bonito retarsees increible puedes aprender medirse obstáculoeres joyita corona',\n",
       " 'hastag esa posibilidad contemplada y si hiciera falta x bien españa responsabilidad volvería hacerse dudar',\n",
       " 'no puedo evitar ir beber lata refresco acordarme hastag family terminar sirviéndome vaso',\n",
       " 'hastag ya gente capaz hacer eso yo si corazón pide poner cara seta no capaz sonreir',\n",
       " 'hastag hastag bastante tener dos puntos salud además enamoradísima princesa princesa mala',\n",
       " 'hastag sí leído eso luego leído otoño noviembre diciembre dado bajón además ds juego',\n",
       " 'hastag el efecto largo plazo desafección mayoritaria izquierda el votante cansa queda casa',\n",
       " 'hastag caso humanidades latín griego vida dado latin griego así iria fatal',\n",
       " 'urgente venta my name tikets tengo dos tickets ultimate vip podemos ir vendo baratos contactad conmigo',\n",
       " 'hastag ya jugando ratón gato no solo voy quedo dormir mañana hotel quedamos fijo',\n",
       " 'hastag hastag toda razon alvaro soy gran fan enseñaste casi clash of clans mejor',\n",
       " 'hastag ajajajaja benidorm vi dos mujeres carritos bebé perros dentro vimos quedamos flipados',\n",
       " 'hastag pero gusta idea ver chicas fuertes luchando estilo dragon ball z versión real',\n",
       " 'hastag juro acuerdo solo sé alba hizo vídeo inmitándole recuerdo absolutamente original',\n",
       " 'hastag ilustres jose qué fácil salen insultos viendo cómo empezado burgos murcia',\n",
       " 'han talado abedul gigante esquina ríos rosas castellana ¿ alguien sabe porqué hastag',\n",
       " 'demasiadas cosas raras tiempo anda follen 0 complicaciones 0 historias me voy playa viva españa',\n",
       " 'acabo comprobar linkedin si antiguo alumno gran recuerdo hastag te deseo mejor hastag',\n",
       " 'hastag uy bien vendría colocar wifi portátil flautas saxo nariclete partituras desorden',\n",
       " 'hastag alegro mucho importante darnos cuenta gran valor podemos aportar encontrar misión',\n",
       " 'yo cansadete repente habla amigo jugar ratejo beta battlefield 1 quién dice chico',\n",
       " 'es decente tener gobierno democrático cumpla función politica no tener gobierno representa hastag',\n",
       " 'hastag d cines hacen findeelaccesible icariaq bajado preciosno pagan xdo matinales diagmar',\n",
       " 'subtitulos bojack horseman tan hablando el pais el mundo el mundo today alaska mario ye terrible',\n",
       " 'hastag hastag intentado espero q próximos días veamos ilustración práctica q ayude clarificarlo',\n",
       " 'hastag bien ahora solo falta pongáis estación hastag poder ir bici casa',\n",
       " 'hastag uno aprende mejor momento crear bonitos recuerdos presente pasado fue toca elegir va ser',\n",
       " 'hastag hastag hastag gracias único hace feliz escuchar música adivinar ganar',\n",
       " 'hastag pues pasó experiencia paranormal allí noche vuelto pasé muchísimo miedo',\n",
       " 'esta decidido ¡ habrá modo carrera canal fifa 17 si tweet llega 30 likes desvelaré equipo hará',\n",
       " 'hastag genial ten cuidado vigilado hecho casi juguete syma x5c casi entraña riesgo',\n",
       " 'tengo dos horas seguidas latín griego dos horas seguidas misma profesora menos mal cae bien pero dos horas',\n",
       " 'hastag cine romántico vale yo regalar dildos lubricantes comida buen uso luego doy masaje',\n",
       " 'hastag llevo medio verano debajo dichosa máquina menudo verano largo año sofoco',\n",
       " 'cuando suben tres fotos representando tres clases escuela música sales menos bien',\n",
       " 'hastag borrao lol',\n",
       " 'hastag sí resignado',\n",
       " 'era grande parece',\n",
       " 'hastag deprimida',\n",
       " 'serio tan tontos',\n",
       " 'iré bgw pequeña sorpresa',\n",
       " 'no ahora feliz',\n",
       " 'mañana va ser día guay',\n",
       " 'qué jodido ser hastag hoy',\n",
       " 'hastag pasado 76',\n",
       " 'estoy preocupado bueno',\n",
       " 'hastag activados',\n",
       " 'hastag dos triste',\n",
       " 'hastag 9 buenico 💜',\n",
       " 'hastag dura',\n",
       " 'ya encuentro mejor',\n",
       " 'alguien duo noche',\n",
       " 'que den buenas noches',\n",
       " 'hoy día especial feliz 11s',\n",
       " 'hastag mas pobre',\n",
       " 'quiero ir diada',\n",
       " 'hastag directo',\n",
       " 'hastag tanto impartas',\n",
       " 'hastag no gilipollas',\n",
       " 'un día casa sola fín',\n",
       " 'hastag pero si inocente soy',\n",
       " 'luego rara',\n",
       " 'esto feliz mola',\n",
       " 'ganas ff xv serio',\n",
       " 'hastag si tan despreciable',\n",
       " 'las 7 cuarto aquí',\n",
       " 'hastag des serio',\n",
       " 'hastag bonita',\n",
       " 'hastag tan facil',\n",
       " 'ya picado bichos',\n",
       " 'me voy despejarme poquito ahi',\n",
       " 'que coraje dan lxs prepotentxs',\n",
       " 'ah bueno dinero',\n",
       " 'encima mal amores tio',\n",
       " 'en realidad tan blanca',\n",
       " 'tengo md abiertos crushes',\n",
       " 'lo peor aposta',\n",
       " 'hastag tia raro',\n",
       " 'soy obvia suda',\n",
       " 'hastag ps conmigo',\n",
       " 'hastag normal',\n",
       " 'mensaje directo',\n",
       " 'hastag es tranquila',\n",
       " 'me muriendo malo',\n",
       " 'hastag bonica',\n",
       " 'a ver súper cutie tal',\n",
       " 'me encuentro mal hoy trabajar',\n",
       " 'hastag bof tio duro',\n",
       " 'hastag intimidas',\n",
       " 'hastag oye tan burguesito sbs',\n",
       " 'hastag no lemado',\n",
       " 'xq ta cerrao estanco',\n",
       " 'hastag grasioso',\n",
       " 'hastag pero rica',\n",
       " 'hastag eso antiguo',\n",
       " 'mi tl vacía horas',\n",
       " 'ya uno cheto hastag',\n",
       " 'estoy sensible lloro']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 980 candidates, totalling 4900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4900 out of 4900 | elapsed: 16.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3877597477137727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 100,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.8,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (3, 6)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", LogisticRegression())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__C\":[1,10,100,1000,10000]\n",
    "             }\n",
    "\n",
    "clf_lr = GridSearchCV(pipe,\n",
    "                      param_grid,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=2,\n",
    "                      scoring=\"f1_macro\")\n",
    "\n",
    "clf_lr.fit(train_clean, y_train)\n",
    "print(clf_lr.best_score_)\n",
    "clf_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1960 candidates, totalling 9800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9800 out of 9800 | elapsed: 39.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3848031342070236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10,\n",
       " 'clf__kernel': 'linear',\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.8,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (4, 5)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", SVC())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__kernel\":['linear', 'rbf'],\n",
    "              \"clf__C\":[1,10,100,1000,10000]\n",
    "             }\n",
    "\n",
    "clf_svc = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_svc.fit(train_clean, y_train)\n",
    "print(clf_svc.best_score_)\n",
    "clf_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 980 candidates, totalling 4900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4900 out of 4900 | elapsed: 28.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31993195546535513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__n_estimators': 50,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.5,\n",
       " 'tfidf__min_df': 3,\n",
       " 'tfidf__ngram_range': (4, 5)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__n_estimators\":[50,100,150,200,300]\n",
    "             }\n",
    "\n",
    "clf_rfc = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_rfc.fit(train_clean, y_train)\n",
    "print(clf_rfc.best_score_)\n",
    "clf_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1372 candidates, totalling 6860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   42.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36833977444122495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6860 out of 6860 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__n_neighbors': 5,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.3,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (3, 4)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__n_neighbors\":[3,5,7,11,15,21,25]\n",
    "             }\n",
    "\n",
    "clf_knn = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_knn.fit(train_clean, y_train)\n",
    "print(clf_knn.best_score_)\n",
    "clf_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation over development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.56      0.67      0.61       219\n",
      "         NEU       0.11      0.07      0.09        69\n",
      "        NONE       0.39      0.19      0.26        62\n",
      "           P       0.54      0.57      0.55       156\n",
      "\n",
      "    accuracy                           0.50       506\n",
      "   macro avg       0.40      0.38      0.38       506\n",
      "weighted avg       0.47      0.50      0.48       506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,6),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.56      0.71      0.63       219\n",
      "         NEU       0.07      0.01      0.02        69\n",
      "        NONE       0.32      0.19      0.24        62\n",
      "           P       0.55      0.63      0.59       156\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.38      0.39      0.37       506\n",
      "weighted avg       0.46      0.53      0.49       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.5,\n",
    "                                 min_df=3,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=50))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.57      0.71      0.63       219\n",
      "         NEU       0.14      0.10      0.12        69\n",
      "        NONE       0.38      0.19      0.26        62\n",
      "           P       0.55      0.53      0.54       156\n",
      "\n",
      "    accuracy                           0.51       506\n",
      "   macro avg       0.41      0.38      0.39       506\n",
      "weighted avg       0.48      0.51      0.49       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,4),\n",
    "                                 max_df=0.3,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.52      0.63      0.57       219\n",
      "         NEU       0.15      0.12      0.13        69\n",
      "        NONE       0.43      0.32      0.37        62\n",
      "           P       0.55      0.49      0.52       156\n",
      "\n",
      "    accuracy                           0.48       506\n",
      "   macro avg       0.41      0.39      0.40       506\n",
      "weighted avg       0.47      0.48      0.47       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=10, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.53      0.83      0.65       219\n",
      "         NEU       0.10      0.01      0.03        69\n",
      "        NONE       0.70      0.11      0.19        62\n",
      "           P       0.59      0.55      0.57       156\n",
      "\n",
      "    accuracy                           0.55       506\n",
      "   macro avg       0.48      0.38      0.36       506\n",
      "weighted avg       0.51      0.55      0.48       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=10, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.58      0.73      0.65       219\n",
      "         NEU       0.17      0.12      0.14        69\n",
      "        NONE       0.51      0.31      0.38        62\n",
      "           P       0.57      0.54      0.55       156\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.46      0.42      0.43       506\n",
      "weighted avg       0.51      0.53      0.52       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=100, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS\n",
    "{'clf__C': 100,\n",
    " 'clf__kernel': 'linear',\n",
    " 'tfidf__analyzer': 'char_wb',\n",
    " 'tfidf__max_df': 0.8,\n",
    " 'tfidf__min_df': 1,\n",
    " 'tfidf__ngram_range': (3, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=100, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.txt\", \"w\") as f:\n",
    "    for id_ts, pred in zip(id_test , predictions):\n",
    "        f.write(\"{}\\t{}\\n\".format(id_ts, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
