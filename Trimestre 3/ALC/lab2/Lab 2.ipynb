{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(path):\n",
    "    # load data\n",
    "    root = ET.parse(path).getroot()    \n",
    "    ids = []\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for tag in root.findall('tweet'):\n",
    "        tweetid = tag.find('tweetid')\n",
    "        content = tag.find('content')\n",
    "        value = tag.find('sentiment/polarity/value')\n",
    "        ids.append(tweetid.text)\n",
    "        tweets.append(content.text)\n",
    "        labels.append(value.text)\n",
    "        \n",
    "    return ids, tweets, labels\n",
    "\n",
    "def parse_data_test(path):\n",
    "    # load data\n",
    "    root = ET.parse(path).getroot()    \n",
    "    ids = []\n",
    "    tweets = []\n",
    "    for tag in root.findall('tweet'):\n",
    "        tweetid = tag.find('tweetid')\n",
    "        content = tag.find('content')\n",
    "        ids.append(tweetid.text)\n",
    "        tweets.append(content.text)\n",
    "        \n",
    "    return ids, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, x_train, y_train = parse_data('TASS2017_T1_training.xml')\n",
    "id_dev, x_dev, y_dev = parse_data('TASS2017_T1_development.xml')\n",
    "id_test, x_test = parse_data_test(\"TASS2017_T1_test.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reUser = re.compile(r'@+\\w+')\n",
    "reHashtag = re.compile(r'#+\\w+')\n",
    "reWeb = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "\n",
    "def clean_text(text):\n",
    "    res = []\n",
    "    for element in text:\n",
    "        aux = []    \n",
    "\n",
    "        # Remove stopwords\n",
    "        for word in element.split():\n",
    "            if word not in stopwords.words(\"spanish\"):\n",
    "                aux.append(word)\n",
    "        element = \" \".join(aux)\n",
    "        # Normalize user tags\n",
    "        for item in re.finditer(reUser, element):\n",
    "            element = reUser.sub('#user', element)\n",
    "        # Normalize hastags\n",
    "        for item in re.finditer(reHashtag, element):\n",
    "            element = reHashtag.sub('#hastag', element)\n",
    "        # Normalize urls\n",
    "        for item in re.finditer(reWeb, element):\n",
    "            element = reWeb.sub('#web', element)\n",
    "        # Remove punctuation\n",
    "        element = element.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        res.append(element)\n",
    "    return res\n",
    "    \n",
    "train = clean_text(x_train)\n",
    "dev = clean_text(x_dev)\n",
    "test = clean_text(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n",
    "\n",
    "train_clean = list(map(\" \".join, map(tokenizer.tokenize, train)))\n",
    "dev_clean = list(map(\" \".join, map(tokenizer.tokenize, dev)))\n",
    "test_clean = list(map(\" \".join, map(tokenizer.tokenize, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hastag 100010 verdad voy decir petarda quiero mismo ‚ú®',\n",
       " 'hastag hastag hastag a√∫n le√≠do caer√°n prontito',\n",
       " 'al final sido 3h bueno ma√±ana fiesta as√≠ que no quejo',\n",
       " 'hastag tiempo cosas ahora mismo',\n",
       " 'hastag ves brillo coso hace sepan kk',\n",
       " 'tengo perrina adorable sab√©is acompa√±a habitaci√≥n voy dormir',\n",
       " 'hastag es ojeando a√±o pasado tampoco muchas canciones jajajajaja',\n",
       " 'bueno batalla final conquista despu√©s faltar√≠a revelaci√≥n',\n",
       " 'hastag ¬ø ma√±ana s√°bado 31 en d√≠a vives ma√±ana mi√©rcoles 31',\n",
       " 'hastag caminante mar niebla cuadros favoritos portada',\n",
       " 'hastag ¬° s√≠ y encantado ¬ø t√∫ visto ¬ø cu√°ndo comentamos',\n",
       " 'hastag se olvidaban grandes hastag hastag a ver si interesa hilillo',\n",
       " 'hastag mejor si pones link cuenta costado encontrarte',\n",
       " 'hastag por ten√≠a pensado verla despu√©s segunda daredevil',\n",
       " 'hastag lado manita usas vea negro',\n",
       " 'llevo despierto 8 puto mosquito volando puta oreja',\n",
       " 'hastag qu√© estupendo y ¬ø c√≥mo encargo ¬ø es estupend√≠sima versi√≥n barce',\n",
       " 'cosas enamora tostadas calentitas horas ma√±ana',\n",
       " 'hola buenos d√≠as dormido 5 acabo despertar ido sue√±o',\n",
       " 'hastag creo dise√±ado tipo propia',\n",
       " 'hastag sabes 2012 13 novela mierda perrie mala',\n",
       " 'hastag ninguna visto directo concierto madrid pronto pls',\n",
       " 'hastag esperanza realidad personaje mas oscuro',\n",
       " 'pa vez pongo ver pel√≠culas amigo queda dormido',\n",
       " 'hastag nfin puede ser peor minikayato clauyato is real',\n",
       " 'hastag que kinox quiere zi zoy buena perzona',\n",
       " 'este viernes ir√© cine ver hastag celebrar cumple por cierto invitados',\n",
       " 'podr√≠amos empezar septiembre d√≠a tres s√© guay',\n",
       " 'peor funcionaba maldita jaco quiero',\n",
       " 'aqui fardando vegano hace 2 dias amigos tuiter',\n",
       " 'hastag pues gente as√≠ mala mundo',\n",
       " 'hastag atraigo dos lados problema duro',\n",
       " 'hastag ofrecer√≠a gerald demasiado lindo',\n",
       " 'hastag direcci√≥n proyectos empresas audiovisuales',\n",
       " 'como destrozaba puto movil ahora mismo',\n",
       " 'hastag todas grandes tardes hacen pasar pasaran',\n",
       " 'hastag va t√≠a viejos madre chino los encantan comprando kiko',\n",
       " 'parece ma√±ana podr√© catar madden hoy habra directo',\n",
       " 'mi compa√±ero trabajo futuro marido pena sepa',\n",
       " 'hastag hastag fallecido hace meses hay q contrastar',\n",
       " 'hastag necesito raci√≥n atenci√≥n diaria si mantengo autoestima',\n",
       " 'hastag gracias dar me gusta tweets ¬ø que tal ¬ø est√°s trabajando cristina',\n",
       " 'hastag nuestros ni√±os andaban animado acaban vacaciones',\n",
       " 'hastag ser malo bueno salvas queremos igual',\n",
       " 'pero solo cojo 5 d√≠as cambio curro',\n",
       " 'hastag yo imaginaba as√≠ tambi√©n creen casting buen√≠simo menos contenta',\n",
       " 'despues trabajar duro merezco caprichos',\n",
       " 'hastag yo puse bonito cuanto desprecio',\n",
       " 'no encuentro mando tele poniendo nerviosita',\n",
       " 'lo mejor m√°laga poder ir ver escuadr√≥n suicida tarde',\n",
       " 'hastag insoportable eso dices ya contar√°s espero vaya genial',\n",
       " 'hastag cierto respeto y caso admiraci√≥n mutuo',\n",
       " 'hastag subido gayumbos hace nada qu√© lees paisana hastag',\n",
       " 'hastag ser mala persiana quitarte diversion',\n",
       " 'hastag ¬ø verdad s√≠ yo quit√© gran peso encima',\n",
       " 'hastag ma√±ana seguro gente buena hastag hastag',\n",
       " 'hastag hastag s√© pobre uno te acompa√±o feels',\n",
       " 'hastag hastag a√∫n as√≠ campo l edici√≥n coleccionista acabar pagarla',\n",
       " 'mantengo optimista hace dif√≠cil veces verdad',\n",
       " 'hastag hastag hastag s√≠ yo creo vamos acabar muuuy contentas',\n",
       " 'vale entiendo r√°pido bajada haber entrado hastag',\n",
       " 'hastag correcci√≥ nin pasa pola cabeza non ser segundos',\n",
       " 'ha muerto profesora volumenescultura serio bajonazo',\n",
       " 'haces vaya melancol√≠a devuelves nuevo vida',\n",
       " 'hastag tia punto acabar novena voy retraso',\n",
       " 'hastag acabo aportar peque√±a contribuci√≥n te sientes tan bien',\n",
       " 'hastag pues aludida y mismos√≠buen d√≠a',\n",
       " 'hoy mosca dej√≥ dormir cago puta vida',\n",
       " 'hastag gran gran tipo hastag',\n",
       " 'hastag entiende sue√±o adem√°s vi creo 5 primaria as√≠',\n",
       " 'hastag hastag aqu√≠ habemos saturado leer idnwhy',\n",
       " 'hastag espero mejores pronto vomitar desagradable',\n",
       " 'chancla representa asco harta madre aqu√≠ menda',\n",
       " 'hola hastag gracias informaci√≥n tomamos nota qu√© buen d√≠a',\n",
       " 'hastag gracias etiquetarme suelo hacer cosas',\n",
       " 'hastag hastag ¬ø socialistas honrados esto ox√≠moron',\n",
       " 'pasa tumblrr oo vuelto extra√±o sale musica',\n",
       " 'cada busque mecanicas haga rapido morir',\n",
       " 'hastag hastag hastag bien pienso enquistas temas secundarios ver toda pel√≠cula',\n",
       " 'ojal√° alguien llamandome bonito precioso sad',\n",
       " 'hastag hastag hacemos presi√≥n social avanza',\n",
       " 'hastag consigo ayudame porfi super importante',\n",
       " 'despues tener combo perfecto pa sentirme mierda dormir',\n",
       " 'necesito ver menos vez riri irme dormir conforme ¬ø por echar horas gala hastag',\n",
       " 'hastag barbaridad dinero aunque puedo contrata show noe',\n",
       " 'hastag ¬ø eliminada nooo eliminado pas√≥ semifinal d√≠a',\n",
       " 'hastag deseo much√≠sima suerte pr√≥xima aventura t√≠o',\n",
       " 'hastag hastag preciosa vista gran resultado enhorabuena',\n",
       " 'hastag amo aunque parece dormido quedaban 5 6',\n",
       " 'hastag hastag jajajajaja el all bran pro buenos d√≠as compis',\n",
       " 'de verdad puse gafas moradas soporto ning√∫n t√≠o',\n",
       " 'el puente mas bonito constituci√≥n a√±o alicante 1213 14 hastag directo all√≠',\n",
       " 'hastag ouch llego ver si pr√≥xima si puedo saludo',\n",
       " 'aaaa aaa tan cansada espera semana tan horrible jueves',\n",
       " 'hastag es chisme expresa matices no idea elemento cortante',\n",
       " 'venga va luego solo dormido 3 horas',\n",
       " 'hastag hastag uf freudianos vinieron tomar caf√© mi√©rcoles aqu√≠ a√∫n acampados pasillo bon dia',\n",
       " 'un vibrador estudiase adapta necesidades lo mejor hastag hastag inteligente web',\n",
       " 'hastag ja ja gente cansina partir cierta edad paciencia',\n",
       " 'hastag holaaa origen vegetal nunca nunca nunca origen animal buen d√≠a',\n",
       " 'hastag si trabajando üò≠ vistas s√≠ buenos d√≠as',\n",
       " 'hastag troleaer usar store data valdr√≠a auqn tampoco preciso hastag',\n",
       " 'oir hastag hastag haran directos wow acuerdan server mas seguro vaya jaja',\n",
       " 'hastag s√≠ en realidad pensaba q recortar tanto la querencia mutua',\n",
       " 'hastag hastag no si pagar pagaba bien tranquilo',\n",
       " 'mejor to examino justo dia cumple bueno vida',\n",
       " 'hastag hastag hay historia escrita salen mujeres tan popular estudia escuelas',\n",
       " 'hastag genial muchas gracias mi fanpage facebook sigue activa tambi√©n aunque',\n",
       " '2 si eres crush m√°ximo aunque considere mejores amigos',\n",
       " 'digo mala memoria vecino tatuado nombre mujer',\n",
       " 'hastag mil gracias compartir enric feliz noche',\n",
       " 'hastag eres emo fav lo emo co√±a eres quiero verte cantas bien maldito',\n",
       " 'hastag hastag hastag hastag hahahahha mmm nos vamos en 2 semanas hotel pa primera noche üòÇ üòÇ üòÇ',\n",
       " 'hastag sigue mejor pelorizo feelsbadman',\n",
       " 'limo sadboy mierda vida maravillosa',\n",
       " 'hastag ufff imaginas claro encima d√°ndome golpes cabeza humor',\n",
       " 'aqu√≠ segundo v√≠deo d√≠a 14 d√≠as tener pes 2017 demo fifa 17 wuouuoo web',\n",
       " 'verdad duele vida recordar momento tan bonito',\n",
       " 'hastag ehhh sido yo digo serio cojones roben puto identidad',\n",
       " 'que mal rollo da dormir sola diariamente casa',\n",
       " 'lo ma ser optimista que lo hay aunque sea min√∫sculo lo meno hecho confiar web',\n",
       " 'bueno links guardados summer finale pll espero funcionen hmm',\n",
       " 'hastag pues deja hacer pervertido viciar haz vida',\n",
       " 'dicen twitch va mal hoy a√∫n as√≠ pondr√© stream 20 mins',\n",
       " 'hastag a haces favor encantado hastag',\n",
       " 'hastag hastag hastag hastag elpartido pa lugano',\n",
       " 'hastag hastag gracias feliz noche ambos',\n",
       " 'necesito dos rodamientos marcos ma√±ana traemelos porfa',\n",
       " 'hastag podemos usar t√©rminoequipo pepino',\n",
       " 'hastag hastag hastag hastag yo creo consumo cerebral inversamente proporcional uso',\n",
       " 'tengo dinero nivel nivel 6 bitches as√≠ historia farmear contratos muerte',\n",
       " 'hastag hastag hastag claro claro buscamos justo',\n",
       " 'hastag hastag dios guapo sale pena espa√±a supongo saldr√°',\n",
       " 'hastag si sal√≥n 25m pasar√° ser 50 ‚Ä¶ igual si caso ahora mismo ideal',\n",
       " '¬° eh hastag hastag pero nosotros si vend√©is mandarlo valencia eh hastag hastag',\n",
       " 'hastag quedo t√≥nic sabes cl√°sico',\n",
       " 'hastag es verdad prefiero mil veces estea sana ir verla concierto encuentre mal',\n",
       " 'hastag hastag por desgracia vende ri√±astrifulcaspeleasal cuello m√°talo',\n",
       " 'hastag propio ellos ensucio manos personas dependientes',\n",
       " 'hastag te comprendodominar anular haces conmigo hablarme voz altaok',\n",
       " 'hastag ¬° enhorabuena eres ganadora sorteoescr√≠benos mensaje privado darte premio',\n",
       " 'hastag ¬° enhorabuena eres ganadora sorteoescr√≠benos mensaje privado darte premio',\n",
       " 'hastag habr√° conozco tb mucha etimolog√≠a popular falsa el wiktionary mirar va bastante bien',\n",
       " 'hastag claro s√≠ par√≠s merece mil visitas',\n",
       " 'hastag vamos primera hora viendo hastag hastag bienvenidas alternativas emergentes',\n",
       " 'hastag pffftambien gente caia genialy da muchisima pena',\n",
       " 'la persona vale algo otra cosa creas maravilloso arte puedes desplegarfingelo exprese',\n",
       " 'hastag le conoc√≠ 2011 pareci√≥ inteligente respetuoso respetable combativo si',\n",
       " 'hastag hastag hastag malo pide pib d√©ficit 4 4000 millones gastados ingresados jaja',\n",
       " 'hastag legionnovia lo bonito legi√≥n novia pack asi juegas ella',\n",
       " 'hastag asi va quedar encuentre villa nueva calidad',\n",
       " 'doght amabledoght cari√±osotodos quieren doghtdisponiblesen ninguna parte',\n",
       " 'hastag especie titanic versi√≥n cutre no',\n",
       " '15 no gusta t√©rmino 16 meh 17 depende 18 no 19 un perrete prechiocho 20 no 21 no 22 el a√±o pasado',\n",
       " 'hastag doy gusta rt cuenta privadavale',\n",
       " 'hastag practico ya no dijiste hac√≠as pr√°cticas',\n",
       " 'hastag hastag muchas gracias feliz noche ambos',\n",
       " 'hastag hastag hastag s√≠ ep√≥nimos rebeca',\n",
       " 'hastag mago dejaras v√≠deo subido youtube ahora puedo ver',\n",
       " 'hastag ll√©vales bibliograf√≠a la evidencia educaci√≥n m√©dica existe',\n",
       " 'si pap√° vas regalar piano tranquilo dejo preocupes',\n",
       " 'hastag hastag ojo nombres hsm ponen dignos',\n",
       " 'me retuitea gente conozco amigos mensajes abiertos s√© haser',\n",
       " 'hastag espero guste escocia precioso',\n",
       " 'hastag hastag hastag hastag hastag hastag dulce leche hizo da√±o',\n",
       " 'hastag hastag meto sacar cosas contexto llevarlas terreno diferente',\n",
       " 'hastag en castalla alicante oportunamente primeros mes celebran fiestas moros i cristians i fiesta',\n",
       " 'hastag gracias m√≥nica feliz noche guapa',\n",
       " 'encantado hastag siempre bienvenido espero visitaros m√°laga',\n",
       " 'hastag ¬ø y agente cni deseas buena noche pobre',\n",
       " 'hastag ostia jugaba primeros snes japon√©s y jodido acab√© me encantaban juegos',\n",
       " 'hastag hastag vale san confundio oido hac√≠a escuchaba cara sol jajaja',\n",
       " 'hastag siquiera llegu√© ver competiciones r√≠o directo solo barras asim√©tricas aliya',\n",
       " 'hastag hastag hastag hastag ah no se uso sickrage entiende divisiones',\n",
       " 'hastag hastag no sabe mierda buena',\n",
       " 'hastag hastag 1 coincido imaginaba 4k notar√≠a distancias pensando pasar netflix uhd',\n",
       " 'hastag siento juan sabes hetero',\n",
       " 'hastag hastag tiro pedos princeso',\n",
       " 'mi m√≥vil deja abrir grupos 600 mensajes as√≠ regalo',\n",
       " 'hastag pues s√≠ precioso bien puedes ver v√≠deo',\n",
       " 'hastag veo ahora mismo f√°cil pnv ma√±ana dejado clarito piensa di√°logo aqu√≠',\n",
       " 'hastag bueno piro piscina hace d√≠a estupendo',\n",
       " 'hastag hastag suma semana verte abocado elecciones',\n",
       " 'isco juande seria titular sabeis mister van mas tissones',\n",
       " 'esto salir punto casa bus tarde 10 min llegar vaya muuuy tranquilo gusta eh',\n",
       " 'hastag hastag puede hacer directo ps4 capturadora',\n",
       " 'me meo voces espa√±olas encima pronuncian raven raiven vez reiven',\n",
       " 'hastag y noel significa navide√±o pedo asumimos pero alguien poner primera vez',\n",
       " 'muchas ganas mimitos sabia pasaria ahora pos ser fuerte',\n",
       " 'verdadera eriichii dado besitos so marrana',\n",
       " 'hastag ay pues gracias al menos ves ingenioso ayer dijeron insufrible sarcasmo √°cido',\n",
       " 'hastag vale vuelvo preguntar no sabia siguiera',\n",
       " 'hastag feria tierra solo espero √©xito ser gran anfitri√≥n',\n",
       " 'hastag hastag gracias por cierto vete rellenando bodega 17 all√≠',\n",
       " 'hastag hastag ¬° enhorabuena eres ganadora sorteoescr√≠benos mensaje privado darte descuento',\n",
       " 'hastag nada menos compr√© guitarra el√©ctrica sali√≥ parecido comprado thomann',\n",
       " 'un poquito ministerio tiempo dormir ma√±ana hacer muchas cosas bona nit',\n",
       " 'hastag si alg√∫n d√≠a rica llevo conmigo verlo sea promesita üëâ üëà',\n",
       " 'no no ya quiero declaro soltera partir ahora hecho da√±o hastag',\n",
       " 'hastag coraz√≥n dej√≥ palpitar papa frita empez√≥ cap√≠tulo hastag',\n",
       " 'lo mejor todola felicidad felices pronto veremos boda jp ‚Ä¶ web',\n",
       " 'hastag de jo veces solo hace falta esperar tener paciencia',\n",
       " 'hastag lo mejor amiguitos defendi√©ndole dici√©ndome pase p√°gina deja paz',\n",
       " 'hastag peor todo verdad c√≥mo da verg√ºenza dan ganas llorar',\n",
       " 'hastag emberd√° dibujos paint mejores',\n",
       " 'hastag hastag creo duran tantos amores eternos',\n",
       " 'hastag preguntas mediocres respuestas ingeniosas',\n",
       " 'hastag oye madre cort√≥ leche mirame 185 xd aunque enfermizo peque√±o ahora leche x',\n",
       " 'ya lleg√≥ unidos podemos hastag hastag acaban llegar juntos hastag',\n",
       " 'hastag hastag destrozar posibilidades gente guapa',\n",
       " 'hastag bueno fuistes decir quevaaa poca cosa tambien',\n",
       " 'hastag hastag tio libros encantaban verdad mensaje pasado clasista xd',\n",
       " 'hastag hastag idiota pude comprarlas hace d√≠as parezco nuevo',\n",
       " 'ido ducha olvidado coger ropa ahors salir alguien ventana ve desnuda',\n",
       " 'hastag ¬° hastag lo hablaremos hablaremos seguro coincidimos convenci√≥n',\n",
       " 'hastag hastag hastag hastag hastag hastag hastag hastag abrazo mutuo',\n",
       " 'hastag jajajaja creo improvisto eh muchas grasias hermosa',\n",
       " 'hastag tu vida parido grandisimo hijo gran p maravilloso hombre',\n",
       " 'hastag dice √©l as√≠ nada t√≥mate tiempo preparado llegar√°',\n",
       " 'hastag te puedes creer iba poner mismo te quiero nini',\n",
       " 'hastag pues disfrutar√°s dulce suelo',\n",
       " 'hastag hastag jaja peor juegos ubisoft tan malos si malo ser solo sony',\n",
       " '¬° ya contactado ganadores hastag atentos ma√±ana lanzamos nuevo hastag web',\n",
       " 'cuando puedo dormir escribo preocupa libreta alguien regal√≥ somn√≠fero instant√°neo',\n",
       " 'hastag omito con alguien pensaba claro',\n",
       " 'hastag espero bien ahora gran semana',\n",
       " 'hastag si puedo trabajar madrid buenos aires bogot√° feliz hihihihi',\n",
       " 'al principio gusta acaba gustando deja extra√±as',\n",
       " 'hastag hastag hastag t√©rmino coloquial an√°lisis',\n",
       " 'encantar√≠a tener buena c√°mara saber usarla perfecci√≥n',\n",
       " 'hastag cualquier caso gracias welcome back qu√© c√°lido sido',\n",
       " 'han buena iniciativa hastag encima amigos',\n",
       " 'hastag dicho bienvenida nuevo pero hecho ir all√≠ sabes suele haber bueno',\n",
       " 'ahora daba manguerazo ver si quitaba cara hola dormido 2 horas',\n",
       " 'hastag anda subido ah√≠ paseo nuevo fiestas semana grande',\n",
       " 'hastag tio librillo recien comprao der chino toca polla üòÇ üòÇ vente pa casa doy piti',\n",
       " 'janowiiicz ole cojones ole set over no pago fiestas si alg√∫n cubata buenos diiias mundo',\n",
       " 'hastag p√≥ntela das vuelta m√°laga porfa',\n",
       " 'me explotado vaso mano forma tan bonita empezar d√≠a',\n",
       " 'hastag significa resultado gracioso c√≥mo dicho como comentamos final 2016 imposible traerlos',\n",
       " 'hastag es asqueroso esto amiguitos ex diciendo normal pegase tengo mucha mala leche incre√≠ble',\n",
       " 'alguien sabe puedo intercambiar photocards exo oo',\n",
       " 'ahhh todo mundo manda audios puedo escuchar canciones tranquila',\n",
       " 'hastag somos peque√±os modestos podemos abarcar m√°s',\n",
       " 'hastag 3 ediciones originales buenono aguanto demasiado antiguos',\n",
       " 'hastag que bonita zona entreno escogieron',\n",
       " 'hastag verlo tio jaja merece hacia tiempo merecia',\n",
       " 'hastag ve√≠a hermano jugar malas pulgaaas',\n",
       " 'hastag buen art√≠culo carlos va lucas tampoco dramatizar casi haces llorar',\n",
       " 'despues sigues pendiente sabes autoenga√±arse is moreee easyy',\n",
       " 'hastag acabo hora jajaja expectante ver si tocado hastag llevarlo lados puesto',\n",
       " 'alg√∫n alma caritativa prestar pokemon',\n",
       " 'hastag mejor cuando pagan esa nasa fin cabo pagamos que chulada',\n",
       " 'hastag puedo cambiar opini√≥n cara dif√≠cil vale pasta',\n",
       " 'hastag aunque aqu√≠ precisamente ninguno dos da igual haces',\n",
       " 'hastag im back aunque desaparecida combate alegro lot graduaci√≥n dr mucho √°nimo mir un abrazo grande',\n",
       " 'hastag hastag hastag bastante bastante ganitas verlo romper premier',\n",
       " 'hastag nah pringao lleva aguant√°ndole cansado',\n",
       " 'hastag ahora abel resfriado ke juntos tiempo jajaja curro bien',\n",
       " 'hastag a√∫n no si jugable tgs creo tarde demasiado',\n",
       " 'est√° puesta pel√≠cula c√≥mo entrenar drag√≥n autob√∫s pantalla demasiado lejos topo',\n",
       " '√∫ltimo dia elche ahora toca benidorm',\n",
       " 'hastag aunque lucha jodidos punto',\n",
       " 'de aqu√≠ semana har√° cuatro a√±os vivo barcelona creo momento dado mal',\n",
       " 'hastag hastag si vaticina club 2 3 a√±os malas palabras vale',\n",
       " 'hastag hecho nintendo magic principal fuente informaci√≥n especiales biogr√°ficos iwata',\n",
       " 'tio dawn dispara beth daryl dispara inmediatamente pone llorar',\n",
       " 'hastag bueno algo cuando encima sabes pasan cosas peor dicho √°nimos',\n",
       " 'ha ardido caravana aparcada lado playa fuego sofocado bomberos ‚Ä¶ web',\n",
       " 'hastag pero gral acuerd opini√≥n interesante tendr√≠a q escribir entrada blog hastag',\n",
       " 'hastag cosas hilo discrepo como sigues hace poco aviso hago rt interesante coincida',\n",
       " 'hastag hastag hastag hastag est√° camino fri√©ndose bajo sol sevillano',\n",
       " 'hastag pero verdad general voy mal motivaci√≥n casi rev√©s sobra motivaci√≥n y ego',\n",
       " 'hastag hastag hastag hastag ten seguro si queda libre cuento contigo',\n",
       " 'hastag ja ja imagino cada forma distinta yo compro gadgets veo peli como interstellar',\n",
       " 'mi √∫ltima partida jugada sona support la grandes razones jugar sona web',\n",
       " 'me cansando dar rt d rt suspendes pa luego suspender fijo',\n",
       " 'mi madre deja ponerme rubia pelo morado',\n",
       " 'por parte necesito cena s√°bado volver juntos lado pienso vamos separar',\n",
       " 'he hecho primer pur√© verduras vida parece pota sabe mal',\n",
       " 'por ser pelo tan goals mojado tan mierda despues',\n",
       " 'hastag duchate rapido babe sue√±o',\n",
       " 'eso echar menos propia casa leyendas urbanas',\n",
       " 'tengo ganas bronca aburrido proximo tweet abro cajon mierda',\n",
       " 'ea ma√±ana despierte empieza fiesta estare distraido largo viaje valencia cadiz',\n",
       " 'hastag las chicas fuertes guapas luchar oscuridad mundo mucha',\n",
       " 'hastag hastag hastag hastag hastag hastag peque√±as diferencias enfrentan dos espa√±as',\n",
       " 'hastag ¬° muchas gracias respuesta cualquier cosa aqu√≠ necesites ¬° saludos',\n",
       " 'estaba autob√∫s benidormmadrid lado puertas abiertas tentado subirme volver',\n",
       " 'hastag es solo piensa todas canciones tristes loser ejemplo basadas experiencia',\n",
       " 'hastag hastag no no ¬° dijo dejaban surfear cornea que quedaban quietecitas',\n",
       " 'hastag hastag ¬° hola ana te contestado mensaje privado limitaci√≥n caracteres ¬° gracias',\n",
       " 'hastag hecho confirmo caminos casitas mismas',\n",
       " 'si alg√∫n alma caritativa mac sabe instalar excell windows diga pls',\n",
       " 'hastag hastag derecha aparecen personajes jugados ranked vamos jugar',\n",
       " 'ya dec√≠a bueno iba llegar tarde temprano espero poder equivocarme',\n",
       " 'voy so√±ar f√≥rmulas matem√°ticas historia mundo pasiva ingl√©s le explotar√° cabesa',\n",
       " 'hastag pues escap√≥ y dimos encontrado menos 5 personas buscando',\n",
       " 'hastag yo veces hecho menos pelo largo mismo cosas',\n",
       " 'hastag cosas molan a mejor conoces speakingio zack holman',\n",
       " 'cansado tenes albacete cansado sino vuelvo rosario verano ojo',\n",
       " 'he visto documental pir√°mides moais predicciones ahora da cosita dormir sola',\n",
       " 'la regla destrzando cuerpo quiero vomitar duele tripa cabeza fiebre mareada pf',\n",
       " 'hastag hastag hastag hastag hastag hastag qu√© bonito pagar horas extras tambi√©n as√≠',\n",
       " 'hastag nazismo fangirleo filosof√≠a vida doble moral juju',\n",
       " 'hastag hastag si ayer reunidos moncloa final repetir elecciones üòÇ üòÇ üòÇ est√° responde',\n",
       " 'hastag cuando borr√© twitter jeje cuenta nueva aunque mismo nick',\n",
       " 'ya oficia quedamos konoplyanka hastag eso s√≠ alternativa barata reus bundes web',\n",
       " 'con ganas llegue septiembre y comenzamos ma√±ana nueva temporada hastag hastag',\n",
       " 'hastag gracias √≠dolo aunque solo persona normal corriente escribe libros',\n",
       " 'hastag todos siempre carita super buenos adorables luego l√≠an',\n",
       " 'hastag hola vengo intagram verdad seguia hola igualmente üôÑ ‚ù§ Ô∏è',\n",
       " 'llevo toda ma√±ana haciendo cosas podido ver moon lovers signal',\n",
       " 'no sensaci√≥n ir trabajo din hastag eu na ofi lendo hastag tampouco te√±o web',\n",
       " 'hastag hastag hastag hombre le√≠do la pregunta es ¬ø cu√°l pretende ser titular completo',\n",
       " 'las horas sigo ser capaz ponerme estudiarel sue√±o puede conmigo',\n",
       " 'voy sentar sobrina m√©dico dicen s√∫per mal est√° ocupado digo yo pues desocupa ni√±a',\n",
       " 'hastag sabueso malo le√±ador mola quiero minero mago hielo',\n",
       " 'hijos de puta dejad darle rt preguntadle alguien cuantos dao rt solo respondido 1',\n",
       " 'hastag hastag anda castr√≥n com√≠ aquel troc√≠n peque√±o cortaste',\n",
       " 'a nunca podr√°n hacer broma cojo llamadas menos ocultas',\n",
       " 'hastag hastag hastag peque√±a donaci√≥n har√° felices miles chicas hastag',\n",
       " 'hastag muchas gracias feliz haber formado parte edici√≥n master',\n",
       " 'cago puta vida puto vecino hacer ruido buena ma√±ana',\n",
       " 'hastag llama wolf mascota hastag apoyo deportistas paral√≠mpicos m√°s inf web üòÉ',\n",
       " 'hastag hastag genial rebeca este deseo escribirlo seguir√© escribiendo linea pr√°ctica',\n",
       " 'hastag hoy trabajo compa√±√≠a hastag trabaja rodeado gente maja hastag hastag',\n",
       " 'hastag calla ahora d√≠a libre horas pasan r√°pido poniendo copas bar',\n",
       " 'hastag hastag como mejor luchar crisis unidos siempre enfrentados lo claro',\n",
       " 'as√≠ leyendo l√≠neas discurso hastag igual vota rajoy hastag',\n",
       " 'hastag precauci√≥n amigo conductor senda peligrosa lo importante llegar',\n",
       " 'hastag hola julio pizzas lleguen mejores condiciones marcamos pizza cortamos',\n",
       " 'ayer d√≠a emociones encontrada acerca septiembre lado personita siempre hace sonre√≠r',\n",
       " 'no silencio calla es vida suspendida brotan ganas bailarla hastag web',\n",
       " 'qu√© bonito ser mujer joda planes tenias despierte dolor ovarios üò° üò° üò°',\n",
       " 'hastag casi nadie hablando tema y tampoco puedo encontrar v√≠deo sad',\n",
       " 'hastag ¬° hooola buenas soy ojos rojos fiesta elle ¬° te sigo',\n",
       " 'hastag 178 sabes caes bien futuro dar√© co√±azo dudas dise√±o gr√°fico',\n",
       " 'hastag ¬° muchas gracias carla de momento ca√≠do tres as√≠ empieza mal d√≠a',\n",
       " '¬° ¬° muchas felicidades d√≠a cumplea√±os buen amigo hastag a pasarlo grande compa√±ero',\n",
       " 'hola reviento haberme comido box kebab ayudadme morir gracias',\n",
       " 'despu√©s casi mes entero √©l puff espero echar temporadas tan largas verlo',\n",
       " '¬ø no pasa ferias pod√©is ver persona unas 29 veces justo quieres ver lela ves jodida vez',\n",
       " 'tambien subire changelog largo dias estaros preparados viene golpe',\n",
       " 'hastag que guap√≠simo dejaron mendizorroza san mames guapos mundo ¬° ¬° dudaopini√≥n',\n",
       " 'baby hastag hastag suerte semana pasada demasiado grande mi',\n",
       " 'hastag ja ja ja ja ja teneis ojino bandera jap√≥n hijos gran puta',\n",
       " 'hastag √∫ltimos d√≠as sido detr√°s otra ahora mismo vamos la fe miren pau vez bronquitis',\n",
       " 'creo pocas veces tan enfadado ma√±ana bueno uncharted 4 ejercicio animado',\n",
       " 'echo menos lexa bragas vuelan antes ya hace falta limpiarlas cada 5 minutos',\n",
       " 'hastag lo pasado grande 4 estaciones una aut√©ntica gozada toda familia',\n",
       " 'plan perfecto terminar fin semana hastag hastag despu√©s hastag hastag',\n",
       " 'utop√≠ audiencia estable media 85 imdb etc pero cancelada no puede tirar manta farmac√©utica',\n",
       " 'vida mierda primero enamoro macedonio ibiza ahora timbales panorama',\n",
       " 'tranquilos eldinero camacho seguro ma√±ana empiezan escabadoras arraijanal',\n",
       " 'hastag hastag pellegrino dotado equipo gran consistencia defensiva y buenos fichajes',\n",
       " 'hastag queda bien tio espero esteis pasando buen verano',\n",
       " 'hastag denada porcierto hazme hueco agenda combates quiero ver capaz monotype mio',\n",
       " 'esta semana sido tan chachiosa puedo decir verano acabado manera decente lloro',\n",
       " 'hastag hastag hastag m√°s bien imposible a ser q convierta islam claro',\n",
       " 'apenas muerto miedo pasando plaza hace hoguera san juan toda oscura',\n",
       " 'hastag mclaren parece q mejora 15s mas r√°pido a√±o pasado qualy mantuvo raya williams spa',\n",
       " 'como dice hastag escapa verano furg√≥n blindado hastag hastag hastag web',\n",
       " 'no ganado ganar√© concurso me esperabasoy gafe fin cabo suerte nunca parte hastag',\n",
       " 'en verd√° llevo jugar pokemon go volv√≠ galicia ver madrid sale guay deprime',\n",
       " 'mi padre dado manotazo m√≥vil gracias dios roto sido cristal templado',\n",
       " 'quiero pelo negro azulado s√© c√≥mo conseguirlo tintes color echado hecho nada',\n",
       " 'hastag diciendo feminismo contrario machismo hembrismo q dices existe',\n",
       " 'hastag hastag la verdad si bonito me alegro gustado y aprovechado paseo virtual jeje',\n",
       " 'hastag textos ingl√©s voces japon√©s que desilusi√≥n llevado poner juego acostumbrarse üòï',\n",
       " '¬ø lo gracioso si defiendesapoyaseres activista feminismo pues machista opresor',\n",
       " 'hastag hastag hastag genial est√° genial gente conozca alternativas',\n",
       " 'hastag porque nunca toca arbitro bueno hay alguno bueno valencia',\n",
       " 'nunca sensacion ¬ø me har√≠as sentir es importante blas hastag hastag',\n",
       " 'hastag hastag hastag obviamente tia jajajajaja q mayor osqqq venir ofu',\n",
       " 'hastag ahora alternativa incoherente igual inviable d√≠a hoy en creo acuerdo',\n",
       " 'hastag pero verano tampoco llegado hacer calor si comparamos pasado',\n",
       " 'hastag regales capturas imagen actividades sorpresa lo sorprendente inivitado',\n",
       " 'hastag bq aquaris m55 hace mes exacto fallan cosas busco fluido problemas buena garantia',\n",
       " 'hastag jajaja genial deseo mismo d√≠as construyen verdad',\n",
       " 'hastag ya sido grotesco parte poner foto ojeda 1030 perdona',\n",
       " 'hastag por cierto usa obligan parir ni√±a 12 a√±os embarazada caua violaci√≥n es unas amigas',\n",
       " 'y directos deber√≠an ser totalmente estables ya madrid sol√≠an darme problemas se vienen cosas chulas',\n",
       " 'odio ubicaci√≥n tw quiero dejar puesta cada vez tuiteo sale nombre pueblo quiero',\n",
       " 'hola hastag gustar√≠a escuchar hastag nuevo single promocional hastag hastag hastag',\n",
       " 'hastag home pues todos l m levantar prontisimo solo hora luego x j v muchas horas clase',\n",
       " 'hastag hastag brutal como parece manis separatistas catalunya espero consigan anhelada libertad',\n",
       " 'hastag hastag hastag s√≠ usual pa√≠s vasco zonas lim√≠trofes cab√≠a √∫ltimo tuit',\n",
       " 'hastag hastag ia lleg√≥ ser social fue espacio social dinamizaci√≥n √∫til fecha caducidad y abrumador',\n",
       " 'hastag juro gan√≥ pantin classic tiro gorra salte calleron 2 encima aplastaron jaj',\n",
       " 'hastag aun hace calor piscinas cerradas mar pilla lejos el corte ingl√©s dicho a√∫n acaba',\n",
       " 'hastag hastag hastag hastag hastag el sigu escuela administraci√≥n triling√ºe ¬ø le parece hastag',\n",
       " 'la envidia conociendo jenko danielle ahora mismo normal es aunque coja avi√≥n llego',\n",
       " 'me alegro vosotros sois familia ideal hastag aqui v si ni√±o erik si ni√±a esther jaja üíó',\n",
       " 'hastag ojal√° personaje pero examen 12 de todas formas hablamos semana siguiente dicho fernando',\n",
       " 'hastag necesito se acaba romper snifff snifff gran podcast saludos a coru√±a espa√±a',\n",
       " 'bueno pues vamos recuperar tiempo perdido empezare descargarme manchester city west ham ver tal',\n",
       " 'hastag hastag hastag hastag hastag gran capacidad an√°lisis hastag entrevista hastag',\n",
       " 'hastag domingo encontremos gloria bendita ocio magia besos compartidos amor duende',\n",
       " 'hastag hastag hastag hastag hastag hastag hastag gracias paso genial haci√©ndolo',\n",
       " 'hastag en alg√∫n momento empec√© mirar suelo cielo estoy harto llorar as√≠ intento sonre√≠r',\n",
       " 'hastag tengo entendido v√≠deo salen trampeadores dedican atrapar luego liberar lobos muerte',\n",
       " 'ahora empiezan jornadas toros ciudad pongo triste ver seguimos evolucionar muchas cosas',\n",
       " 'que dificil retomar estudios despues dos meses tocar libro hastag hastag hastagexamen',\n",
       " 'hastag hastag hastag ¬° cada vez m√°s las risas aseguradas entonces ma√±ana preguntamos dadlo hecho',\n",
       " 'hastag digamos bastante mayor q si pone tontaponerla rodillas darle azotes culete',\n",
       " 'hastag hastag hablaba normal alguien q usted metiera insultando deber√≠a tener educaci√≥n',\n",
       " 'hastag muchas felicidades d√≠a cumplea√±os sensual espero pases genial recibas regalos suculentos',\n",
       " 'hastag septiembrees bonito retarsees increible puedes aprender medirse obst√°culoeres joyita corona',\n",
       " 'hastag esa posibilidad contemplada y si hiciera falta x bien espa√±a responsabilidad volver√≠a hacerse dudar',\n",
       " 'no puedo evitar ir beber lata refresco acordarme hastag family terminar sirvi√©ndome vaso',\n",
       " 'hastag ya gente capaz hacer eso yo si coraz√≥n pide poner cara seta no capaz sonreir',\n",
       " 'hastag hastag bastante tener dos puntos salud adem√°s enamorad√≠sima princesa princesa mala',\n",
       " 'hastag s√≠ le√≠do eso luego le√≠do oto√±o noviembre diciembre dado baj√≥n adem√°s ds juego',\n",
       " 'hastag el efecto largo plazo desafecci√≥n mayoritaria izquierda el votante cansa queda casa',\n",
       " 'hastag caso humanidades lat√≠n griego vida dado latin griego as√≠ iria fatal',\n",
       " 'urgente venta my name tikets tengo dos tickets ultimate vip podemos ir vendo baratos contactad conmigo',\n",
       " 'hastag ya jugando rat√≥n gato no solo voy quedo dormir ma√±ana hotel quedamos fijo',\n",
       " 'hastag hastag toda razon alvaro soy gran fan ense√±aste casi clash of clans mejor',\n",
       " 'hastag ajajajaja benidorm vi dos mujeres carritos beb√© perros dentro vimos quedamos flipados',\n",
       " 'hastag pero gusta idea ver chicas fuertes luchando estilo dragon ball z versi√≥n real',\n",
       " 'hastag juro acuerdo solo s√© alba hizo v√≠deo inmit√°ndole recuerdo absolutamente original',\n",
       " 'hastag ilustres jose qu√© f√°cil salen insultos viendo c√≥mo empezado burgos murcia',\n",
       " 'han talado abedul gigante esquina r√≠os rosas castellana ¬ø alguien sabe porqu√© hastag',\n",
       " 'demasiadas cosas raras tiempo anda follen 0 complicaciones 0 historias me voy playa viva espa√±a',\n",
       " 'acabo comprobar linkedin si antiguo alumno gran recuerdo hastag te deseo mejor hastag',\n",
       " 'hastag uy bien vendr√≠a colocar wifi port√°til flautas saxo nariclete partituras desorden',\n",
       " 'hastag alegro mucho importante darnos cuenta gran valor podemos aportar encontrar misi√≥n',\n",
       " 'yo cansadete repente habla amigo jugar ratejo beta battlefield 1 qui√©n dice chico',\n",
       " 'es decente tener gobierno democr√°tico cumpla funci√≥n politica no tener gobierno representa hastag',\n",
       " 'hastag d cines hacen findeelaccesible icariaq bajado preciosno pagan xdo matinales diagmar',\n",
       " 'subtitulos bojack horseman tan hablando el pais el mundo el mundo today alaska mario ye terrible',\n",
       " 'hastag hastag intentado espero q pr√≥ximos d√≠as veamos ilustraci√≥n pr√°ctica q ayude clarificarlo',\n",
       " 'hastag bien ahora solo falta pong√°is estaci√≥n hastag poder ir bici casa',\n",
       " 'hastag uno aprende mejor momento crear bonitos recuerdos presente pasado fue toca elegir va ser',\n",
       " 'hastag hastag hastag gracias √∫nico hace feliz escuchar m√∫sica adivinar ganar',\n",
       " 'hastag pues pas√≥ experiencia paranormal all√≠ noche vuelto pas√© much√≠simo miedo',\n",
       " 'esta decidido ¬° habr√° modo carrera canal fifa 17 si tweet llega 30 likes desvelar√© equipo har√°',\n",
       " 'hastag genial ten cuidado vigilado hecho casi juguete syma x5c casi entra√±a riesgo',\n",
       " 'tengo dos horas seguidas lat√≠n griego dos horas seguidas misma profesora menos mal cae bien pero dos horas',\n",
       " 'hastag cine rom√°ntico vale yo regalar dildos lubricantes comida buen uso luego doy masaje',\n",
       " 'hastag llevo medio verano debajo dichosa m√°quina menudo verano largo a√±o sofoco',\n",
       " 'cuando suben tres fotos representando tres clases escuela m√∫sica sales menos bien',\n",
       " 'hastag borrao lol',\n",
       " 'hastag s√≠ resignado',\n",
       " 'era grande parece',\n",
       " 'hastag deprimida',\n",
       " 'serio tan tontos',\n",
       " 'ir√© bgw peque√±a sorpresa',\n",
       " 'no ahora feliz',\n",
       " 'ma√±ana va ser d√≠a guay',\n",
       " 'qu√© jodido ser hastag hoy',\n",
       " 'hastag pasado 76',\n",
       " 'estoy preocupado bueno',\n",
       " 'hastag activados',\n",
       " 'hastag dos triste',\n",
       " 'hastag 9 buenico üíú',\n",
       " 'hastag dura',\n",
       " 'ya encuentro mejor',\n",
       " 'alguien duo noche',\n",
       " 'que den buenas noches',\n",
       " 'hoy d√≠a especial feliz 11s',\n",
       " 'hastag mas pobre',\n",
       " 'quiero ir diada',\n",
       " 'hastag directo',\n",
       " 'hastag tanto impartas',\n",
       " 'hastag no gilipollas',\n",
       " 'un d√≠a casa sola f√≠n',\n",
       " 'hastag pero si inocente soy',\n",
       " 'luego rara',\n",
       " 'esto feliz mola',\n",
       " 'ganas ff xv serio',\n",
       " 'hastag si tan despreciable',\n",
       " 'las 7 cuarto aqu√≠',\n",
       " 'hastag des serio',\n",
       " 'hastag bonita',\n",
       " 'hastag tan facil',\n",
       " 'ya picado bichos',\n",
       " 'me voy despejarme poquito ahi',\n",
       " 'que coraje dan lxs prepotentxs',\n",
       " 'ah bueno dinero',\n",
       " 'encima mal amores tio',\n",
       " 'en realidad tan blanca',\n",
       " 'tengo md abiertos crushes',\n",
       " 'lo peor aposta',\n",
       " 'hastag tia raro',\n",
       " 'soy obvia suda',\n",
       " 'hastag ps conmigo',\n",
       " 'hastag normal',\n",
       " 'mensaje directo',\n",
       " 'hastag es tranquila',\n",
       " 'me muriendo malo',\n",
       " 'hastag bonica',\n",
       " 'a ver s√∫per cutie tal',\n",
       " 'me encuentro mal hoy trabajar',\n",
       " 'hastag bof tio duro',\n",
       " 'hastag intimidas',\n",
       " 'hastag oye tan burguesito sbs',\n",
       " 'hastag no lemado',\n",
       " 'xq ta cerrao estanco',\n",
       " 'hastag grasioso',\n",
       " 'hastag pero rica',\n",
       " 'hastag eso antiguo',\n",
       " 'mi tl vac√≠a horas',\n",
       " 'ya uno cheto hastag',\n",
       " 'estoy sensible lloro']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 980 candidates, totalling 4900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4900 out of 4900 | elapsed: 16.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3877597477137727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 100,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.8,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (3, 6)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", LogisticRegression())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__C\":[1,10,100,1000,10000]\n",
    "             }\n",
    "\n",
    "clf_lr = GridSearchCV(pipe,\n",
    "                      param_grid,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=2,\n",
    "                      scoring=\"f1_macro\")\n",
    "\n",
    "clf_lr.fit(train_clean, y_train)\n",
    "print(clf_lr.best_score_)\n",
    "clf_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1960 candidates, totalling 9800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9800 out of 9800 | elapsed: 39.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3848031342070236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10,\n",
       " 'clf__kernel': 'linear',\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.8,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (4, 5)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", SVC())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__kernel\":['linear', 'rbf'],\n",
    "              \"clf__C\":[1,10,100,1000,10000]\n",
    "             }\n",
    "\n",
    "clf_svc = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_svc.fit(train_clean, y_train)\n",
    "print(clf_svc.best_score_)\n",
    "clf_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 980 candidates, totalling 4900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4900 out of 4900 | elapsed: 28.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31993195546535513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__n_estimators': 50,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.5,\n",
       " 'tfidf__min_df': 3,\n",
       " 'tfidf__ngram_range': (4, 5)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__n_estimators\":[50,100,150,200,300]\n",
    "             }\n",
    "\n",
    "clf_rfc = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_rfc.fit(train_clean, y_train)\n",
    "print(clf_rfc.best_score_)\n",
    "clf_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1372 candidates, totalling 6860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   42.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36833977444122495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6860 out of 6860 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__n_neighbors': 5,\n",
       " 'tfidf__analyzer': 'char_wb',\n",
       " 'tfidf__max_df': 0.3,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (3, 4)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "])\n",
    "    \n",
    "param_grid = {\"tfidf__ngram_range\" : [(1,2),(1,3),(2,3),(3,4),(3,5),(3,6),(4,5)],\n",
    "              \"tfidf__max_df\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              \"tfidf__min_df\":[1,2,3,5], # or percentages\n",
    "              \"tfidf__analyzer\":[\"char_wb\"], # n-grams\n",
    "              \"clf__n_neighbors\":[3,5,7,11,15,21,25]\n",
    "             }\n",
    "\n",
    "clf_knn = GridSearchCV(pipe,\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=2,\n",
    "                       scoring=\"f1_macro\")\n",
    "\n",
    "clf_knn.fit(train_clean, y_train)\n",
    "print(clf_knn.best_score_)\n",
    "clf_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation over development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.56      0.67      0.61       219\n",
      "         NEU       0.11      0.07      0.09        69\n",
      "        NONE       0.39      0.19      0.26        62\n",
      "           P       0.54      0.57      0.55       156\n",
      "\n",
      "    accuracy                           0.50       506\n",
      "   macro avg       0.40      0.38      0.38       506\n",
      "weighted avg       0.47      0.50      0.48       506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,6),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.56      0.71      0.63       219\n",
      "         NEU       0.07      0.01      0.02        69\n",
      "        NONE       0.32      0.19      0.24        62\n",
      "           P       0.55      0.63      0.59       156\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.38      0.39      0.37       506\n",
      "weighted avg       0.46      0.53      0.49       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.5,\n",
    "                                 min_df=3,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=50))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.57      0.71      0.63       219\n",
      "         NEU       0.14      0.10      0.12        69\n",
      "        NONE       0.38      0.19      0.26        62\n",
      "           P       0.55      0.53      0.54       156\n",
      "\n",
      "    accuracy                           0.51       506\n",
      "   macro avg       0.41      0.38      0.39       506\n",
      "weighted avg       0.48      0.51      0.49       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,4),\n",
    "                                 max_df=0.3,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.52      0.63      0.57       219\n",
      "         NEU       0.15      0.12      0.13        69\n",
      "        NONE       0.43      0.32      0.37        62\n",
      "           P       0.55      0.49      0.52       156\n",
      "\n",
      "    accuracy                           0.48       506\n",
      "   macro avg       0.41      0.39      0.40       506\n",
      "weighted avg       0.47      0.48      0.47       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=10, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.53      0.83      0.65       219\n",
      "         NEU       0.10      0.01      0.03        69\n",
      "        NONE       0.70      0.11      0.19        62\n",
      "           P       0.59      0.55      0.57       156\n",
      "\n",
      "    accuracy                           0.55       506\n",
      "   macro avg       0.48      0.38      0.36       506\n",
      "weighted avg       0.51      0.55      0.48       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(4,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=2,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=10, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.58      0.73      0.65       219\n",
      "         NEU       0.17      0.12      0.14        69\n",
      "        NONE       0.51      0.31      0.38        62\n",
      "           P       0.57      0.54      0.55       156\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.46      0.42      0.43       506\n",
      "weighted avg       0.51      0.53      0.52       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=100, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(dev_clean)\n",
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS\n",
    "{'clf__C': 100,\n",
    " 'clf__kernel': 'linear',\n",
    " 'tfidf__analyzer': 'char_wb',\n",
    " 'tfidf__max_df': 0.8,\n",
    " 'tfidf__min_df': 1,\n",
    " 'tfidf__ngram_range': (3, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL\n",
    "\n",
    "pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(3,5),\n",
    "                                 max_df=0.8,\n",
    "                                 min_df=1,\n",
    "                                 analyzer=\"char_wb\")),\n",
    "        (\"clf\", SVC(C=100, kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(train_clean, y_train)\n",
    "predictions = pipe.predict(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.txt\", \"w\") as f:\n",
    "    for id_ts, pred in zip(id_test , predictions):\n",
    "        f.write(\"{}\\t{}\\n\".format(id_ts, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
